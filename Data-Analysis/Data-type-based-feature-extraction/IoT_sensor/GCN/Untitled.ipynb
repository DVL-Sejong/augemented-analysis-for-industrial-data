{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f59d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df52fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    normalized_adj = normalized_adj.astype(np.float32)\n",
    "    return normalized_adj\n",
    "    \n",
    "def sparse_to_tuple(mx):\n",
    "    mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    L = tf.SparseTensor(coords, mx.data, mx.shape)\n",
    "    return tf.sparse_reorder(L) \n",
    "    \n",
    "def calculate_laplacian(adj, lambda_max=1):  \n",
    "    adj = normalized_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sp.csr_matrix(adj)\n",
    "    adj = adj.astype(np.float32)\n",
    "    return sparse_to_tuple(adj)\n",
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                            maxval=init_range, dtype=tf.float32)\n",
    "\n",
    "    return tf.Variable(initial,name=name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bf3534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
    "    train_size = int(time_len * rate)\n",
    "    train_data = data[0:train_size]\n",
    "    test_data = data[train_size:time_len]\n",
    "    \n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for i in range(len(train_data) - seq_len - pre_len):\n",
    "        a = train_data[i: i + seq_len + pre_len]\n",
    "        trainX.append(a[0 : seq_len])\n",
    "        trainY.append(a[seq_len : seq_len + pre_len])\n",
    "    for i in range(len(test_data) - seq_len -pre_len):\n",
    "        b = test_data[i: i + seq_len + pre_len]\n",
    "        testX.append(b[0 : seq_len])\n",
    "        testY.append(b[seq_len : seq_len + pre_len])\n",
    "      \n",
    "    trainX1 = np.array(trainX)\n",
    "    trainY1 = np.array(trainY)\n",
    "    testX1 = np.array(testX)\n",
    "    testY1 = np.array(testY)\n",
    "    return trainX1, trainY1, testX1, testY1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95bef6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(object):\n",
    "    def __init__(self, num_units, adj, inputs, output_dim, activation=tf.nn.tanh,\n",
    "                input_size = None, num_proj = None, reuse = None, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        \n",
    "        self._num_units = num_units\n",
    "        self._output_dim = output_dim\n",
    "        self._inputs = inputs\n",
    "        self._num_nodes = inputs.get_shape()[2].value\n",
    "        self._input_dim = inputs.get_shape()[1].value\n",
    "        self._batch_size = tf.shape(inputs)[0]\n",
    "        self._adj = []\n",
    "        self._adj.append(calculate_laplacian(adj))\n",
    "        self._activation = activation\n",
    "        self._gconv()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_units\n",
    "        return output_size\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "           \n",
    "    def _gconv(self,scope=None):\n",
    "        inputs = self._inputs       \n",
    "        inputs = tf.transpose(inputs, perm=[2,0,1])\n",
    "        x0 = tf.reshape(inputs,shape=[self._num_nodes,self._batch_size*self._input_dim])\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for adj in self._adj:\n",
    "                x1 = tf.sparse_tensor_dense_matmul(adj, x0)\n",
    "            x1 = tf.reshape(x1,shape=[self._num_nodes,self._batch_size,self._input_dim])\n",
    "            x1 = tf.reshape(x1,shape=[self._num_nodes*self._batch_size,self._input_dim])\n",
    "            \n",
    "            weights = weight_variable_glorot(self._input_dim, self.output_size, name='weights')\n",
    "            self.hidden1 = self._activation(tf.matmul(x1, weights))\n",
    "                                 \n",
    "            weights = weight_variable_glorot(self.output_size,self._output_dim, name='weights')\n",
    "            self.output = tf.matmul(self.hidden1, weights)\n",
    "            self.output = tf.reshape(self.output,shape=[self._num_nodes,self._batch_size,\n",
    "                                                        self._output_dim])\n",
    "            self.output = tf.transpose(self.output, perm=[1,2,0])\n",
    "            self.output = tf.reshape(self.output,shape=[-1,self._num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4412cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpath = '../Data/Gangnam/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f481da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = mpath + 'speed_gangnam.csv'\n",
    "adj_path = mpath + 'adj_mx_gangnam.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72bcb675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1210007700</th>\n",
       "      <th>1210008500</th>\n",
       "      <th>1210009500</th>\n",
       "      <th>1210010300</th>\n",
       "      <th>1210011300</th>\n",
       "      <th>1210013700</th>\n",
       "      <th>1220011900</th>\n",
       "      <th>1220016300</th>\n",
       "      <th>1220021100</th>\n",
       "      <th>1220025100</th>\n",
       "      <th>...</th>\n",
       "      <th>1210013800</th>\n",
       "      <th>1210013000</th>\n",
       "      <th>1210012200</th>\n",
       "      <th>1210012300</th>\n",
       "      <th>1210013100</th>\n",
       "      <th>1210013900</th>\n",
       "      <th>1210014900</th>\n",
       "      <th>1210015900</th>\n",
       "      <th>1210017100</th>\n",
       "      <th>1210018300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.70</td>\n",
       "      <td>21.00</td>\n",
       "      <td>33.54</td>\n",
       "      <td>43.21</td>\n",
       "      <td>24.31</td>\n",
       "      <td>26.54</td>\n",
       "      <td>35.91</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.78</td>\n",
       "      <td>21.76</td>\n",
       "      <td>...</td>\n",
       "      <td>24.98</td>\n",
       "      <td>24.54</td>\n",
       "      <td>32.76</td>\n",
       "      <td>43.75</td>\n",
       "      <td>32.36</td>\n",
       "      <td>32.13</td>\n",
       "      <td>35.47</td>\n",
       "      <td>22.96</td>\n",
       "      <td>26.62</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.50</td>\n",
       "      <td>38.00</td>\n",
       "      <td>25.61</td>\n",
       "      <td>43.89</td>\n",
       "      <td>22.75</td>\n",
       "      <td>24.37</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.07</td>\n",
       "      <td>25.92</td>\n",
       "      <td>22.66</td>\n",
       "      <td>...</td>\n",
       "      <td>26.22</td>\n",
       "      <td>25.14</td>\n",
       "      <td>32.15</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>35.04</td>\n",
       "      <td>26.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>27.22</td>\n",
       "      <td>29.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>24.36</td>\n",
       "      <td>35.75</td>\n",
       "      <td>24.63</td>\n",
       "      <td>21.96</td>\n",
       "      <td>22.67</td>\n",
       "      <td>...</td>\n",
       "      <td>21.55</td>\n",
       "      <td>25.16</td>\n",
       "      <td>34.37</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>36.32</td>\n",
       "      <td>22.42</td>\n",
       "      <td>32.40</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.00</td>\n",
       "      <td>34.39</td>\n",
       "      <td>32.70</td>\n",
       "      <td>44.15</td>\n",
       "      <td>23.64</td>\n",
       "      <td>27.76</td>\n",
       "      <td>40.06</td>\n",
       "      <td>41.00</td>\n",
       "      <td>28.65</td>\n",
       "      <td>21.68</td>\n",
       "      <td>...</td>\n",
       "      <td>25.33</td>\n",
       "      <td>24.11</td>\n",
       "      <td>33.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>29.88</td>\n",
       "      <td>34.93</td>\n",
       "      <td>37.35</td>\n",
       "      <td>21.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>32.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.20</td>\n",
       "      <td>37.65</td>\n",
       "      <td>30.75</td>\n",
       "      <td>43.66</td>\n",
       "      <td>22.76</td>\n",
       "      <td>28.04</td>\n",
       "      <td>28.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.92</td>\n",
       "      <td>...</td>\n",
       "      <td>24.44</td>\n",
       "      <td>24.15</td>\n",
       "      <td>34.94</td>\n",
       "      <td>28.11</td>\n",
       "      <td>33.88</td>\n",
       "      <td>33.53</td>\n",
       "      <td>39.30</td>\n",
       "      <td>30.68</td>\n",
       "      <td>27.68</td>\n",
       "      <td>30.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>27.50</td>\n",
       "      <td>36.33</td>\n",
       "      <td>29.53</td>\n",
       "      <td>39.86</td>\n",
       "      <td>26.15</td>\n",
       "      <td>20.05</td>\n",
       "      <td>25.37</td>\n",
       "      <td>18.27</td>\n",
       "      <td>21.88</td>\n",
       "      <td>18.25</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>24.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>21.95</td>\n",
       "      <td>29.47</td>\n",
       "      <td>28.47</td>\n",
       "      <td>31.52</td>\n",
       "      <td>32.13</td>\n",
       "      <td>28.22</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>25.51</td>\n",
       "      <td>37.62</td>\n",
       "      <td>30.48</td>\n",
       "      <td>41.88</td>\n",
       "      <td>26.52</td>\n",
       "      <td>23.00</td>\n",
       "      <td>31.94</td>\n",
       "      <td>16.56</td>\n",
       "      <td>24.29</td>\n",
       "      <td>19.14</td>\n",
       "      <td>...</td>\n",
       "      <td>28.65</td>\n",
       "      <td>24.49</td>\n",
       "      <td>32.52</td>\n",
       "      <td>27.44</td>\n",
       "      <td>31.76</td>\n",
       "      <td>33.00</td>\n",
       "      <td>44.68</td>\n",
       "      <td>24.89</td>\n",
       "      <td>33.04</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>29.40</td>\n",
       "      <td>28.26</td>\n",
       "      <td>31.81</td>\n",
       "      <td>39.68</td>\n",
       "      <td>26.45</td>\n",
       "      <td>24.28</td>\n",
       "      <td>28.92</td>\n",
       "      <td>18.00</td>\n",
       "      <td>23.41</td>\n",
       "      <td>21.11</td>\n",
       "      <td>...</td>\n",
       "      <td>32.26</td>\n",
       "      <td>23.88</td>\n",
       "      <td>32.02</td>\n",
       "      <td>26.94</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.10</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.70</td>\n",
       "      <td>30.21</td>\n",
       "      <td>18.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>24.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>29.02</td>\n",
       "      <td>41.93</td>\n",
       "      <td>25.49</td>\n",
       "      <td>25.38</td>\n",
       "      <td>25.57</td>\n",
       "      <td>17.61</td>\n",
       "      <td>23.63</td>\n",
       "      <td>21.70</td>\n",
       "      <td>...</td>\n",
       "      <td>27.08</td>\n",
       "      <td>27.32</td>\n",
       "      <td>34.00</td>\n",
       "      <td>26.27</td>\n",
       "      <td>27.32</td>\n",
       "      <td>29.02</td>\n",
       "      <td>37.90</td>\n",
       "      <td>27.58</td>\n",
       "      <td>30.23</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>31.77</td>\n",
       "      <td>38.30</td>\n",
       "      <td>30.65</td>\n",
       "      <td>39.34</td>\n",
       "      <td>34.20</td>\n",
       "      <td>23.70</td>\n",
       "      <td>30.08</td>\n",
       "      <td>15.28</td>\n",
       "      <td>27.10</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>21.30</td>\n",
       "      <td>32.74</td>\n",
       "      <td>33.75</td>\n",
       "      <td>27.22</td>\n",
       "      <td>32.43</td>\n",
       "      <td>39.29</td>\n",
       "      <td>29.04</td>\n",
       "      <td>34.64</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1210007700  1210008500  1210009500  1210010300  1210011300  1210013700  \\\n",
       "0          32.70       21.00       33.54       43.21       24.31       26.54   \n",
       "1          31.50       38.00       25.61       43.89       22.75       24.37   \n",
       "2          22.00       38.00       47.00       54.00       19.00       24.36   \n",
       "3          31.00       34.39       32.70       44.15       23.64       27.76   \n",
       "4          39.20       37.65       30.75       43.66       22.76       28.04   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       27.50       36.33       29.53       39.86       26.15       20.05   \n",
       "2876       25.51       37.62       30.48       41.88       26.52       23.00   \n",
       "2877       29.40       28.26       31.81       39.68       26.45       24.28   \n",
       "2878       24.00       28.21       29.02       41.93       25.49       25.38   \n",
       "2879       31.77       38.30       30.65       39.34       34.20       23.70   \n",
       "\n",
       "      1220011900  1220016300  1220021100  1220025100  ...  1210013800  \\\n",
       "0          35.91       29.00       28.78       21.76  ...       24.98   \n",
       "1          30.73       29.07       25.92       22.66  ...       26.22   \n",
       "2          35.75       24.63       21.96       22.67  ...       21.55   \n",
       "3          40.06       41.00       28.65       21.68  ...       25.33   \n",
       "4          28.10       23.25       23.00       19.92  ...       24.44   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2875       25.37       18.27       21.88       18.25  ...       28.17   \n",
       "2876       31.94       16.56       24.29       19.14  ...       28.65   \n",
       "2877       28.92       18.00       23.41       21.11  ...       32.26   \n",
       "2878       25.57       17.61       23.63       21.70  ...       27.08   \n",
       "2879       30.08       15.28       27.10       19.33  ...       26.68   \n",
       "\n",
       "      1210013000  1210012200  1210012300  1210013100  1210013900  1210014900  \\\n",
       "0          24.54       32.76       43.75       32.36       32.13       35.47   \n",
       "1          25.14       32.15       43.75       25.00       35.04       26.00   \n",
       "2          25.16       34.37       43.75       25.00       33.00       36.32   \n",
       "3          24.11       33.41       28.61       29.88       34.93       37.35   \n",
       "4          24.15       34.94       28.11       33.88       33.53       39.30   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       24.28       38.21       21.95       29.47       28.47       31.52   \n",
       "2876       24.49       32.52       27.44       31.76       33.00       44.68   \n",
       "2877       23.88       32.02       26.94       30.75       31.10       17.00   \n",
       "2878       27.32       34.00       26.27       27.32       29.02       37.90   \n",
       "2879       21.30       32.74       33.75       27.22       32.43       39.29   \n",
       "\n",
       "      1210015900  1210017100  1210018300  \n",
       "0          22.96       26.62       14.00  \n",
       "1          16.00       27.22       29.58  \n",
       "2          22.42       32.40       11.00  \n",
       "3          21.35       29.07       32.34  \n",
       "4          30.68       27.68       30.03  \n",
       "...          ...         ...         ...  \n",
       "2875       32.13       28.22       15.96  \n",
       "2876       24.89       33.04       11.00  \n",
       "2877       24.70       30.21       18.65  \n",
       "2878       27.58       30.23       15.41  \n",
       "2879       29.04       34.64       15.30  \n",
       "\n",
       "[2880 rows x 506 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46dd7901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 506),\n",
       " array([[1.        , 0.40327677, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.40327677, 1.        , 0.91133416, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.91133416, 1.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 1.        , 0.38495174,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.38495174, 1.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         1.        ]], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(adj_path, 'rb') as file:\n",
    "    temp = pickle.load(file)\n",
    "adj = temp[2]\n",
    "adj.shape, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c786167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 506)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]\n",
    "time_len, num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ac5ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.8\n",
    "seq_len = 12\n",
    "pre_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac027109",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.mat(data, dtype=np.float32)\n",
    "\n",
    "max_value = np.max(data1)\n",
    "data1 /= max_value\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "121051da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2289, 12, 506), (2289, 3, 506), (561, 12, 506), (561, 3, 506))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf1cbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "totalbatch = int(trainX.shape[0] / batch_size)\n",
    "training_data_count =len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e79292d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 2289)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalbatch, training_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff07e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "self._adj = []\n",
    "        self._adj.append(calculate_laplacian(adj))\n",
    "        self._activation = activation\n",
    "        self._gconv()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_units\n",
    "        return output_size\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "           \n",
    "    def _gconv(self,scope=None):\n",
    "        inputs = self._inputs       \n",
    "        inputs = tf.transpose(inputs, perm=[2,0,1])\n",
    "        x0 = tf.reshape(inputs,shape=[self._num_nodes,self._batch_size*self._input_dim])\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for adj in self._adj:\n",
    "                x1 = tf.sparse_tensor_dense_matmul(adj, x0)\n",
    "            x1 = tf.reshape(x1,shape=[self._num_nodes,self._batch_size,self._input_dim])\n",
    "            x1 = tf.reshape(x1,shape=[self._num_nodes*self._batch_size,self._input_dim])\n",
    "            \n",
    "            weights = weight_variable_glorot(self._input_dim, self.output_size, name='weights')\n",
    "            self.hidden1 = self._activation(tf.matmul(x1, weights))\n",
    "                                 \n",
    "            weights = weight_variable_glorot(self.output_size,self._output_dim, name='weights')\n",
    "            self.output = tf.matmul(self.hidden1, weights)\n",
    "            self.output = tf.reshape(self.output,shape=[self._num_nodes,self._batch_size,\n",
    "                                                        self._output_dim])\n",
    "            self.output = tf.transpose(self.output, perm=[1,2,0])\n",
    "            self.output = tf.reshape(self.output,shape=[-1,self._num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13c0a323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x2bedb804e08>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs = []\n",
    "adjs.append(calculate_laplacian(adj))\n",
    "adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7472704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 506)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca20c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68230a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03879b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1167494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8138eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035f087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cctv",
   "language": "python",
   "name": "cctv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
