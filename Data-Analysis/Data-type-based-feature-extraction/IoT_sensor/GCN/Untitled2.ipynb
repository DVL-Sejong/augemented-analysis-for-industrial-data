{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bd24237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from torch.nn.functional import normalize\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3eba1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    return 1 - torch.linalg.norm(y - pred, 'fro') / torch.linalg.norm(y, 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53d63c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, adj, input_dim, output_dim, **kwargs):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"laplacian\", calculate_laplacian_with_self_loop(torch.FloatTensor(adj))\n",
    "        )\n",
    "        \n",
    "        self._num_nodes = adj.shape[0]\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.FloatTensor(self._input_dim, self._output_dim)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weights, gain = nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = inputs.transpose(0, 2).transpose(1, 2)\n",
    "        inputs = inputs.reshape((self._num_nodes, batch_size * self._input_dim))\n",
    "        \n",
    "        ax = self.laplacian @ inputs\n",
    "        ax = ax.reshape((self._num_nodes, batch_size, self._input_dim))\n",
    "        ax = ax.reshape((self._num_nodes * batch_size, self._input_dim))\n",
    "        \n",
    "        outputs = torch.tanh(ax @ self.weights)\n",
    "        outputs = outputs.reshape((self._num_nodes, batch_size, self._output_dim))\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    @property\n",
    "    def hyperparameters(self):\n",
    "        return {\n",
    "            \"num_nodes\": self._num_nodes,\n",
    "            \"input_dim\": self._input_dim,\n",
    "            \"output_dim\": self._output_dim\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16fe7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laplacian_with_self_loop(matrix):\n",
    "    matrix = matrix + torch.eye(matrix.size(0))\n",
    "    row_sum = matrix.sum(1)\n",
    "    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
    "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    normalized_laplacian = (\n",
    "        matrix.matmul(d_mat_inv_sqrt).transpose(0,1).matmul(d_mat_inv_sqrt)\n",
    "    )\n",
    "    return normalized_laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ade99460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedForecastTask(pl.LightningModule):\n",
    "    def __init__(self, model, regressor='linear', loss='mse', pre_len=3, learning_rate=1e-3,\n",
    "                weight_decay=1.5e-3, feat_max_val=1.0, **kwargs):\n",
    "        super(SupervisedForecastTask, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.regressor = (\n",
    "            nn.Linear(\n",
    "                self.model.hyperparameters.get('output_dim'),\n",
    "                self.hparams.pre_len,\n",
    "            )\n",
    "            if regressor == 'linear'\n",
    "            else regressor\n",
    "        )\n",
    "        self._loss = loss\n",
    "        self.feat_max_val = feat_max_val\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, _, num_nodes = x.size()\n",
    "        hidden = self.model(x)\n",
    "        hidden = hidden.reshape((-1, hidden.size(2)))\n",
    "        \n",
    "        if self.regressor is not None:\n",
    "            predictions = self.regressor(hidden)\n",
    "        else:\n",
    "            predictions = hidden\n",
    "            \n",
    "        predictions = predictions.reshape((batch_size, num_nodes, -1))\n",
    "        return predictions\n",
    "    \n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        num_nodes = x.size(2)\n",
    "        predictions = self(x)\n",
    "        predicctions = predictions.transpose(1, 2).reshape((-1, num_nodes))\n",
    "        y = y.reshape((-1, y.size(2)))\n",
    "        return predictions, y\n",
    "    \n",
    "    def loss(self, inputs, targets):\n",
    "        return F.mse_loss(inputs, targets)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        predictions, y = self.shared_step(batch, batch_idx)\n",
    "        loss = self.loss(predictions, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        predictions, y = self.shared_step(batch, batch_idx)\n",
    "        predictions = predictions * self.feat_max_val\n",
    "        y = y * self.feat_max_val\n",
    "        loss = self.loss(predictions, y)\n",
    "        \n",
    "        rmse = torch.sqrt(torchmetrics.functional.mean_squared_error(predictions, y))\n",
    "        mae = torchmetrics.functional.mean_absolute_error(predictions, y)\n",
    "        accuracy = utils.metrics.accuracy(predictions, y)\n",
    "        mape = torchmetrics.functional.mean_absolute_percentage_error(predictions, y)\n",
    "        metrics = {\n",
    "            'val_loss':loss,\n",
    "            'RMSE':rmse,\n",
    "            'MAE':mae,\n",
    "            'accuracy':accuracy,\n",
    "            'MAPE':mape\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "        return predictions.reshape(batch[1].size()), y.reshape(batch[1].size())\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr = self.hparams.learning_rate,\n",
    "            weight_decay = self.hparams.weight_decay,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac26d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(feat_path, dtype=np.float32):\n",
    "    feat_df = pd.read_csv(feat_path)\n",
    "    feat_df = feat_df.drop('Unnamed: 0', axis=1)\n",
    "    feat = np.array(feat_df, dtype=dtype)\n",
    "    return feat\n",
    "\n",
    "def load_adjacency_matrix(adj_path, dtype=np.float32):\n",
    "    adj_df = pd.read_csv(adj_path, header=None)\n",
    "    adj = np.array(adj_df, dtype = dtype)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "24d9da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalCSVDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, feat_path, adj_path, batch_size=64,\n",
    "                seq_len = 12, pre_len = 3, split_ratio = 0.8, normalize=True):\n",
    "        super(SpatioTemporalCSVDataModule, self).__init__()\n",
    "        \n",
    "        self._feat_path = feat_path\n",
    "        self._adj_path = adj_path\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pre_len = pre_len\n",
    "        self.split_ratio = split_ratio\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self._feat = load_features(self._feat_path)\n",
    "        self._feat_max_val = np.max(self._feat)\n",
    "        self._adj = load_adjacency_matrix(self._adj_path)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        (self.train_dataset, self.val_dataset) = generate_torch_datasets(\n",
    "            self._feat, self.seq_len, self.pre_len, \n",
    "            split_ratio=self.split_ratio, normalize=self.normalize\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=len(self.val_dataset))\n",
    "    \n",
    "    @property\n",
    "    def feat_max_val(self):\n",
    "        return self._feat_max_val\n",
    "\n",
    "    @property\n",
    "    def adj(self):\n",
    "        return self._adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1d75fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
    "):\n",
    "    if time_len is None:\n",
    "        time_len = data.shape[0]\n",
    "    if normalize:\n",
    "        max_val = np.max(data)\n",
    "        data = data / max_val\n",
    "    train_size = int(time_len * split_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:time_len]\n",
    "    train_X, train_Y, test_X, test_Y = list(), list(), list(), list()\n",
    "    for i in range(len(train_data) - seq_len - pre_len):\n",
    "        train_X.append(np.array(train_data[i : i + seq_len]))\n",
    "        train_Y.append(np.array(train_data[i + seq_len : i + seq_len + pre_len]))\n",
    "    for i in range(len(test_data) - seq_len - pre_len):\n",
    "        test_X.append(np.array(test_data[i : i + seq_len]))\n",
    "        test_Y.append(np.array(test_data[i + seq_len : i + seq_len + pre_len]))\n",
    "    return np.array(train_X), np.array(train_Y), np.array(test_X), np.array(test_Y)\n",
    "\n",
    "\n",
    "def generate_torch_datasets(\n",
    "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
    "):\n",
    "    train_X, train_Y, test_X, test_Y = generate_dataset(\n",
    "        data,\n",
    "        seq_len,\n",
    "        pre_len,\n",
    "        time_len=time_len,\n",
    "        split_ratio=split_ratio,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(train_X), torch.FloatTensor(train_Y)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(test_X), torch.FloatTensor(test_Y)\n",
    "    )\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6be3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_path = '../Data/METR-LA/speed_la_0.csv'\n",
    "adj_path = '../Data/METR-LA/adj_mx_la.csv'\n",
    "seq_len = 12\n",
    "pre_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0258035b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vfgtr554\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\vfgtr554\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:89: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type   | Params\n",
      "-------------------------------------\n",
      "0 | model     | GCN    | 768   \n",
      "1 | regressor | Linear | 195   \n",
      "-------------------------------------\n",
      "963       Trainable params\n",
      "0         Non-trainable params\n",
      "963       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c1b4a252c84a3697e57d46c57b7a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vfgtr554\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\vfgtr554\\AppData\\Local\\Temp\\ipykernel_19480\\880312603.py:40: UserWarning: Using a target size (torch.Size([20520, 207])) that is different to the input size (torch.Size([6840, 207, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(inputs, targets)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (207) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m task \u001b[38;5;241m=\u001b[39m SupervisedForecastTask(model\u001b[38;5;241m=\u001b[39mmodel, feat_max_val \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mfeat_max_val)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate(datamodule\u001b[38;5;241m=\u001b[39mdm)\n\u001b[0;32m      7\u001b[0m results\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    731\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    733\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m )\n\u001b[1;32m--> 735\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1166\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1343\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 240\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1704\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:370\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mSupervisedForecastTask.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     49\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_max_val\n\u001b[0;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_max_val\n\u001b[1;32m---> 51\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m rmse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmean_squared_error(predictions, y))\n\u001b[0;32m     54\u001b[0m mae \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmean_absolute_error(predictions, y)\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mSupervisedForecastTask.loss\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3279\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3277\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3279\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (207) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "dm = SpatioTemporalCSVDataModule(feat_path = feat_path, adj_path = adj_path)\n",
    "model = GCN(adj=dm.adj, input_dim=seq_len, output_dim = 64)\n",
    "task = SupervisedForecastTask(model=model, feat_max_val = dm.feat_max_val)\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(task, dm)\n",
    "results = trainer.validate(datamodule=dm)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f4becfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34272, 208)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_features(feat_path).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d4e1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34271.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(load_features(feat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51f04a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16990560"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "207 * 82080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af6bf540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7094304"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "207 * 34272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f131f914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34267</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.888889</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>32.833333</td>\n",
       "      <td>54.555556</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>59.222222</td>\n",
       "      <td>65.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>52.888889</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>66.333333</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>66.777778</td>\n",
       "      <td>64.888889</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>62.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34268</th>\n",
       "      <td>61.375000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>32.833333</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>65.250000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>69.250000</td>\n",
       "      <td>60.125000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>67.250000</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34269</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>69.555556</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>32.833333</td>\n",
       "      <td>44.777778</td>\n",
       "      <td>64.222222</td>\n",
       "      <td>63.777778</td>\n",
       "      <td>59.777778</td>\n",
       "      <td>57.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>67.888889</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>67.444444</td>\n",
       "      <td>61.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34270</th>\n",
       "      <td>66.750000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>32.833333</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>64.125000</td>\n",
       "      <td>60.875000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>51.125000</td>\n",
       "      <td>69.375000</td>\n",
       "      <td>61.625000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>66.375000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.875000</td>\n",
       "      <td>63.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34271</th>\n",
       "      <td>65.111111</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>66.777778</td>\n",
       "      <td>61.222222</td>\n",
       "      <td>32.833333</td>\n",
       "      <td>49.555556</td>\n",
       "      <td>65.777778</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>67.444444</td>\n",
       "      <td>64.888889</td>\n",
       "      <td>60.888889</td>\n",
       "      <td>64.222222</td>\n",
       "      <td>66.444444</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>61.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34272 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          773869     767541     767542     717447     717446     717445  \\\n",
       "0      64.375000  67.625000  67.125000  61.500000  66.875000  68.750000   \n",
       "1      62.666667  68.555556  65.444444  62.444444  64.444444  68.111111   \n",
       "2      64.000000  63.750000  60.000000  59.000000  66.500000  66.250000   \n",
       "3      64.000000  63.750000  60.000000  59.000000  66.500000  66.250000   \n",
       "4      64.000000  63.750000  60.000000  59.000000  66.500000  66.250000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "34267  65.000000  65.888889  68.555556  61.666667  32.833333  54.555556   \n",
       "34268  61.375000  65.625000  66.500000  62.750000  32.833333  50.500000   \n",
       "34269  67.000000  59.666667  69.555556  61.000000  32.833333  44.777778   \n",
       "34270  66.750000  62.250000  66.000000  59.625000  32.833333  53.000000   \n",
       "34271  65.111111  66.888889  66.777778  61.222222  32.833333  49.555556   \n",
       "\n",
       "          773062     767620     737529     717816  ...     772167     769372  \\\n",
       "0      65.125000  67.125000  59.625000  62.750000  ...  45.625000  65.500000   \n",
       "1      65.000000  65.000000  57.444444  63.333333  ...  50.666667  69.875000   \n",
       "2      64.500000  64.250000  63.875000  65.375000  ...  44.125000  69.000000   \n",
       "3      64.500000  64.250000  63.875000  65.375000  ...  44.125000  69.000000   \n",
       "4      64.500000  64.250000  63.875000  65.375000  ...  44.125000  69.000000   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "34267  62.444444  63.333333  59.222222  65.333333  ...  52.888889  69.000000   \n",
       "34268  62.000000  67.000000  65.250000  67.125000  ...  54.000000  69.250000   \n",
       "34269  64.222222  63.777778  59.777778  57.666667  ...  51.333333  67.888889   \n",
       "34270  64.285714  64.125000  60.875000  66.250000  ...  51.125000  69.375000   \n",
       "34271  65.777778  65.111111  63.000000  61.666667  ...  56.000000  67.444444   \n",
       "\n",
       "          774204     769806     717590     717592     717595     772168  \\\n",
       "0      64.500000  66.428571  66.875000  59.375000  69.000000  59.250000   \n",
       "1      66.666667  58.555556  62.000000  61.111111  64.444444  55.888889   \n",
       "2      56.500000  59.250000  68.125000  62.500000  65.625000  61.375000   \n",
       "3      56.500000  59.250000  68.125000  62.500000  65.625000  61.375000   \n",
       "4      56.500000  59.250000  68.125000  62.500000  65.625000  61.375000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "34267  65.111111  55.666667  66.333333  62.444444  66.777778  64.888889   \n",
       "34268  60.125000  60.500000  67.250000  59.375000  66.000000  61.250000   \n",
       "34269  64.333333  57.000000  66.000000  62.666667  68.666667  63.333333   \n",
       "34270  61.625000  60.500000  65.625000  66.375000  69.500000  63.000000   \n",
       "34271  64.888889  60.888889  64.222222  66.444444  68.444444  63.555556   \n",
       "\n",
       "          718141     769373  \n",
       "0      69.000000  61.875000  \n",
       "1      68.444444  62.875000  \n",
       "2      69.857143  62.000000  \n",
       "3      69.857143  62.000000  \n",
       "4      69.857143  62.000000  \n",
       "...          ...        ...  \n",
       "34267  69.666667  62.333333  \n",
       "34268  69.000000  62.000000  \n",
       "34269  67.444444  61.222222  \n",
       "34270  67.875000  63.500000  \n",
       "34271  68.666667  61.777778  \n",
       "\n",
       "[34272 rows x 207 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df = pd.read_csv(feat_path)\n",
    "feat_df = feat_df.drop('Unnamed: 0', axis=1)\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e383d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
