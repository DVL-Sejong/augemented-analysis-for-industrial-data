{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daeaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6024436",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "A_inv_sqrt = sqrtm(A_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a09c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10306396+1.24742804j, 0.1502082 -0.5706074j ],\n",
       "       [0.22531231-0.8559111j , 0.32837626+0.39151694j]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inv_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7278c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5536886+0.46439415j, 0.8069607-0.21242648j],\n",
       "       [1.2104411-0.31863973j, 1.7641296+0.14575444j]], dtype=complex64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sqrt = sqrtm(A)\n",
    "A_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f643d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69fcddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2880, 506), (506, 506))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('speed_gangnam_0.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('adj_mx_gangnam.pkl', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='CP949')\n",
    "adj = data[2]\n",
    "\n",
    "df.shape, adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46890ed9",
   "metadata": {},
   "source": [
    "### Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6f3094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = df.values\n",
    "kernel = adj\n",
    "\n",
    "#image = np.random.random((5, 5))\n",
    "#kernel = np.random.random((3, 3))\n",
    "\n",
    "image_height, image_width = image.shape[0], image.shape[1]\n",
    "kernel_height, kernel_width = kernel.shape[0], kernel.shape[1]\n",
    "\n",
    "padding = 0\n",
    "strides = 1\n",
    "\n",
    "output_height = int(((image_height - kernel_height + 2 * padding) / strides) + 1)\n",
    "output_width = int(((image_width - kernel_width + 2 * padding) / strides) + 1)\n",
    "\n",
    "output = np.zeros((output_height, output_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3d67f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePadded = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96e2e0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 506)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePadded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a6ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4bbe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(0, output_height):\n",
    "    if (y * strides + kernel_height) <= imagePadded.shape[0]:\n",
    "        for x in range(0, output_width):\n",
    "            if (x * strides + kernel_width) <= imagePadded.shape[1]:\n",
    "                #print(y*strides, x*strides, y*strides+kernel_height, x*strides + kernel_width)\n",
    "                output[y][x] = np.sum(\n",
    "                    imagePadded[y*strides: y*strides + kernel_height,\n",
    "                                x*strides: x*strides + kernel_width] \n",
    "                    * kernel).astype(np.float32)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43b176c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[92573.203125 ],\n",
       "       [92522.5390625],\n",
       "       [92240.7734375],\n",
       "       ...,\n",
       "       [80673.4375   ],\n",
       "       [79964.96875  ],\n",
       "       [80014.125    ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39534819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195880733.90625"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4482566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e14a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b3394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef306cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionGraphConv(BaseModel):\n",
    "    def __init__(self, supports, input_dim, hid_dim, num_nodes, max_diffusion_step, output_dim, bias_start=0.0):\n",
    "        super(DiffusionGraphConv, self).__init__()\n",
    "        self.num_matrices = len(supports) * max_diffusion_step + 1\n",
    "        input_size = input_dim + hid_dim\n",
    "        self._num_nodes = num_nodes\n",
    "        self._max_diffusion_step = max_diffusion_step\n",
    "        self._supports = supports\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(size=(input_size*self.num_matrices, output_dim)))\n",
    "        self.biases = nn.Parameter(torch.FloatTensor(size=(output_dim,)))\n",
    "        nn.init.xavier_normal_(self.weight.data, gain=1.414)\n",
    "        nn.init.constant_(self.biases.data, val=bias_start)\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = torch.unsqueeze(x_, 0)\n",
    "        return torch.cat([x, x_], dim=0)\n",
    "\n",
    "    def forward(self, inputs, state, output_size, bias_start=0.0):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = torch.reshape(inputs, (batch_size, self._num_nodes, -1))\n",
    "        state = torch.reshape(state, (batch_size, self._num_nodes, -1))\n",
    "        inputs_and_state = torch.cat([inputs, state], dim=2)\n",
    "        input_size = inputs_and_state.shape[2]\n",
    "\n",
    "        x = inputs_and_state\n",
    "        x0 = torch.transpose(x, dim0=0, dim1=1)\n",
    "        x0 = torch.transpose(x0, dim0=1, dim1=2)\n",
    "        x0 = torch.reshape(x0, shape=[self._num_nodes, input_size * batch_size])\n",
    "        x = torch.unsqueeze(x0, dim=0)\n",
    "\n",
    "        if self._max_diffusion_step == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for support in self._supports:\n",
    "                x1 = torch.sparse.mm(support, x0)\n",
    "                x = self._concat(x, x1)\n",
    "                for k in range(2, self._max_diffusion_step + 1):\n",
    "                    x2 = 2 * torch.sparse.mm(support, x1) - x0\n",
    "                    x = self._concat(x, x2)\n",
    "                    x1, x0 = x2, x1\n",
    "\n",
    "        x = torch.reshape(x, shape=[self.num_matrices, self._num_nodes, input_size, batch_size])\n",
    "        x = torch.transpose(x, dim0=0, dim1=3)\n",
    "        x = torch.reshape(x, shape=[batch_size * self._num_nodes, input_size * self.num_matrices])\n",
    "\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.add(x, self.biases)\n",
    "        \n",
    "        return torch.reshape(x, [batch_size, self._num_nodes * output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d1713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_of_vertices, num_of_features):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "        self.W2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_features)) # (F, T)\n",
    "        self.W3 = nn.Parameter(torch.FloatTensor(in_channels)) # (F)\n",
    "\n",
    "        self.Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices)) # (N, N)\n",
    "        self.bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices)) # (1, N, N)\n",
    "\n",
    "    def forward(self, x): #X: (B, N, F, T)\n",
    "        xw = torch.matmul(x, self.W1) # (B, N, F, T) (T) -> (B, N, F)\n",
    "        txw = torch.matmul(self.W3, x).transpose(-1, -2) # (F) (B, N, F, T) -> (B, N, T) -> (B, T, N)\n",
    "        product = torch.matmul(torch.matmul(xw, self.W2), txw) # (B, N, F) (F, T) -> (B, N, T) (B, T, N) -> (B, N, N)\n",
    "\n",
    "        sig = torch.sigmoid(product + self.bs) \n",
    "        S = torch.matmul(self.Vs, sig) # (N, N) (B, N, N) -> (B, N, N)\n",
    "        S_prime = F.softmax(S, dim=1)\n",
    "        \n",
    "        return S_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472d6551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1210007700</th>\n",
       "      <th>1210008500</th>\n",
       "      <th>1210009500</th>\n",
       "      <th>1210010300</th>\n",
       "      <th>1210011300</th>\n",
       "      <th>1210013700</th>\n",
       "      <th>1220011900</th>\n",
       "      <th>1220016300</th>\n",
       "      <th>1220021100</th>\n",
       "      <th>1220025100</th>\n",
       "      <th>...</th>\n",
       "      <th>1210013800</th>\n",
       "      <th>1210013000</th>\n",
       "      <th>1210012200</th>\n",
       "      <th>1210012300</th>\n",
       "      <th>1210013100</th>\n",
       "      <th>1210013900</th>\n",
       "      <th>1210014900</th>\n",
       "      <th>1210015900</th>\n",
       "      <th>1210017100</th>\n",
       "      <th>1210018300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.70</td>\n",
       "      <td>21.00</td>\n",
       "      <td>33.54</td>\n",
       "      <td>43.21</td>\n",
       "      <td>24.31</td>\n",
       "      <td>26.54</td>\n",
       "      <td>35.91</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.78</td>\n",
       "      <td>21.76</td>\n",
       "      <td>...</td>\n",
       "      <td>24.98</td>\n",
       "      <td>24.54</td>\n",
       "      <td>32.76</td>\n",
       "      <td>43.75</td>\n",
       "      <td>32.36</td>\n",
       "      <td>32.13</td>\n",
       "      <td>35.47</td>\n",
       "      <td>22.96</td>\n",
       "      <td>26.62</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.50</td>\n",
       "      <td>38.00</td>\n",
       "      <td>25.61</td>\n",
       "      <td>43.89</td>\n",
       "      <td>22.75</td>\n",
       "      <td>24.37</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.07</td>\n",
       "      <td>25.92</td>\n",
       "      <td>22.66</td>\n",
       "      <td>...</td>\n",
       "      <td>26.22</td>\n",
       "      <td>25.14</td>\n",
       "      <td>32.15</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>35.04</td>\n",
       "      <td>26.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>27.22</td>\n",
       "      <td>29.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>24.36</td>\n",
       "      <td>35.75</td>\n",
       "      <td>24.63</td>\n",
       "      <td>21.96</td>\n",
       "      <td>22.67</td>\n",
       "      <td>...</td>\n",
       "      <td>21.55</td>\n",
       "      <td>25.16</td>\n",
       "      <td>34.37</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>36.32</td>\n",
       "      <td>22.42</td>\n",
       "      <td>32.40</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.00</td>\n",
       "      <td>34.39</td>\n",
       "      <td>32.70</td>\n",
       "      <td>44.15</td>\n",
       "      <td>23.64</td>\n",
       "      <td>27.76</td>\n",
       "      <td>40.06</td>\n",
       "      <td>41.00</td>\n",
       "      <td>28.65</td>\n",
       "      <td>21.68</td>\n",
       "      <td>...</td>\n",
       "      <td>25.33</td>\n",
       "      <td>24.11</td>\n",
       "      <td>33.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>29.88</td>\n",
       "      <td>34.93</td>\n",
       "      <td>37.35</td>\n",
       "      <td>21.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>32.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.20</td>\n",
       "      <td>37.65</td>\n",
       "      <td>30.75</td>\n",
       "      <td>43.66</td>\n",
       "      <td>22.76</td>\n",
       "      <td>28.04</td>\n",
       "      <td>28.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.92</td>\n",
       "      <td>...</td>\n",
       "      <td>24.44</td>\n",
       "      <td>24.15</td>\n",
       "      <td>34.94</td>\n",
       "      <td>28.11</td>\n",
       "      <td>33.88</td>\n",
       "      <td>33.53</td>\n",
       "      <td>39.30</td>\n",
       "      <td>30.68</td>\n",
       "      <td>27.68</td>\n",
       "      <td>30.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>27.50</td>\n",
       "      <td>36.33</td>\n",
       "      <td>29.53</td>\n",
       "      <td>39.86</td>\n",
       "      <td>26.15</td>\n",
       "      <td>20.05</td>\n",
       "      <td>25.37</td>\n",
       "      <td>18.27</td>\n",
       "      <td>21.88</td>\n",
       "      <td>18.25</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>24.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>21.95</td>\n",
       "      <td>29.47</td>\n",
       "      <td>28.47</td>\n",
       "      <td>31.52</td>\n",
       "      <td>32.13</td>\n",
       "      <td>28.22</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>25.51</td>\n",
       "      <td>37.62</td>\n",
       "      <td>30.48</td>\n",
       "      <td>41.88</td>\n",
       "      <td>26.52</td>\n",
       "      <td>23.00</td>\n",
       "      <td>31.94</td>\n",
       "      <td>16.56</td>\n",
       "      <td>24.29</td>\n",
       "      <td>19.14</td>\n",
       "      <td>...</td>\n",
       "      <td>28.65</td>\n",
       "      <td>24.49</td>\n",
       "      <td>32.52</td>\n",
       "      <td>27.44</td>\n",
       "      <td>31.76</td>\n",
       "      <td>33.00</td>\n",
       "      <td>44.68</td>\n",
       "      <td>24.89</td>\n",
       "      <td>33.04</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>29.40</td>\n",
       "      <td>28.26</td>\n",
       "      <td>31.81</td>\n",
       "      <td>39.68</td>\n",
       "      <td>26.45</td>\n",
       "      <td>24.28</td>\n",
       "      <td>28.92</td>\n",
       "      <td>18.00</td>\n",
       "      <td>23.41</td>\n",
       "      <td>21.11</td>\n",
       "      <td>...</td>\n",
       "      <td>32.26</td>\n",
       "      <td>23.88</td>\n",
       "      <td>32.02</td>\n",
       "      <td>26.94</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.10</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.70</td>\n",
       "      <td>30.21</td>\n",
       "      <td>18.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>24.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>29.02</td>\n",
       "      <td>41.93</td>\n",
       "      <td>25.49</td>\n",
       "      <td>25.38</td>\n",
       "      <td>25.57</td>\n",
       "      <td>17.61</td>\n",
       "      <td>23.63</td>\n",
       "      <td>21.70</td>\n",
       "      <td>...</td>\n",
       "      <td>27.08</td>\n",
       "      <td>27.32</td>\n",
       "      <td>34.00</td>\n",
       "      <td>26.27</td>\n",
       "      <td>27.32</td>\n",
       "      <td>29.02</td>\n",
       "      <td>37.90</td>\n",
       "      <td>27.58</td>\n",
       "      <td>30.23</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>31.77</td>\n",
       "      <td>38.30</td>\n",
       "      <td>30.65</td>\n",
       "      <td>39.34</td>\n",
       "      <td>34.20</td>\n",
       "      <td>23.70</td>\n",
       "      <td>30.08</td>\n",
       "      <td>15.28</td>\n",
       "      <td>27.10</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>21.30</td>\n",
       "      <td>32.74</td>\n",
       "      <td>33.75</td>\n",
       "      <td>27.22</td>\n",
       "      <td>32.43</td>\n",
       "      <td>39.29</td>\n",
       "      <td>29.04</td>\n",
       "      <td>34.64</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1210007700  1210008500  1210009500  1210010300  1210011300  1210013700  \\\n",
       "0          32.70       21.00       33.54       43.21       24.31       26.54   \n",
       "1          31.50       38.00       25.61       43.89       22.75       24.37   \n",
       "2          22.00       38.00       47.00       54.00       19.00       24.36   \n",
       "3          31.00       34.39       32.70       44.15       23.64       27.76   \n",
       "4          39.20       37.65       30.75       43.66       22.76       28.04   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       27.50       36.33       29.53       39.86       26.15       20.05   \n",
       "2876       25.51       37.62       30.48       41.88       26.52       23.00   \n",
       "2877       29.40       28.26       31.81       39.68       26.45       24.28   \n",
       "2878       24.00       28.21       29.02       41.93       25.49       25.38   \n",
       "2879       31.77       38.30       30.65       39.34       34.20       23.70   \n",
       "\n",
       "      1220011900  1220016300  1220021100  1220025100  ...  1210013800  \\\n",
       "0          35.91       29.00       28.78       21.76  ...       24.98   \n",
       "1          30.73       29.07       25.92       22.66  ...       26.22   \n",
       "2          35.75       24.63       21.96       22.67  ...       21.55   \n",
       "3          40.06       41.00       28.65       21.68  ...       25.33   \n",
       "4          28.10       23.25       23.00       19.92  ...       24.44   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2875       25.37       18.27       21.88       18.25  ...       28.17   \n",
       "2876       31.94       16.56       24.29       19.14  ...       28.65   \n",
       "2877       28.92       18.00       23.41       21.11  ...       32.26   \n",
       "2878       25.57       17.61       23.63       21.70  ...       27.08   \n",
       "2879       30.08       15.28       27.10       19.33  ...       26.68   \n",
       "\n",
       "      1210013000  1210012200  1210012300  1210013100  1210013900  1210014900  \\\n",
       "0          24.54       32.76       43.75       32.36       32.13       35.47   \n",
       "1          25.14       32.15       43.75       25.00       35.04       26.00   \n",
       "2          25.16       34.37       43.75       25.00       33.00       36.32   \n",
       "3          24.11       33.41       28.61       29.88       34.93       37.35   \n",
       "4          24.15       34.94       28.11       33.88       33.53       39.30   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       24.28       38.21       21.95       29.47       28.47       31.52   \n",
       "2876       24.49       32.52       27.44       31.76       33.00       44.68   \n",
       "2877       23.88       32.02       26.94       30.75       31.10       17.00   \n",
       "2878       27.32       34.00       26.27       27.32       29.02       37.90   \n",
       "2879       21.30       32.74       33.75       27.22       32.43       39.29   \n",
       "\n",
       "      1210015900  1210017100  1210018300  \n",
       "0          22.96       26.62       14.00  \n",
       "1          16.00       27.22       29.58  \n",
       "2          22.42       32.40       11.00  \n",
       "3          21.35       29.07       32.34  \n",
       "4          30.68       27.68       30.03  \n",
       "...          ...         ...         ...  \n",
       "2875       32.13       28.22       15.96  \n",
       "2876       24.89       33.04       11.00  \n",
       "2877       24.70       30.21       18.65  \n",
       "2878       27.58       30.23       15.41  \n",
       "2879       29.04       34.64       15.30  \n",
       "\n",
       "[2880 rows x 506 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('speed_gangnam_0.csv').drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb9d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab51e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dc6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0990e6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 506)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = np.ones((506, 506))\n",
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4b76c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0782],\n",
       "        [-0.0455],\n",
       "        [-0.0232],\n",
       "        ...,\n",
       "        [-0.0206],\n",
       "        [-0.0935],\n",
       "        [-0.0994]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "in_features = 506\n",
    "out_features = 506\n",
    "\n",
    "W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc28b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakyrelu = nn.LeakyReLU(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fcd9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.values\n",
    "h = torch.FloatTensor(h)\n",
    "Wh = torch.mm(h, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e87475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33137810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:out_features, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e2515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh1 = torch.matmul(Wh, a[:out_features, :])\n",
    "Wh2 = torch.matmul(Wh, a[out_features:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "977ce621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 2880])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Wh1 + Wh2.T\n",
    "e = leakyrelu(e)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d17de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.FloatTensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd251226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 506])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e7aa6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (506) must match the size of tensor b (2880) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m zero_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9e15\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(e)\n\u001b[1;32m----> 2\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_vec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (506) must match the size of tensor b (2880) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "zero_vec = -9e15 * torch.ones_like(e)\n",
    "attention = torch.where(adj > 0, e, zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d350d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prime = torch.matmul(zero_vec, Wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce83db42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f379f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        ...,\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c25a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.7000, 21.0000, 33.5400,  ..., 22.9600, 26.6200, 14.0000],\n",
       "        [31.5000, 38.0000, 25.6100,  ..., 16.0000, 27.2200, 29.5800],\n",
       "        [22.0000, 38.0000, 47.0000,  ..., 22.4200, 32.4000, 11.0000],\n",
       "        ...,\n",
       "        [29.4000, 28.2600, 31.8100,  ..., 24.7000, 30.2100, 18.6500],\n",
       "        [24.0000, 28.2100, 29.0200,  ..., 27.5800, 30.2300, 15.4100],\n",
       "        [31.7700, 38.3000, 30.6500,  ..., 29.0400, 34.6400, 15.3000]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea05f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b3c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 207\n",
    "num_of_features = 2880\n",
    "num_of_vertices = 207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a38b8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "W2 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "\n",
    "Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices)) # (N, N)\n",
    "bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc7c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2880]),\n",
       " torch.Size([207, 2880]),\n",
       " torch.Size([207]),\n",
       " torch.Size([207, 207]),\n",
       " torch.Size([1, 207, 207]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape, W2.shape, W3.shape, Vs.shape, bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ff6380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 2880])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor(traffic)\n",
    "x = torch.transpose(x, -1, -2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c804ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 506, 2880])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_prime = x.repeat(16, 1).reshape(-1, 506, 2880)\n",
    "x_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "474c3f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 506])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xw = torch.matmul(x_prime, W1)\n",
    "xw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "050ee51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 16, 16x506,2880",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xw \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m xw\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, got 16, 16x506,2880"
     ]
    }
   ],
   "source": [
    "xw = torch.matmul(xw, W2)\n",
    "xw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e43a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54672aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xw = torch.matmul(x, self.W1) # (B, N, F, T) (T) -> (B, N, F)\n",
    "        txw = torch.matmul(self.W3, x).transpose(-1, -2) # (F) (B, N, F, T) -> (B, N, T) -> (B, T, N)\n",
    "        product = torch.matmul(torch.matmul(xw, self.W2), txw) # (B, N, F) (F, T) -> (B, N, T) (B, T, N) -> (B, N, N)\n",
    "\n",
    "        sig = torch.sigmoid(product + self.bs) \n",
    "        S = torch.matmul(self.Vs, sig) # (N, N) (B, N, N) -> (B, N, N)\n",
    "        S_prime = F.softmax(S, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a568f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7153422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0488,  0.0575, -0.0026,  ...,  0.0157, -0.1635,  0.0107]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = 506\n",
    "out_features = 506\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "nn.init.xavier_normal_(W.data, gain=1.414)\n",
    "\n",
    "a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "nn.init.xavier_normal_(a.data, gain=1.414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ddbbf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.FloatTensor(df.values)\n",
    "N = h.size()[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "504ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.ones((506, 506))\n",
    "adj = torch.FloatTensor(adj)\n",
    "edge = adj.nonzero().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e50617e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.mm(h, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4111d987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20c4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11f1f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakyrelu = nn.LeakyReLU(alpha)\n",
    "edge_e = torch.exp(-leakyrelu(a.mm(edge_h).squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbfb6319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256036]), torch.Size([256036]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge.shape, edge_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71c3ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a6dfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_spmm = SpecialSpmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3442180",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_rowsum = special_spmm(edge, edge_e, torch.Size([N,N]), torch.ones(size=(N,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c91df48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 959.4462],\n",
       "        [ 286.5499],\n",
       "        [1987.9010],\n",
       "        ...,\n",
       "        [   0.0000],\n",
       "        [   0.0000],\n",
       "        [   0.0000]], grad_fn=<SpecialSpmmFunctionBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_rowsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68a92358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime = special_spmm(edge, edge_e, torch.Size([N,N]), h)\n",
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ff73a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20.7039, -22.8317,  45.6013,  ..., -18.8642, -19.1760, -36.4217],\n",
       "        [-20.7096, -22.8344,  45.6733,  ..., -18.9175, -19.1660, -36.4120],\n",
       "        [-20.7017, -22.8398,  45.5797,  ..., -18.8467, -19.1660, -36.4197],\n",
       "        ...,\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime = h_prime.div(e_rowsum)\n",
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "814425d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1201244)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "958a9635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5ce54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457280"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape[0] * h_prime.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cbbeb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243055555555555"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1201244 / 1457280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e4194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
