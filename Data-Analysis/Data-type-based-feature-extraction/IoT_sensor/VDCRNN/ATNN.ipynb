{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daeaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6024436",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "A_inv_sqrt = sqrtm(A_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a09c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10306396+1.24742804j, 0.1502082 -0.5706074j ],\n",
       "       [0.22531231-0.8559111j , 0.32837626+0.39151694j]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inv_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7278c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5536886+0.46439415j, 0.8069607-0.21242648j],\n",
       "       [1.2104411-0.31863973j, 1.7641296+0.14575444j]], dtype=complex64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sqrt = sqrtm(A)\n",
    "A_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cafed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04546e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2880, 506), (506, 506))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('speed_gangnam_0.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('adj_mx_gangnam.pkl', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='CP949')\n",
    "adj = data[2]\n",
    "\n",
    "df.shape, adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca2490",
   "metadata": {},
   "source": [
    "### Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d82cc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = df.values\n",
    "kernel = adj\n",
    "\n",
    "#image = np.random.random((5, 5))\n",
    "#kernel = np.random.random((3, 3))\n",
    "\n",
    "image_height, image_width = image.shape[0], image.shape[1]\n",
    "kernel_height, kernel_width = kernel.shape[0], kernel.shape[1]\n",
    "\n",
    "padding = 0\n",
    "strides = 1\n",
    "\n",
    "output_height = int(((image_height - kernel_height + 2 * padding) / strides) + 1)\n",
    "output_width = int(((image_width - kernel_width + 2 * padding) / strides) + 1)\n",
    "\n",
    "output = np.zeros((output_height, output_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fb35308",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePadded = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b15021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 506)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePadded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "396175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(0, output_height):\n",
    "    if (y * strides + kernel_height) <= imagePadded.shape[0]:\n",
    "        for x in range(0, output_width):\n",
    "            if (x * strides + kernel_width) <= imagePadded.shape[1]:\n",
    "                #print(y*strides, x*strides, y*strides+kernel_height, x*strides + kernel_width)\n",
    "                output[y][x] = np.sum(\n",
    "                    imagePadded[y*strides: y*strides + kernel_height,\n",
    "                                x*strides: x*strides + kernel_width] \n",
    "                    * kernel).astype(np.float32)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53552875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[92573.203125 ],\n",
       "       [92522.5390625],\n",
       "       [92240.7734375],\n",
       "       ...,\n",
       "       [80673.4375   ],\n",
       "       [79964.96875  ],\n",
       "       [80014.125    ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e5486e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.40327677, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.40327677, 1.        , 0.91133416, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.91133416, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.38495174,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.38495174, 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed232e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195880733.90625"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a6879414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[273.16220448, 202.56419772, 167.72485007, ..., 303.57415218,\n",
       "        228.84379668, 199.02047791],\n",
       "       [249.66616005, 189.45193931, 156.64285913, ..., 281.16564069,\n",
       "        228.37975824, 242.9986063 ],\n",
       "       [243.86885553, 202.736215  , 185.90254924, ..., 279.41321781,\n",
       "        241.39661813, 220.37047939],\n",
       "       ...,\n",
       "       [235.83328419, 189.56337791, 162.86751901, ..., 282.54185251,\n",
       "        226.08895569, 219.79172238],\n",
       "       [233.76795502, 184.6679757 , 160.88231704, ..., 269.77866633,\n",
       "        218.6070577 , 212.47407996],\n",
       "       [240.38100303, 195.13222251, 167.55241826, ..., 294.67956451,\n",
       "        233.72052721, 227.11547766]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(df, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fd1d68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1210007700</th>\n",
       "      <th>1210008500</th>\n",
       "      <th>1210009500</th>\n",
       "      <th>1210010300</th>\n",
       "      <th>1210011300</th>\n",
       "      <th>1210013700</th>\n",
       "      <th>1220011900</th>\n",
       "      <th>1220016300</th>\n",
       "      <th>1220021100</th>\n",
       "      <th>1220025100</th>\n",
       "      <th>...</th>\n",
       "      <th>1210013800</th>\n",
       "      <th>1210013000</th>\n",
       "      <th>1210012200</th>\n",
       "      <th>1210012300</th>\n",
       "      <th>1210013100</th>\n",
       "      <th>1210013900</th>\n",
       "      <th>1210014900</th>\n",
       "      <th>1210015900</th>\n",
       "      <th>1210017100</th>\n",
       "      <th>1210018300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.70</td>\n",
       "      <td>21.00</td>\n",
       "      <td>33.54</td>\n",
       "      <td>43.21</td>\n",
       "      <td>24.31</td>\n",
       "      <td>26.54</td>\n",
       "      <td>35.91</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.78</td>\n",
       "      <td>21.76</td>\n",
       "      <td>...</td>\n",
       "      <td>24.98</td>\n",
       "      <td>24.54</td>\n",
       "      <td>32.76</td>\n",
       "      <td>43.75</td>\n",
       "      <td>32.36</td>\n",
       "      <td>32.13</td>\n",
       "      <td>35.47</td>\n",
       "      <td>22.96</td>\n",
       "      <td>26.62</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.50</td>\n",
       "      <td>38.00</td>\n",
       "      <td>25.61</td>\n",
       "      <td>43.89</td>\n",
       "      <td>22.75</td>\n",
       "      <td>24.37</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.07</td>\n",
       "      <td>25.92</td>\n",
       "      <td>22.66</td>\n",
       "      <td>...</td>\n",
       "      <td>26.22</td>\n",
       "      <td>25.14</td>\n",
       "      <td>32.15</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>35.04</td>\n",
       "      <td>26.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>27.22</td>\n",
       "      <td>29.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>24.36</td>\n",
       "      <td>35.75</td>\n",
       "      <td>24.63</td>\n",
       "      <td>21.96</td>\n",
       "      <td>22.67</td>\n",
       "      <td>...</td>\n",
       "      <td>21.55</td>\n",
       "      <td>25.16</td>\n",
       "      <td>34.37</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>36.32</td>\n",
       "      <td>22.42</td>\n",
       "      <td>32.40</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.00</td>\n",
       "      <td>34.39</td>\n",
       "      <td>32.70</td>\n",
       "      <td>44.15</td>\n",
       "      <td>23.64</td>\n",
       "      <td>27.76</td>\n",
       "      <td>40.06</td>\n",
       "      <td>41.00</td>\n",
       "      <td>28.65</td>\n",
       "      <td>21.68</td>\n",
       "      <td>...</td>\n",
       "      <td>25.33</td>\n",
       "      <td>24.11</td>\n",
       "      <td>33.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>29.88</td>\n",
       "      <td>34.93</td>\n",
       "      <td>37.35</td>\n",
       "      <td>21.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>32.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.20</td>\n",
       "      <td>37.65</td>\n",
       "      <td>30.75</td>\n",
       "      <td>43.66</td>\n",
       "      <td>22.76</td>\n",
       "      <td>28.04</td>\n",
       "      <td>28.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.92</td>\n",
       "      <td>...</td>\n",
       "      <td>24.44</td>\n",
       "      <td>24.15</td>\n",
       "      <td>34.94</td>\n",
       "      <td>28.11</td>\n",
       "      <td>33.88</td>\n",
       "      <td>33.53</td>\n",
       "      <td>39.30</td>\n",
       "      <td>30.68</td>\n",
       "      <td>27.68</td>\n",
       "      <td>30.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>27.50</td>\n",
       "      <td>36.33</td>\n",
       "      <td>29.53</td>\n",
       "      <td>39.86</td>\n",
       "      <td>26.15</td>\n",
       "      <td>20.05</td>\n",
       "      <td>25.37</td>\n",
       "      <td>18.27</td>\n",
       "      <td>21.88</td>\n",
       "      <td>18.25</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>24.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>21.95</td>\n",
       "      <td>29.47</td>\n",
       "      <td>28.47</td>\n",
       "      <td>31.52</td>\n",
       "      <td>32.13</td>\n",
       "      <td>28.22</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>25.51</td>\n",
       "      <td>37.62</td>\n",
       "      <td>30.48</td>\n",
       "      <td>41.88</td>\n",
       "      <td>26.52</td>\n",
       "      <td>23.00</td>\n",
       "      <td>31.94</td>\n",
       "      <td>16.56</td>\n",
       "      <td>24.29</td>\n",
       "      <td>19.14</td>\n",
       "      <td>...</td>\n",
       "      <td>28.65</td>\n",
       "      <td>24.49</td>\n",
       "      <td>32.52</td>\n",
       "      <td>27.44</td>\n",
       "      <td>31.76</td>\n",
       "      <td>33.00</td>\n",
       "      <td>44.68</td>\n",
       "      <td>24.89</td>\n",
       "      <td>33.04</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>29.40</td>\n",
       "      <td>28.26</td>\n",
       "      <td>31.81</td>\n",
       "      <td>39.68</td>\n",
       "      <td>26.45</td>\n",
       "      <td>24.28</td>\n",
       "      <td>28.92</td>\n",
       "      <td>18.00</td>\n",
       "      <td>23.41</td>\n",
       "      <td>21.11</td>\n",
       "      <td>...</td>\n",
       "      <td>32.26</td>\n",
       "      <td>23.88</td>\n",
       "      <td>32.02</td>\n",
       "      <td>26.94</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.10</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.70</td>\n",
       "      <td>30.21</td>\n",
       "      <td>18.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>24.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>29.02</td>\n",
       "      <td>41.93</td>\n",
       "      <td>25.49</td>\n",
       "      <td>25.38</td>\n",
       "      <td>25.57</td>\n",
       "      <td>17.61</td>\n",
       "      <td>23.63</td>\n",
       "      <td>21.70</td>\n",
       "      <td>...</td>\n",
       "      <td>27.08</td>\n",
       "      <td>27.32</td>\n",
       "      <td>34.00</td>\n",
       "      <td>26.27</td>\n",
       "      <td>27.32</td>\n",
       "      <td>29.02</td>\n",
       "      <td>37.90</td>\n",
       "      <td>27.58</td>\n",
       "      <td>30.23</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>31.77</td>\n",
       "      <td>38.30</td>\n",
       "      <td>30.65</td>\n",
       "      <td>39.34</td>\n",
       "      <td>34.20</td>\n",
       "      <td>23.70</td>\n",
       "      <td>30.08</td>\n",
       "      <td>15.28</td>\n",
       "      <td>27.10</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>21.30</td>\n",
       "      <td>32.74</td>\n",
       "      <td>33.75</td>\n",
       "      <td>27.22</td>\n",
       "      <td>32.43</td>\n",
       "      <td>39.29</td>\n",
       "      <td>29.04</td>\n",
       "      <td>34.64</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1210007700  1210008500  1210009500  1210010300  1210011300  1210013700  \\\n",
       "0          32.70       21.00       33.54       43.21       24.31       26.54   \n",
       "1          31.50       38.00       25.61       43.89       22.75       24.37   \n",
       "2          22.00       38.00       47.00       54.00       19.00       24.36   \n",
       "3          31.00       34.39       32.70       44.15       23.64       27.76   \n",
       "4          39.20       37.65       30.75       43.66       22.76       28.04   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       27.50       36.33       29.53       39.86       26.15       20.05   \n",
       "2876       25.51       37.62       30.48       41.88       26.52       23.00   \n",
       "2877       29.40       28.26       31.81       39.68       26.45       24.28   \n",
       "2878       24.00       28.21       29.02       41.93       25.49       25.38   \n",
       "2879       31.77       38.30       30.65       39.34       34.20       23.70   \n",
       "\n",
       "      1220011900  1220016300  1220021100  1220025100  ...  1210013800  \\\n",
       "0          35.91       29.00       28.78       21.76  ...       24.98   \n",
       "1          30.73       29.07       25.92       22.66  ...       26.22   \n",
       "2          35.75       24.63       21.96       22.67  ...       21.55   \n",
       "3          40.06       41.00       28.65       21.68  ...       25.33   \n",
       "4          28.10       23.25       23.00       19.92  ...       24.44   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2875       25.37       18.27       21.88       18.25  ...       28.17   \n",
       "2876       31.94       16.56       24.29       19.14  ...       28.65   \n",
       "2877       28.92       18.00       23.41       21.11  ...       32.26   \n",
       "2878       25.57       17.61       23.63       21.70  ...       27.08   \n",
       "2879       30.08       15.28       27.10       19.33  ...       26.68   \n",
       "\n",
       "      1210013000  1210012200  1210012300  1210013100  1210013900  1210014900  \\\n",
       "0          24.54       32.76       43.75       32.36       32.13       35.47   \n",
       "1          25.14       32.15       43.75       25.00       35.04       26.00   \n",
       "2          25.16       34.37       43.75       25.00       33.00       36.32   \n",
       "3          24.11       33.41       28.61       29.88       34.93       37.35   \n",
       "4          24.15       34.94       28.11       33.88       33.53       39.30   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       24.28       38.21       21.95       29.47       28.47       31.52   \n",
       "2876       24.49       32.52       27.44       31.76       33.00       44.68   \n",
       "2877       23.88       32.02       26.94       30.75       31.10       17.00   \n",
       "2878       27.32       34.00       26.27       27.32       29.02       37.90   \n",
       "2879       21.30       32.74       33.75       27.22       32.43       39.29   \n",
       "\n",
       "      1210015900  1210017100  1210018300  \n",
       "0          22.96       26.62       14.00  \n",
       "1          16.00       27.22       29.58  \n",
       "2          22.42       32.40       11.00  \n",
       "3          21.35       29.07       32.34  \n",
       "4          30.68       27.68       30.03  \n",
       "...          ...         ...         ...  \n",
       "2875       32.13       28.22       15.96  \n",
       "2876       24.89       33.04       11.00  \n",
       "2877       24.70       30.21       18.65  \n",
       "2878       27.58       30.23       15.41  \n",
       "2879       29.04       34.64       15.30  \n",
       "\n",
       "[2880 rows x 506 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e0122037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.117866130228606"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.T[0] * adj[0]).sum() / adj[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "880633d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,  28, 326, 327, 344, 345, 487], dtype=int64),)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(adj[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e7b62aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.7 , 21.  , 43.2 , 25.31, 56.51, 40.61, 51.97, 46.49])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T[0][np.where(adj[0]>0)[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a470dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.40327677, 0.40327677, 0.9191608 , 0.99950624,\n",
       "       0.99950624, 0.9191608 , 0.99950624], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[0][np.where(adj[0]>0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7568d08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.11786716118455"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sum = 0\n",
    "adj_sum = adj[0].sum()\n",
    "\n",
    "for idx in np.where(adj[0] > 0)[0]:\n",
    "    weight = adj[0][idx] / adj_sum\n",
    "    v = df.T[0][idx] * weight\n",
    "    v_sum += v\n",
    "v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "09521204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2870</th>\n",
       "      <th>2871</th>\n",
       "      <th>2872</th>\n",
       "      <th>2873</th>\n",
       "      <th>2874</th>\n",
       "      <th>2875</th>\n",
       "      <th>2876</th>\n",
       "      <th>2877</th>\n",
       "      <th>2878</th>\n",
       "      <th>2879</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1210007700</th>\n",
       "      <td>32.70</td>\n",
       "      <td>31.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>39.20</td>\n",
       "      <td>37.01</td>\n",
       "      <td>34.00</td>\n",
       "      <td>30.65</td>\n",
       "      <td>25.50</td>\n",
       "      <td>28.30</td>\n",
       "      <td>...</td>\n",
       "      <td>28.00</td>\n",
       "      <td>31.57</td>\n",
       "      <td>23.82</td>\n",
       "      <td>24.00</td>\n",
       "      <td>30.55</td>\n",
       "      <td>27.50</td>\n",
       "      <td>25.51</td>\n",
       "      <td>29.40</td>\n",
       "      <td>24.00</td>\n",
       "      <td>31.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210008500</th>\n",
       "      <td>21.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>34.39</td>\n",
       "      <td>37.65</td>\n",
       "      <td>36.75</td>\n",
       "      <td>48.00</td>\n",
       "      <td>36.86</td>\n",
       "      <td>48.00</td>\n",
       "      <td>37.21</td>\n",
       "      <td>...</td>\n",
       "      <td>41.00</td>\n",
       "      <td>30.41</td>\n",
       "      <td>33.91</td>\n",
       "      <td>29.00</td>\n",
       "      <td>34.97</td>\n",
       "      <td>36.33</td>\n",
       "      <td>37.62</td>\n",
       "      <td>28.26</td>\n",
       "      <td>28.21</td>\n",
       "      <td>38.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210009500</th>\n",
       "      <td>33.54</td>\n",
       "      <td>25.61</td>\n",
       "      <td>47.00</td>\n",
       "      <td>32.70</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.91</td>\n",
       "      <td>44.00</td>\n",
       "      <td>32.23</td>\n",
       "      <td>38.00</td>\n",
       "      <td>31.82</td>\n",
       "      <td>...</td>\n",
       "      <td>30.00</td>\n",
       "      <td>31.36</td>\n",
       "      <td>28.06</td>\n",
       "      <td>32.00</td>\n",
       "      <td>29.87</td>\n",
       "      <td>29.53</td>\n",
       "      <td>30.48</td>\n",
       "      <td>31.81</td>\n",
       "      <td>29.02</td>\n",
       "      <td>30.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210010300</th>\n",
       "      <td>43.21</td>\n",
       "      <td>43.89</td>\n",
       "      <td>54.00</td>\n",
       "      <td>44.15</td>\n",
       "      <td>43.66</td>\n",
       "      <td>45.49</td>\n",
       "      <td>41.05</td>\n",
       "      <td>42.92</td>\n",
       "      <td>49.00</td>\n",
       "      <td>43.37</td>\n",
       "      <td>...</td>\n",
       "      <td>37.58</td>\n",
       "      <td>39.35</td>\n",
       "      <td>40.13</td>\n",
       "      <td>52.00</td>\n",
       "      <td>38.04</td>\n",
       "      <td>39.86</td>\n",
       "      <td>41.88</td>\n",
       "      <td>39.68</td>\n",
       "      <td>41.93</td>\n",
       "      <td>39.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210011300</th>\n",
       "      <td>24.31</td>\n",
       "      <td>22.75</td>\n",
       "      <td>19.00</td>\n",
       "      <td>23.64</td>\n",
       "      <td>22.76</td>\n",
       "      <td>23.88</td>\n",
       "      <td>17.00</td>\n",
       "      <td>21.71</td>\n",
       "      <td>18.00</td>\n",
       "      <td>22.48</td>\n",
       "      <td>...</td>\n",
       "      <td>29.16</td>\n",
       "      <td>25.57</td>\n",
       "      <td>25.31</td>\n",
       "      <td>27.00</td>\n",
       "      <td>26.69</td>\n",
       "      <td>26.15</td>\n",
       "      <td>26.52</td>\n",
       "      <td>26.45</td>\n",
       "      <td>25.49</td>\n",
       "      <td>34.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210013900</th>\n",
       "      <td>32.13</td>\n",
       "      <td>35.04</td>\n",
       "      <td>33.00</td>\n",
       "      <td>34.93</td>\n",
       "      <td>33.53</td>\n",
       "      <td>30.09</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>31.59</td>\n",
       "      <td>...</td>\n",
       "      <td>31.07</td>\n",
       "      <td>41.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>29.57</td>\n",
       "      <td>28.47</td>\n",
       "      <td>33.00</td>\n",
       "      <td>31.10</td>\n",
       "      <td>29.02</td>\n",
       "      <td>32.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210014900</th>\n",
       "      <td>35.47</td>\n",
       "      <td>26.00</td>\n",
       "      <td>36.32</td>\n",
       "      <td>37.35</td>\n",
       "      <td>39.30</td>\n",
       "      <td>40.02</td>\n",
       "      <td>39.67</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>...</td>\n",
       "      <td>26.34</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>31.52</td>\n",
       "      <td>44.68</td>\n",
       "      <td>17.00</td>\n",
       "      <td>37.90</td>\n",
       "      <td>39.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210015900</th>\n",
       "      <td>22.96</td>\n",
       "      <td>16.00</td>\n",
       "      <td>22.42</td>\n",
       "      <td>21.35</td>\n",
       "      <td>30.68</td>\n",
       "      <td>28.42</td>\n",
       "      <td>33.60</td>\n",
       "      <td>33.51</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>...</td>\n",
       "      <td>29.43</td>\n",
       "      <td>27.77</td>\n",
       "      <td>23.99</td>\n",
       "      <td>32.00</td>\n",
       "      <td>24.16</td>\n",
       "      <td>32.13</td>\n",
       "      <td>24.89</td>\n",
       "      <td>24.70</td>\n",
       "      <td>27.58</td>\n",
       "      <td>29.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210017100</th>\n",
       "      <td>26.62</td>\n",
       "      <td>27.22</td>\n",
       "      <td>32.40</td>\n",
       "      <td>29.07</td>\n",
       "      <td>27.68</td>\n",
       "      <td>25.25</td>\n",
       "      <td>26.22</td>\n",
       "      <td>23.00</td>\n",
       "      <td>26.54</td>\n",
       "      <td>32.20</td>\n",
       "      <td>...</td>\n",
       "      <td>29.38</td>\n",
       "      <td>25.71</td>\n",
       "      <td>30.08</td>\n",
       "      <td>33.26</td>\n",
       "      <td>27.71</td>\n",
       "      <td>28.22</td>\n",
       "      <td>33.04</td>\n",
       "      <td>30.21</td>\n",
       "      <td>30.23</td>\n",
       "      <td>34.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210018300</th>\n",
       "      <td>14.00</td>\n",
       "      <td>29.58</td>\n",
       "      <td>11.00</td>\n",
       "      <td>32.34</td>\n",
       "      <td>30.03</td>\n",
       "      <td>29.03</td>\n",
       "      <td>22.00</td>\n",
       "      <td>25.43</td>\n",
       "      <td>16.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>...</td>\n",
       "      <td>16.73</td>\n",
       "      <td>17.04</td>\n",
       "      <td>19.97</td>\n",
       "      <td>15.22</td>\n",
       "      <td>17.88</td>\n",
       "      <td>15.96</td>\n",
       "      <td>11.00</td>\n",
       "      <td>18.65</td>\n",
       "      <td>15.41</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 2880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4      5      6      7      8     \\\n",
       "1210007700  32.70  31.50  22.00  31.00  39.20  37.01  34.00  30.65  25.50   \n",
       "1210008500  21.00  38.00  38.00  34.39  37.65  36.75  48.00  36.86  48.00   \n",
       "1210009500  33.54  25.61  47.00  32.70  30.75  31.91  44.00  32.23  38.00   \n",
       "1210010300  43.21  43.89  54.00  44.15  43.66  45.49  41.05  42.92  49.00   \n",
       "1210011300  24.31  22.75  19.00  23.64  22.76  23.88  17.00  21.71  18.00   \n",
       "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1210013900  32.13  35.04  33.00  34.93  33.53  30.09  34.00  35.00  36.00   \n",
       "1210014900  35.47  26.00  36.32  37.35  39.30  40.02  39.67  30.00  30.00   \n",
       "1210015900  22.96  16.00  22.42  21.35  30.68  28.42  33.60  33.51  23.00   \n",
       "1210017100  26.62  27.22  32.40  29.07  27.68  25.25  26.22  23.00  26.54   \n",
       "1210018300  14.00  29.58  11.00  32.34  30.03  29.03  22.00  25.43  16.00   \n",
       "\n",
       "             9     ...   2870   2871   2872   2873   2874   2875   2876  \\\n",
       "1210007700  28.30  ...  28.00  31.57  23.82  24.00  30.55  27.50  25.51   \n",
       "1210008500  37.21  ...  41.00  30.41  33.91  29.00  34.97  36.33  37.62   \n",
       "1210009500  31.82  ...  30.00  31.36  28.06  32.00  29.87  29.53  30.48   \n",
       "1210010300  43.37  ...  37.58  39.35  40.13  52.00  38.04  39.86  41.88   \n",
       "1210011300  22.48  ...  29.16  25.57  25.31  27.00  26.69  26.15  26.52   \n",
       "...           ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1210013900  31.59  ...  31.07  41.00  18.00  34.00  29.57  28.47  33.00   \n",
       "1210014900  30.00  ...  26.34  14.00  14.00  36.00  26.00  31.52  44.68   \n",
       "1210015900  23.00  ...  29.43  27.77  23.99  32.00  24.16  32.13  24.89   \n",
       "1210017100  32.20  ...  29.38  25.71  30.08  33.26  27.71  28.22  33.04   \n",
       "1210018300  11.00  ...  16.73  17.04  19.97  15.22  17.88  15.96  11.00   \n",
       "\n",
       "             2877   2878   2879  \n",
       "1210007700  29.40  24.00  31.77  \n",
       "1210008500  28.26  28.21  38.30  \n",
       "1210009500  31.81  29.02  30.65  \n",
       "1210010300  39.68  41.93  39.34  \n",
       "1210011300  26.45  25.49  34.20  \n",
       "...           ...    ...    ...  \n",
       "1210013900  31.10  29.02  32.43  \n",
       "1210014900  17.00  37.90  39.29  \n",
       "1210015900  24.70  27.58  29.04  \n",
       "1210017100  30.21  30.23  34.64  \n",
       "1210018300  18.65  15.41  15.30  \n",
       "\n",
       "[506 rows x 2880 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "688eb772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 2880)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputY = df.T.shape[0]\n",
    "outputX = df.T.shape[1]\n",
    "output = np.zeros((outputY, outputX))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "82603332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_time(second):\n",
    "    h = second // 3600\n",
    "    m = int(second // 60 - h * 60)\n",
    "    s = round(second % 60, 2)\n",
    "    \n",
    "    time = ''\n",
    "    if h > 0: \n",
    "        time += str(h) + 'h '\n",
    "        time += str(m) + 'm '\n",
    "    elif m > 0: time += str(m) + 'm '\n",
    "    time += str(s) + 's'\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0969cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ~ 10: 53.32s\n",
      "10 ~ 20: 30.59s\n",
      "20 ~ 30: 53.37s\n",
      "30 ~ 40: 45.89s\n",
      "40 ~ 50: 35.75s\n",
      "50 ~ 60: 41.21s\n",
      "60 ~ 70: 53.07s\n",
      "70 ~ 80: 39.39s\n",
      "80 ~ 90: 54.55s\n",
      "90 ~ 100: 44.78s\n",
      "100 ~ 110: 52.73s\n",
      "110 ~ 120: 54.32s\n",
      "120 ~ 130: 1m 5.73s\n",
      "130 ~ 140: 51.38s\n",
      "140 ~ 150: 34.57s\n",
      "150 ~ 160: 48.4s\n",
      "160 ~ 170: 53.51s\n",
      "170 ~ 180: 45.55s\n",
      "180 ~ 190: 38.3s\n",
      "190 ~ 200: 43.86s\n",
      "200 ~ 210: 1m 2.5s\n",
      "210 ~ 220: 43.73s\n",
      "220 ~ 230: 54.58s\n",
      "230 ~ 240: 56.47s\n",
      "240 ~ 250: 53.43s\n",
      "250 ~ 260: 51.64s\n",
      "260 ~ 270: 52.95s\n",
      "270 ~ 280: 48.61s\n",
      "280 ~ 290: 56.37s\n",
      "290 ~ 300: 55.22s\n",
      "300 ~ 310: 43.31s\n",
      "310 ~ 320: 46.24s\n",
      "320 ~ 330: 33.43s\n",
      "330 ~ 340: 50.87s\n",
      "340 ~ 350: 35.24s\n",
      "350 ~ 360: 40.12s\n",
      "360 ~ 370: 53.14s\n",
      "370 ~ 380: 41.46s\n",
      "380 ~ 390: 1m 1.26s\n",
      "390 ~ 400: 1m 9.51s\n",
      "400 ~ 410: 1m 5.6s\n",
      "410 ~ 420: 1m 2.15s\n",
      "420 ~ 430: 54.02s\n",
      "430 ~ 440: 36.55s\n",
      "440 ~ 450: 40.38s\n",
      "450 ~ 460: 46.11s\n",
      "460 ~ 470: 58.42s\n",
      "470 ~ 480: 37.88s\n",
      "480 ~ 490: 41.21s\n",
      "490 ~ 500: 47.89s\n",
      "500 ~ 510: 34.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[41.11786716, 37.58111418, 36.70847217, ..., 35.49891413,\n",
       "        35.18802949, 36.18346159],\n",
       "       [37.26839051, 34.85595636, 37.30003864, ..., 34.87645924,\n",
       "        33.97578792, 35.90103252],\n",
       "       [33.90569951, 31.66546705, 37.58033442, ..., 32.92378652,\n",
       "        32.52247653, 33.87084227],\n",
       "       ...,\n",
       "       [32.1178778 , 29.74707706, 29.56167226, ..., 29.89267902,\n",
       "        28.54234516, 31.17683823],\n",
       "       [25.76884023, 25.71658743, 27.18234439, ..., 25.45863277,\n",
       "        24.61613742, 26.31798204],\n",
       "       [22.74371553, 27.76945981, 25.18355673, ..., 25.11741732,\n",
       "        24.28116985, 25.95436342]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print('0 ~ 10', end = ': ')\n",
    "for y in range(outputY):\n",
    "    if y % 10 == 0 and y > 0:\n",
    "        print(translate_time(time.time() - start))\n",
    "        print(str(y) + ' ~ ' + str(y+10), end = ': ')\n",
    "        start = time.time()\n",
    "        \n",
    "    for x in range(outputX):\n",
    "        v_sum = 0\n",
    "        adj_sum = adj[y].sum()\n",
    "        \n",
    "        for idx in np.where(adj[y] > 0)[0]:\n",
    "            weight = adj[y][idx] / adj_sum\n",
    "            v = df.T[x][idx] * weight\n",
    "            v_sum += v\n",
    "        output[y][x] = v_sum\n",
    "        \n",
    "print(translate_time(time.time() - start))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e500eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f1ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e6c917d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.274181001592034, 11.135170919736774)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.mean(), df.values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "22f0e00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.27640308928024, 7.405970114851601)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.mean(), output.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9af99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9224248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfee7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.11786716118455 32.7\n",
      "34.85595635861158 31.5\n",
      "37.58033442273736 22.0\n",
      "28.615117293447256 31.0\n",
      "27.618520742580298 39.2\n",
      "27.215403730664406 37.01\n",
      "36.34479475690052 34.0\n",
      "28.26424498785287 30.65\n",
      "25.86498176276684 25.5\n",
      "25.88601643245667 28.3\n",
      "25.366655357703564 32.0\n",
      "30.84996803790331 29.5\n",
      "30.77785994250327 28.0\n",
      "58.201504267156125 25.86\n",
      "48.78189110025763 30.0\n",
      "52.717776196599004 35.21\n",
      "36.65018258616328 34.5\n",
      "31.12402012761682 36.5\n",
      "29.33694441154599 33.0\n",
      "25.99127701342106 27.0\n",
      "27.681830271855 38.65\n",
      "25.974910844974218 34.0\n",
      "30.304181253779674 38.0\n",
      "31.87220797846094 34.98\n",
      "28.996861449535935 24.5\n",
      "29.26037752301432 26.5\n",
      "30.791403799653054 29.0\n",
      "36.593428273499015 29.0\n",
      "45.196770541369915 61.73\n",
      "27.5763127348572 41.89\n",
      "32.349721771031625 46.0\n",
      "32.453540017902846 41.0\n",
      "31.03853891339153 37.5\n",
      "30.16751674428582 37.5\n",
      "33.22747668832541 35.68\n",
      "35.71110929250717 38.8\n",
      "36.882149654366074 39.69\n",
      "33.8626780279167 33.5\n",
      "30.515804379321633 33.5\n",
      "30.2249246719107 33.0\n",
      "27.770208964347837 33.1\n",
      "30.555431426912545 31.5\n",
      "31.211033360362052 33.1\n",
      "32.56191564522683 41.0\n",
      "28.940760419741274 30.8\n",
      "36.5 40.5\n",
      "42.8384586437419 50.0\n",
      "32.63394914418459 47.5\n",
      "43.69825740337372 28.0\n",
      "36.754412753582 28.0\n",
      "24.98678407587111 28.0\n",
      "31.068551493100827 40.8\n",
      "26.584883583560586 25.0\n",
      "30.40100118977949 23.0\n",
      "27.25916957564652 23.0\n",
      "36.41196283131838 33.0\n",
      "32.93147386088967 33.0\n",
      "33.11540148310363 43.05\n",
      "39.18313836935908 33.0\n",
      "32.03822536200285 33.0\n",
      "37.1151546446979 33.0\n",
      "36.999160748720165 33.0\n",
      "36.04971299350261 33.3\n",
      "36.67465884337202 32.5\n",
      "39.25065379358828 29.0\n",
      "37.6274682816863 23.0\n",
      "38.4642309736833 31.5\n",
      "32.883418056108056 47.38\n",
      "39.49429045367986 22.0\n",
      "35.424935106337074 21.5\n",
      "50.2428474766016 32.91\n",
      "52.297552080750464 35.0\n",
      "58.215802884288124 35.0\n",
      "47.1191930713132 20.5\n",
      "66.7873544825241 35.0\n",
      "55.56006888225674 26.5\n",
      "53.670508662760255 20.0\n",
      "33.05971514895558 8.0\n",
      "36.018985594492406 13.5\n",
      "36.177590457350014 34.5\n",
      "33.811631599590186 36.5\n",
      "36.74702287510037 33.0\n",
      "30.095562044754626 35.0\n",
      "33.127935852501544 24.5\n",
      "33.965308033004405 33.0\n",
      "35.74349672317505 25.5\n",
      "34.61794514909386 24.5\n",
      "32.91073516055941 28.0\n",
      "40.59646580997855 26.5\n",
      "33.57774121413007 29.5\n",
      "34.58245394691825 34.5\n",
      "37.79962293356657 27.0\n",
      "43.80797113105655 31.5\n",
      "37.69420096755028 60.44\n",
      "32.83436539780349 26.5\n",
      "28.116477628052237 30.0\n",
      "30.045461763739585 26.0\n",
      "30.34741864996031 52.77\n",
      "28.138563422691078 44.0\n",
      "34.26398345952853 48.48\n",
      "30.87453467838466 25.5\n",
      "30.57196892593056 25.5\n",
      "27.396111442726106 23.5\n",
      "27.725206484049558 38.08\n",
      "28.250789834931492 29.5\n",
      "37.96293762937188 15.5\n",
      "29.777636689990757 15.0\n",
      "27.17627653654665 38.36\n",
      "28.795834124814718 29.0\n",
      "32.10779174767435 25.0\n",
      "34.63072182685137 41.91\n",
      "31.025418327320367 25.0\n",
      "33.40825663212687 26.5\n",
      "30.816581842694433 36.5\n",
      "31.726424990296362 35.5\n",
      "30.355506630763415 14.43\n",
      "36.00409746527673 23.5\n",
      "37.18420045953244 31.24\n",
      "43.47083153162151 28.5\n",
      "27.874033986665307 29.0\n",
      "32.298408921808004 43.0\n",
      "31.785858703572302 30.07\n",
      "30.631884952690452 35.25\n",
      "26.793112909477202 26.0\n",
      "30.925023202411833 34.0\n",
      "34.93941811442376 32.95\n",
      "36.154428255986424 27.5\n",
      "32.72573063591495 32.68\n",
      "29.85954775022343 28.08\n",
      "29.565820515733208 25.0\n",
      "30.932713138796395 25.0\n",
      "29.031242110282186 38.64\n",
      "37.1468435083516 39.13\n",
      "30.82699697209522 34.6\n",
      "28.77098071206361 44.89\n",
      "29.112995484992858 39.33\n",
      "33.154349853899326 31.03\n",
      "28.387150508165355 34.67\n",
      "30.65976568622515 28.5\n",
      "31.32437300875783 32.78\n",
      "29.016951039396226 29.81\n",
      "29.68771230451762 28.0\n",
      "21.04552927069366 28.5\n",
      "15.910020284056664 28.26\n",
      "29.09405239254236 27.28\n",
      "37.6705987393856 25.99\n",
      "39.16002097994089 23.34\n",
      "27.909476529210806 22.61\n",
      "15.471755170673132 23.3\n",
      "19.826374994516375 30.45\n",
      "22.047112939655783 31.41\n",
      "26.987765489071606 24.52\n",
      "25.206538237277417 26.89\n",
      "29.794291351605203 27.6\n",
      "19.243187083899976 23.61\n",
      "29.118250255510212 31.47\n",
      "24.929979598969222 29.5\n",
      "22.02344883799553 28.47\n",
      "23.91223044740036 26.38\n",
      "28.102293106596917 25.42\n",
      "24.716243336834015 23.0\n",
      "22.71024894207716 26.0\n",
      "24.820665572211148 25.84\n",
      "27.58543205566704 30.2\n",
      "22.452567786760625 24.17\n",
      "23.709380394294854 28.5\n",
      "29.474782512784007 25.11\n",
      "27.360187294818463 29.95\n",
      "31.50698430120945 23.5\n",
      "30.269724398404364 27.58\n",
      "27.60019870692864 31.0\n",
      "29.407411370463667 35.77\n",
      "24.474931564703585 30.0\n",
      "21.724815315697345 29.5\n",
      "24.423424515267833 29.78\n",
      "31.970336682498456 28.88\n",
      "34.536482626199714 24.0\n",
      "29.988170918226242 33.46\n",
      "34.99463999485597 32.88\n",
      "31.802812148332592 40.86\n",
      "34.03865133672953 32.0\n",
      "30.768912666365505 33.98\n",
      "30.192186963856223 27.6\n",
      "25.13193105455488 38.34\n",
      "20.126729731559752 34.43\n",
      "31.874373382776973 27.33\n",
      "25.760597395002844 19.5\n",
      "27.936964970305564 37.4\n",
      "29.435401315838096 20.5\n",
      "27.80894720166922 27.5\n",
      "25.91186031183228 35.11\n",
      "29.879891188777982 45.08\n",
      "29.199419321715833 31.62\n",
      "26.811632047891614 25.5\n",
      "27.479663696736097 34.53\n",
      "27.360765118449926 38.01\n",
      "28.24430494323373 31.87\n",
      "23.844576360806823 28.5\n",
      "28.434997383505106 30.0\n",
      "29.265718370936813 29.86\n",
      "30.057083400189878 28.84\n",
      "28.88154957935214 23.7\n",
      "22.81564173348248 27.0\n",
      "24.455489487089217 27.0\n",
      "20.277693464178594 40.08\n",
      "24.33793301727622 23.69\n",
      "25.411247716918588 28.5\n",
      "23.926599686816335 31.44\n",
      "21.15418624654413 43.1\n",
      "30.77822121031582 33.19\n",
      "29.966385814100505 41.66\n",
      "31.194883203953506 27.5\n",
      "29.03578746974468 29.75\n",
      "31.16491979708895 43.17\n",
      "24.450534560233354 26.0\n",
      "27.062116962373256 30.75\n",
      "27.846426995024085 26.0\n",
      "25.588734322339295 40.7\n",
      "25.904648547470572 38.16\n",
      "32.0922183697857 38.01\n",
      "31.56251542642712 25.0\n",
      "34.017857588678595 19.0\n",
      "28.92958211492747 31.12\n",
      "30.407193034626545 25.0\n",
      "30.87873421423137 25.5\n",
      "28.634742262549697 22.0\n",
      "28.204022290520367 24.0\n",
      "25.27436244100332 25.76\n",
      "25.32584629718214 24.5\n",
      "27.958128083832563 32.83\n",
      "28.429471005797385 20.8\n",
      "26.094724896624683 29.2\n",
      "31.64349164396525 23.0\n",
      "28.738642237447202 22.0\n",
      "27.460576936602596 30.5\n",
      "29.319539213851094 33.5\n",
      "28.03672561623156 21.25\n",
      "29.657987112142152 27.5\n",
      "26.813189774304625 38.5\n",
      "30.363232649713755 24.5\n",
      "26.792011331245302 36.16\n",
      "28.066765713915228 30.0\n",
      "30.3074915416725 44.65\n",
      "48.8355537506193 26.0\n",
      "27.43146874785423 40.17\n",
      "24.49935909178108 23.0\n",
      "22.151303506940604 24.0\n",
      "23.50877108849585 23.0\n",
      "29.061811860650774 21.0\n",
      "24.670963271185755 26.0\n",
      "24.32077891308814 25.5\n",
      "26.30530933126807 5.29\n",
      "26.654877847582103 26.5\n",
      "23.38688842156902 30.0\n",
      "27.332358771450817 9.37\n",
      "27.59354780286551 28.0\n",
      "30.08125698335469 13.41\n",
      "31.20772762995213 26.5\n",
      "26.97522213712335 30.0\n",
      "27.070576146673414 22.35\n",
      "25.6603388793394 22.5\n",
      "28.67062167875469 34.0\n",
      "29.39091893970966 28.22\n",
      "26.353000742793082 36.75\n",
      "26.375594631657002 28.0\n",
      "25.260149455424397 28.17\n",
      "33.10452419187873 30.34\n",
      "28.062256450653077 31.94\n",
      "29.27186219949275 26.3\n",
      "30.71647443335503 25.0\n",
      "32.9890914785862 39.0\n",
      "32.85195218116045 31.81\n",
      "31.33056965887546 30.67\n",
      "29.750788705684243 27.0\n",
      "27.60636273976415 25.0\n",
      "29.47589003946632 32.0\n",
      "26.992899833917615 27.33\n",
      "26.308768746294085 39.97\n",
      "26.287044751681385 38.5\n",
      "33.074369835704566 31.61\n",
      "26.824028469473124 28.22\n",
      "26.34465807892382 31.86\n",
      "26.500560203231874 28.15\n",
      "31.902990050744265 35.31\n",
      "31.294457481689747 31.57\n",
      "29.719491655379535 32.7\n",
      "28.415138682127 32.89\n",
      "31.727286538407206 40.5\n",
      "35.36966153040529 34.0\n",
      "39.61701757103205 38.35\n",
      "35.25334300424904 37.0\n",
      "39.01157894901931 28.5\n",
      "32.16509021833539 33.49\n",
      "30.431712527759377 37.73\n",
      "26.729743063524364 36.0\n",
      "29.526776833161712 32.0\n",
      "26.81593652926385 36.0\n",
      "28.72600762885064 25.5\n",
      "28.583760003745553 34.27\n",
      "30.558259018808606 27.9\n",
      "29.09948760010302 26.84\n",
      "29.8083794600144 35.0\n",
      "32.122629601135856 31.08\n",
      "33.03194295413792 25.45\n",
      "32.15337806642055 30.72\n",
      "35.69962536597624 32.5\n",
      "33.50449735119939 25.56\n",
      "31.17782511755824 27.5\n",
      "34.86565233305097 28.5\n",
      "32.8475480337441 40.42\n",
      "32.308763935714964 40.5\n",
      "34.454458006545906 40.5\n",
      "33.30486044876278 40.5\n",
      "27.929370828866958 40.5\n",
      "29.733809702470896 31.54\n",
      "29.49130352914333 37.5\n",
      "37.48678543023765 37.5\n",
      "32.4681792877987 37.5\n",
      "25.46407964900136 24.54\n",
      "35.05169888753444 40.0\n",
      "36.69458239518106 37.0\n",
      "32.09516581490636 39.0\n",
      "34.40232256054878 39.0\n",
      "39.12698428273201 42.5\n",
      "35.726682949587705 42.5\n",
      "41.93543632879854 26.5\n",
      "40.46641495861113 20.68\n",
      "37.66702429592609 27.0\n",
      "44.91332852587104 28.0\n",
      "53.81171002084389 28.0\n",
      "45.98894666258246 28.0\n",
      "36.47035439040512 28.5\n",
      "39.23099190272391 28.5\n",
      "33.32316178722307 9.0\n",
      "36.498785128667954 40.5\n",
      "52.096686672270295 28.5\n",
      "45.044809928275654 28.5\n",
      "33.81549542799591 40.8\n",
      "35.881726866643874 29.5\n",
      "35.289674064349384 40.8\n",
      "34.302544868551195 40.8\n",
      "39.81742399949581 40.8\n",
      "52.732373395077886 34.5\n",
      "38.037553077489136 50.2\n",
      "46.42336347654463 60.44\n",
      "51.34036652904004 60.44\n",
      "41.08007800228894 24.0\n",
      "39.85114955231547 54.2\n",
      "33.21273670211434 24.0\n",
      "36.29216682180762 55.52\n",
      "29.401616599261757 8.0\n",
      "28.131401354670526 8.0\n",
      "30.02866265585646 45.5\n",
      "26.158153925435617 47.0\n",
      "33.871196912750605 47.0\n",
      "33.391424555182454 51.88\n",
      "32.25068013370037 35.0\n",
      "37.53036685790867 35.5\n",
      "28.760435907468203 31.5\n",
      "30.500124394893646 31.5\n",
      "30.129546692073347 36.5\n",
      "32.72608520053327 38.8\n",
      "28.664143373556435 55.8\n",
      "32.36099584043026 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.41087309941649 21.0\n",
      "36.30381160970777 21.0\n",
      "27.330175332576037 39.21\n",
      "34.450737634859976 31.5\n",
      "31.829774799458683 31.5\n",
      "31.755381691325457 31.5\n",
      "29.819821858927604 32.0\n",
      "34.20602823976428 6.0\n",
      "37.67841061655432 28.65\n",
      "30.567371831387284 32.0\n",
      "28.144155061617496 37.5\n",
      "46.446318041794 40.0\n",
      "51.97686668038368 40.0\n",
      "86.25 25.47\n",
      "57.319375051259996 45.6\n",
      "45.34307805590331 40.5\n",
      "33.32968707695604 32.0\n",
      "31.010638925135133 28.6\n",
      "34.18682405143976 30.41\n",
      "33.68685548007488 34.5\n",
      "29.06881784442812 29.0\n",
      "27.65378693282604 27.5\n",
      "30.880512241311376 28.5\n",
      "31.015772490203382 34.5\n",
      "32.02605924692004 32.0\n",
      "35.03451041487977 51.88\n",
      "39.65910044774413 28.0\n",
      "23.172572485953566 25.5\n",
      "22.349739808104932 26.0\n",
      "31.739679074622686 25.5\n",
      "31.046889116689563 22.5\n",
      "28.947822928205134 32.5\n",
      "25.923507617861034 27.0\n",
      "26.542828300576655 31.0\n",
      "26.572220563292504 28.0\n",
      "27.28124477364123 29.5\n",
      "26.266427534222604 36.0\n",
      "26.236765370741484 36.0\n",
      "31.870430970229208 34.0\n",
      "31.166715818122025 24.5\n",
      "28.333135342262683 27.5\n",
      "38.781636914163826 15.3\n",
      "38.0107365822047 31.0\n",
      "26.908816134370866 29.0\n",
      "33.20505426017567 33.0\n",
      "33.329995866268874 25.5\n",
      "27.60138374850154 19.87\n",
      "33.21044641731307 18.21\n",
      "32.058868287540975 22.95\n",
      "30.48999878205359 31.0\n",
      "28.101438331007962 24.5\n",
      "28.46779492795467 30.5\n",
      "32.09310420580208 20.5\n",
      "30.96411877565086 25.0\n",
      "26.38494920320809 27.0\n",
      "28.586310962606223 56.64\n",
      "32.37637792704627 27.5\n",
      "35.6127939298749 56.92\n",
      "27.21651277959347 53.21\n",
      "31.473597859144206 29.5\n",
      "32.55254394473508 30.5\n",
      "33.69097324341535 31.0\n",
      "36.65124757589773 41.33\n",
      "37.80519497603178 19.37\n",
      "50.71266344189644 32.0\n",
      "51.0 28.5\n",
      "32.365758440829815 23.0\n",
      "61.30440110862255 29.75\n",
      "25.065895608551802 26.0\n",
      "28.515641826987267 43.32\n",
      "29.069838591348383 28.0\n",
      "22.329087871238592 34.0\n",
      "28.816213184222576 37.68\n",
      "27.619016677439216 35.58\n",
      "22.0 41.88\n",
      "20.0 29.74\n",
      "26.297704120725392 35.39\n",
      "27.99404228141531 29.72\n",
      "22.885521116182208 38.98\n",
      "28.41789091780782 26.0\n",
      "23.065448292940857 27.01\n",
      "28.038023702576755 35.0\n",
      "27.742303987778723 28.5\n",
      "23.290874792262912 19.5\n",
      "27.128355363607405 18.5\n",
      "29.390900325775146 31.13\n",
      "23.738199270144104 24.23\n",
      "24.97435086403042 38.92\n",
      "28.195128030776978 25.68\n",
      "25.177188532054423 33.21\n",
      "30.160615371912716 23.5\n",
      "28.553482515178615 24.5\n",
      "25.66217591498047 37.72\n",
      "25.98393444553018 33.62\n",
      "29.402354954555626 31.1\n",
      "29.69130907518789 30.0\n",
      "28.05071738317609 28.5\n",
      "26.97954359089956 27.0\n",
      "26.658405927829442 23.5\n",
      "27.71301597952843 25.5\n",
      "28.066926378142085 24.5\n",
      "27.226580587923525 25.0\n",
      "25.93699026849121 53.76\n",
      "28.94309143042192 21.38\n",
      "25.7436181717366 28.5\n",
      "26.07498621210456 31.24\n",
      "24.586510075330732 27.0\n",
      "28.290220469534397 28.0\n",
      "23.182756713926793 31.47\n",
      "24.19111668072641 16.5\n",
      "29.267716879062355 28.12\n",
      "22.595886780917642 33.27\n",
      "23.699078414589167 28.02\n",
      "28.215852370057256 36.6\n",
      "25.351908799000086 37.99\n",
      "26.719289280287924 31.2\n",
      "25.049669885337355 26.76\n",
      "28.2307267472893 27.64\n",
      "24.622509077414872 35.12\n",
      "28.012063945122065 35.59\n",
      "27.31731667678803 34.11\n",
      "27.365292516388 36.98\n",
      "28.60237079720944 41.24\n",
      "32.69342059478164 41.36\n",
      "51.64783873558045 37.95\n",
      "35.708629028499125 43.71\n",
      "30.54672907203436 32.97\n",
      "40.696702527999875 27.5\n",
      "23.956933227293195 24.0\n",
      "23.024390601608907 24.0\n",
      "25.045772106796505 39.73\n",
      "25.786364581510426 23.5\n",
      "32.59920384334401 38.91\n",
      "25.711553588807583 33.45\n",
      "30.01004099547863 46.48\n",
      "34.89244527876377 22.5\n",
      "28.95292956084013 45.58\n",
      "25.540488290786744 26.0\n",
      "29.02488910958171 36.56\n",
      "25.48084699052386 44.76\n",
      "28.05823874209076 32.98\n",
      "24.779471026100218 31.76\n"
     ]
    }
   ],
   "source": [
    "for i in range(adj.shape[0]):\n",
    "    v_sum = 0\n",
    "    adj_sum = adj[i].sum()\n",
    "    \n",
    "    for idx in np.where(adj[i] > 0)[0]:\n",
    "        weight = adj[i][idx] / adj_sum\n",
    "        v = df.T[i][idx] * weight\n",
    "        v_sum += v\n",
    "    print(v_sum, df.T[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bbf80472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.40327677, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.40327677, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9191608 , 0.99950624, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.99950624,\n",
       "       0.9191608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.99950624, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "156c0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6433945"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082c1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e14a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b3394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Degree = np.zeros((inputX, inputY))\n",
    "for i in range(len(adj)):\n",
    "    deg = 0\n",
    "    for j in range(len(adj)):\n",
    "        deg += adj[i][j]\n",
    "    Degree[i] = deg\n",
    "\n",
    "Laplacian = Degree - Adj\n",
    "Unit = np.zeros((inputX, inputY))\n",
    "for i in range(len(Degree)):\n",
    "    \n",
    "\n",
    "Laplacian_Normalized = np.dot(np.dot(np.inverse(np.sqrt(D)), adj), np.inverse(np.sqrt(D)))+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c95fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chebyshev:\n",
    "    def __init__(self, a, b, n, func): \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.func = func\n",
    "\n",
    "        bma = 0.5 * (b - a)\n",
    "        bpa = 0.5 * (b + a)\n",
    "        f = [func(math.cos(math.pi * (k + 0.5) / n) * bma + bpa) for k in range(n)]\n",
    "        fac = 2.0 / n\n",
    "        self.c = [fac * sum([f[k] * math.cos(math.pi * j * (k + 0.5) / n)\n",
    "                  for k in range(n)]) for j in range(n)]\n",
    "\n",
    "    def eval(self, x):\n",
    "        a,b = self.a, self.b\n",
    "        assert(a <= x <= b)\n",
    "        y = (2.0 * x - a - b) * (1.0 / (b - a))\n",
    "        y2 = 2.0 * y\n",
    "        (d, dd) = (self.c[-1], 0)\n",
    "        for cj in self.c[-2:0:-1]:\n",
    "            (d, dd) = (y2 * d - dd + cj, d)\n",
    "        return y * d - dd + 0.5 * self.c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef306cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionGraphConv(BaseModel):\n",
    "    def __init__(self, supports, input_dim, hid_dim, num_nodes, max_diffusion_step, output_dim, bias_start=0.0):\n",
    "        super(DiffusionGraphConv, self).__init__()\n",
    "        self.num_matrices = len(supports) * max_diffusion_step + 1\n",
    "        input_size = input_dim + hid_dim\n",
    "        self._num_nodes = num_nodes\n",
    "        self._max_diffusion_step = max_diffusion_step\n",
    "        self._supports = supports\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(size=(input_size*self.num_matrices, output_dim)))\n",
    "        self.biases = nn.Parameter(torch.FloatTensor(size=(output_dim,)))\n",
    "        nn.init.xavier_normal_(self.weight.data, gain=1.414)\n",
    "        nn.init.constant_(self.biases.data, val=bias_start)\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = torch.unsqueeze(x_, 0)\n",
    "        return torch.cat([x, x_], dim=0)\n",
    "\n",
    "    def forward(self, inputs, state, output_size, bias_start=0.0):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = torch.reshape(inputs, (batch_size, self._num_nodes, -1))\n",
    "        state = torch.reshape(state, (batch_size, self._num_nodes, -1))\n",
    "        inputs_and_state = torch.cat([inputs, state], dim=2)\n",
    "        input_size = inputs_and_state.shape[2]\n",
    "\n",
    "        x = inputs_and_state\n",
    "        x0 = torch.transpose(x, dim0=0, dim1=1)\n",
    "        x0 = torch.transpose(x0, dim0=1, dim1=2)\n",
    "        x0 = torch.reshape(x0, shape=[self._num_nodes, input_size * batch_size])\n",
    "        x = torch.unsqueeze(x0, dim=0)\n",
    "\n",
    "        if self._max_diffusion_step == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for support in self._supports:\n",
    "                x1 = torch.sparse.mm(support, x0)\n",
    "                x = self._concat(x, x1)\n",
    "                for k in range(2, self._max_diffusion_step + 1):\n",
    "                    x2 = 2 * torch.sparse.mm(support, x1) - x0\n",
    "                    x = self._concat(x, x2)\n",
    "                    x1, x0 = x2, x1\n",
    "\n",
    "        x = torch.reshape(x, shape=[self.num_matrices, self._num_nodes, input_size, batch_size])\n",
    "        x = torch.transpose(x, dim0=0, dim1=3)\n",
    "        x = torch.reshape(x, shape=[batch_size * self._num_nodes, input_size * self.num_matrices])\n",
    "\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.add(x, self.biases)\n",
    "        \n",
    "        return torch.reshape(x, [batch_size, self._num_nodes * output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d1713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_of_vertices, num_of_features):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "        self.W2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_features)) # (F, T)\n",
    "        self.W3 = nn.Parameter(torch.FloatTensor(in_channels)) # (F)\n",
    "\n",
    "        self.Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices)) # (N, N)\n",
    "        self.bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices)) # (1, N, N)\n",
    "\n",
    "    def forward(self, x): #X: (B, N, F, T)\n",
    "        xw = torch.matmul(x, self.W1) # (B, N, F, T) (T) -> (B, N, F)\n",
    "        txw = torch.matmul(self.W3, x).transpose(-1, -2) # (F) (B, N, F, T) -> (B, N, T) -> (B, T, N)\n",
    "        product = torch.matmul(torch.matmul(xw, self.W2), txw) # (B, N, F) (F, T) -> (B, N, T) (B, T, N) -> (B, N, N)\n",
    "\n",
    "        sig = torch.sigmoid(product + self.bs) \n",
    "        S = torch.matmul(self.Vs, sig) # (N, N) (B, N, N) -> (B, N, N)\n",
    "        S_prime = F.softmax(S, dim=1)\n",
    "        \n",
    "        return S_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472d6551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1210007700</th>\n",
       "      <th>1210008500</th>\n",
       "      <th>1210009500</th>\n",
       "      <th>1210010300</th>\n",
       "      <th>1210011300</th>\n",
       "      <th>1210013700</th>\n",
       "      <th>1220011900</th>\n",
       "      <th>1220016300</th>\n",
       "      <th>1220021100</th>\n",
       "      <th>1220025100</th>\n",
       "      <th>...</th>\n",
       "      <th>1210013800</th>\n",
       "      <th>1210013000</th>\n",
       "      <th>1210012200</th>\n",
       "      <th>1210012300</th>\n",
       "      <th>1210013100</th>\n",
       "      <th>1210013900</th>\n",
       "      <th>1210014900</th>\n",
       "      <th>1210015900</th>\n",
       "      <th>1210017100</th>\n",
       "      <th>1210018300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.70</td>\n",
       "      <td>21.00</td>\n",
       "      <td>33.54</td>\n",
       "      <td>43.21</td>\n",
       "      <td>24.31</td>\n",
       "      <td>26.54</td>\n",
       "      <td>35.91</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.78</td>\n",
       "      <td>21.76</td>\n",
       "      <td>...</td>\n",
       "      <td>24.98</td>\n",
       "      <td>24.54</td>\n",
       "      <td>32.76</td>\n",
       "      <td>43.75</td>\n",
       "      <td>32.36</td>\n",
       "      <td>32.13</td>\n",
       "      <td>35.47</td>\n",
       "      <td>22.96</td>\n",
       "      <td>26.62</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.50</td>\n",
       "      <td>38.00</td>\n",
       "      <td>25.61</td>\n",
       "      <td>43.89</td>\n",
       "      <td>22.75</td>\n",
       "      <td>24.37</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.07</td>\n",
       "      <td>25.92</td>\n",
       "      <td>22.66</td>\n",
       "      <td>...</td>\n",
       "      <td>26.22</td>\n",
       "      <td>25.14</td>\n",
       "      <td>32.15</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>35.04</td>\n",
       "      <td>26.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>27.22</td>\n",
       "      <td>29.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>24.36</td>\n",
       "      <td>35.75</td>\n",
       "      <td>24.63</td>\n",
       "      <td>21.96</td>\n",
       "      <td>22.67</td>\n",
       "      <td>...</td>\n",
       "      <td>21.55</td>\n",
       "      <td>25.16</td>\n",
       "      <td>34.37</td>\n",
       "      <td>43.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>36.32</td>\n",
       "      <td>22.42</td>\n",
       "      <td>32.40</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.00</td>\n",
       "      <td>34.39</td>\n",
       "      <td>32.70</td>\n",
       "      <td>44.15</td>\n",
       "      <td>23.64</td>\n",
       "      <td>27.76</td>\n",
       "      <td>40.06</td>\n",
       "      <td>41.00</td>\n",
       "      <td>28.65</td>\n",
       "      <td>21.68</td>\n",
       "      <td>...</td>\n",
       "      <td>25.33</td>\n",
       "      <td>24.11</td>\n",
       "      <td>33.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>29.88</td>\n",
       "      <td>34.93</td>\n",
       "      <td>37.35</td>\n",
       "      <td>21.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>32.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.20</td>\n",
       "      <td>37.65</td>\n",
       "      <td>30.75</td>\n",
       "      <td>43.66</td>\n",
       "      <td>22.76</td>\n",
       "      <td>28.04</td>\n",
       "      <td>28.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>23.00</td>\n",
       "      <td>19.92</td>\n",
       "      <td>...</td>\n",
       "      <td>24.44</td>\n",
       "      <td>24.15</td>\n",
       "      <td>34.94</td>\n",
       "      <td>28.11</td>\n",
       "      <td>33.88</td>\n",
       "      <td>33.53</td>\n",
       "      <td>39.30</td>\n",
       "      <td>30.68</td>\n",
       "      <td>27.68</td>\n",
       "      <td>30.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>27.50</td>\n",
       "      <td>36.33</td>\n",
       "      <td>29.53</td>\n",
       "      <td>39.86</td>\n",
       "      <td>26.15</td>\n",
       "      <td>20.05</td>\n",
       "      <td>25.37</td>\n",
       "      <td>18.27</td>\n",
       "      <td>21.88</td>\n",
       "      <td>18.25</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>24.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>21.95</td>\n",
       "      <td>29.47</td>\n",
       "      <td>28.47</td>\n",
       "      <td>31.52</td>\n",
       "      <td>32.13</td>\n",
       "      <td>28.22</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>25.51</td>\n",
       "      <td>37.62</td>\n",
       "      <td>30.48</td>\n",
       "      <td>41.88</td>\n",
       "      <td>26.52</td>\n",
       "      <td>23.00</td>\n",
       "      <td>31.94</td>\n",
       "      <td>16.56</td>\n",
       "      <td>24.29</td>\n",
       "      <td>19.14</td>\n",
       "      <td>...</td>\n",
       "      <td>28.65</td>\n",
       "      <td>24.49</td>\n",
       "      <td>32.52</td>\n",
       "      <td>27.44</td>\n",
       "      <td>31.76</td>\n",
       "      <td>33.00</td>\n",
       "      <td>44.68</td>\n",
       "      <td>24.89</td>\n",
       "      <td>33.04</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>29.40</td>\n",
       "      <td>28.26</td>\n",
       "      <td>31.81</td>\n",
       "      <td>39.68</td>\n",
       "      <td>26.45</td>\n",
       "      <td>24.28</td>\n",
       "      <td>28.92</td>\n",
       "      <td>18.00</td>\n",
       "      <td>23.41</td>\n",
       "      <td>21.11</td>\n",
       "      <td>...</td>\n",
       "      <td>32.26</td>\n",
       "      <td>23.88</td>\n",
       "      <td>32.02</td>\n",
       "      <td>26.94</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.10</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.70</td>\n",
       "      <td>30.21</td>\n",
       "      <td>18.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>24.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>29.02</td>\n",
       "      <td>41.93</td>\n",
       "      <td>25.49</td>\n",
       "      <td>25.38</td>\n",
       "      <td>25.57</td>\n",
       "      <td>17.61</td>\n",
       "      <td>23.63</td>\n",
       "      <td>21.70</td>\n",
       "      <td>...</td>\n",
       "      <td>27.08</td>\n",
       "      <td>27.32</td>\n",
       "      <td>34.00</td>\n",
       "      <td>26.27</td>\n",
       "      <td>27.32</td>\n",
       "      <td>29.02</td>\n",
       "      <td>37.90</td>\n",
       "      <td>27.58</td>\n",
       "      <td>30.23</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>31.77</td>\n",
       "      <td>38.30</td>\n",
       "      <td>30.65</td>\n",
       "      <td>39.34</td>\n",
       "      <td>34.20</td>\n",
       "      <td>23.70</td>\n",
       "      <td>30.08</td>\n",
       "      <td>15.28</td>\n",
       "      <td>27.10</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>21.30</td>\n",
       "      <td>32.74</td>\n",
       "      <td>33.75</td>\n",
       "      <td>27.22</td>\n",
       "      <td>32.43</td>\n",
       "      <td>39.29</td>\n",
       "      <td>29.04</td>\n",
       "      <td>34.64</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1210007700  1210008500  1210009500  1210010300  1210011300  1210013700  \\\n",
       "0          32.70       21.00       33.54       43.21       24.31       26.54   \n",
       "1          31.50       38.00       25.61       43.89       22.75       24.37   \n",
       "2          22.00       38.00       47.00       54.00       19.00       24.36   \n",
       "3          31.00       34.39       32.70       44.15       23.64       27.76   \n",
       "4          39.20       37.65       30.75       43.66       22.76       28.04   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       27.50       36.33       29.53       39.86       26.15       20.05   \n",
       "2876       25.51       37.62       30.48       41.88       26.52       23.00   \n",
       "2877       29.40       28.26       31.81       39.68       26.45       24.28   \n",
       "2878       24.00       28.21       29.02       41.93       25.49       25.38   \n",
       "2879       31.77       38.30       30.65       39.34       34.20       23.70   \n",
       "\n",
       "      1220011900  1220016300  1220021100  1220025100  ...  1210013800  \\\n",
       "0          35.91       29.00       28.78       21.76  ...       24.98   \n",
       "1          30.73       29.07       25.92       22.66  ...       26.22   \n",
       "2          35.75       24.63       21.96       22.67  ...       21.55   \n",
       "3          40.06       41.00       28.65       21.68  ...       25.33   \n",
       "4          28.10       23.25       23.00       19.92  ...       24.44   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2875       25.37       18.27       21.88       18.25  ...       28.17   \n",
       "2876       31.94       16.56       24.29       19.14  ...       28.65   \n",
       "2877       28.92       18.00       23.41       21.11  ...       32.26   \n",
       "2878       25.57       17.61       23.63       21.70  ...       27.08   \n",
       "2879       30.08       15.28       27.10       19.33  ...       26.68   \n",
       "\n",
       "      1210013000  1210012200  1210012300  1210013100  1210013900  1210014900  \\\n",
       "0          24.54       32.76       43.75       32.36       32.13       35.47   \n",
       "1          25.14       32.15       43.75       25.00       35.04       26.00   \n",
       "2          25.16       34.37       43.75       25.00       33.00       36.32   \n",
       "3          24.11       33.41       28.61       29.88       34.93       37.35   \n",
       "4          24.15       34.94       28.11       33.88       33.53       39.30   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2875       24.28       38.21       21.95       29.47       28.47       31.52   \n",
       "2876       24.49       32.52       27.44       31.76       33.00       44.68   \n",
       "2877       23.88       32.02       26.94       30.75       31.10       17.00   \n",
       "2878       27.32       34.00       26.27       27.32       29.02       37.90   \n",
       "2879       21.30       32.74       33.75       27.22       32.43       39.29   \n",
       "\n",
       "      1210015900  1210017100  1210018300  \n",
       "0          22.96       26.62       14.00  \n",
       "1          16.00       27.22       29.58  \n",
       "2          22.42       32.40       11.00  \n",
       "3          21.35       29.07       32.34  \n",
       "4          30.68       27.68       30.03  \n",
       "...          ...         ...         ...  \n",
       "2875       32.13       28.22       15.96  \n",
       "2876       24.89       33.04       11.00  \n",
       "2877       24.70       30.21       18.65  \n",
       "2878       27.58       30.23       15.41  \n",
       "2879       29.04       34.64       15.30  \n",
       "\n",
       "[2880 rows x 506 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('speed_gangnam_0.csv').drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb9d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab51e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dc6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0990e6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 506)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = np.ones((506, 506))\n",
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4b76c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0782],\n",
       "        [-0.0455],\n",
       "        [-0.0232],\n",
       "        ...,\n",
       "        [-0.0206],\n",
       "        [-0.0935],\n",
       "        [-0.0994]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "in_features = 506\n",
    "out_features = 506\n",
    "\n",
    "W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc28b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakyrelu = nn.LeakyReLU(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fcd9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.values\n",
    "h = torch.FloatTensor(h)\n",
    "Wh = torch.mm(h, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e87475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33137810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:out_features, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e2515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh1 = torch.matmul(Wh, a[:out_features, :])\n",
    "Wh2 = torch.matmul(Wh, a[out_features:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "977ce621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 2880])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Wh1 + Wh2.T\n",
    "e = leakyrelu(e)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d17de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.FloatTensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd251226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 506])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e7aa6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (506) must match the size of tensor b (2880) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m zero_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9e15\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(e)\n\u001b[1;32m----> 2\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_vec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (506) must match the size of tensor b (2880) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "zero_vec = -9e15 * torch.ones_like(e)\n",
    "attention = torch.where(adj > 0, e, zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d350d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prime = torch.matmul(zero_vec, Wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce83db42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f379f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        ...,\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20],\n",
       "        [ 1.2578e+21,  5.1637e+19, -8.4420e+20,  ...,  2.5787e+20,\n",
       "          7.2114e+20,  2.9497e+20]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c25a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.7000, 21.0000, 33.5400,  ..., 22.9600, 26.6200, 14.0000],\n",
       "        [31.5000, 38.0000, 25.6100,  ..., 16.0000, 27.2200, 29.5800],\n",
       "        [22.0000, 38.0000, 47.0000,  ..., 22.4200, 32.4000, 11.0000],\n",
       "        ...,\n",
       "        [29.4000, 28.2600, 31.8100,  ..., 24.7000, 30.2100, 18.6500],\n",
       "        [24.0000, 28.2100, 29.0200,  ..., 27.5800, 30.2300, 15.4100],\n",
       "        [31.7700, 38.3000, 30.6500,  ..., 29.0400, 34.6400, 15.3000]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea05f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b3c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 207\n",
    "num_of_features = 2880\n",
    "num_of_vertices = 207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a38b8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "W2 = nn.Parameter(torch.FloatTensor(num_of_features)) # (T)\n",
    "\n",
    "Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices)) # (N, N)\n",
    "bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc7c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2880]),\n",
       " torch.Size([207, 2880]),\n",
       " torch.Size([207]),\n",
       " torch.Size([207, 207]),\n",
       " torch.Size([1, 207, 207]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape, W2.shape, W3.shape, Vs.shape, bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ff6380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 2880])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor(traffic)\n",
    "x = torch.transpose(x, -1, -2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c804ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 506, 2880])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_prime = x.repeat(16, 1).reshape(-1, 506, 2880)\n",
    "x_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "474c3f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 506])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xw = torch.matmul(x_prime, W1)\n",
    "xw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "050ee51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 16, 16x506,2880",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xw \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m xw\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, got 16, 16x506,2880"
     ]
    }
   ],
   "source": [
    "xw = torch.matmul(xw, W2)\n",
    "xw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e43a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54672aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xw = torch.matmul(x, self.W1) # (B, N, F, T) (T) -> (B, N, F)\n",
    "        txw = torch.matmul(self.W3, x).transpose(-1, -2) # (F) (B, N, F, T) -> (B, N, T) -> (B, T, N)\n",
    "        product = torch.matmul(torch.matmul(xw, self.W2), txw) # (B, N, F) (F, T) -> (B, N, T) (B, T, N) -> (B, N, N)\n",
    "\n",
    "        sig = torch.sigmoid(product + self.bs) \n",
    "        S = torch.matmul(self.Vs, sig) # (N, N) (B, N, N) -> (B, N, N)\n",
    "        S_prime = F.softmax(S, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a568f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7153422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0488,  0.0575, -0.0026,  ...,  0.0157, -0.1635,  0.0107]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = 506\n",
    "out_features = 506\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "nn.init.xavier_normal_(W.data, gain=1.414)\n",
    "\n",
    "a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "nn.init.xavier_normal_(a.data, gain=1.414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ddbbf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.FloatTensor(df.values)\n",
    "N = h.size()[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "504ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.ones((506, 506))\n",
    "adj = torch.FloatTensor(adj)\n",
    "edge = adj.nonzero().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e50617e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.mm(h, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4111d987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20c4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11f1f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakyrelu = nn.LeakyReLU(alpha)\n",
    "edge_e = torch.exp(-leakyrelu(a.mm(edge_h).squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbfb6319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256036]), torch.Size([256036]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge.shape, edge_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71c3ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a6dfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_spmm = SpecialSpmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3442180",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_rowsum = special_spmm(edge, edge_e, torch.Size([N,N]), torch.ones(size=(N,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c91df48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 959.4462],\n",
       "        [ 286.5499],\n",
       "        [1987.9010],\n",
       "        ...,\n",
       "        [   0.0000],\n",
       "        [   0.0000],\n",
       "        [   0.0000]], grad_fn=<SpecialSpmmFunctionBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_rowsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68a92358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime = special_spmm(edge, edge_e, torch.Size([N,N]), h)\n",
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ff73a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20.7039, -22.8317,  45.6013,  ..., -18.8642, -19.1760, -36.4217],\n",
       "        [-20.7096, -22.8344,  45.6733,  ..., -18.9175, -19.1660, -36.4120],\n",
       "        [-20.7017, -22.8398,  45.5797,  ..., -18.8467, -19.1660, -36.4197],\n",
       "        ...,\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime = h_prime.div(e_rowsum)\n",
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "814425d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1201244)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "958a9635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 506])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5ce54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457280"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime.shape[0] * h_prime.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cbbeb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243055555555555"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1201244 / 1457280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e4194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
