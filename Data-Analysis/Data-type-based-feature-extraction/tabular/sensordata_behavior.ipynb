{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        continue\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:59:51.648146Z","iopub.execute_input":"2023-05-01T12:59:51.64936Z","iopub.status.idle":"2023-05-01T12:59:51.709975Z","shell.execute_reply.started":"2023-05-01T12:59:51.649309Z","shell.execute_reply":"2023-05-01T12:59:51.708461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing numpy \nimport numpy as np\n# Importing Scipy \nimport scipy as sp\n# Importing Pandas Library \nimport pandas as pd\n# import glob function to scrap files path\nfrom glob import glob\n# import display() for better visualitions of DataFrames and arrays\nfrom IPython.display import display\n# import pyplot for plotting\nimport matplotlib.pyplot as plt\nplt.style.use('bmh') # for better plots\nimport tqdm\n\n# import data_loader for data loading\nfrom data_loader import import_raw_signals, import_labels_file,normalize5,normalize2","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:59:51.712072Z","iopub.execute_input":"2023-05-01T12:59:51.712445Z","iopub.status.idle":"2023-05-01T12:59:51.748Z","shell.execute_reply.started":"2023-05-01T12:59:51.712409Z","shell.execute_reply":"2023-05-01T12:59:51.74691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Raw 데이터 불러오기\n\n### Data Tree\n\n```\n+--RawData\n|  +--acc_exp01_user01.txt\n|  +--...(61개)\n|  +--gyro_exp01_user01.txt\n|  +--...(61개)\n|  +--label_train.txt\n|  +--label_test.txt\n\n```","metadata":{}},{"cell_type":"code","source":"Raw_data_paths = sorted(glob(\"/kaggle/input/2023-ml-w10p1/RawData/*\"))\nRaw_acc_paths=Raw_data_paths[0:61]\nRaw_gyro_paths=Raw_data_paths[61:122]\n\nprint ((\"RawData folder contains in total {:d} file \").format(len(Raw_data_paths)))\nprint ((\"The first {:d} are Acceleration files:\").format(len(Raw_acc_paths)))\nprint ((\"The second {:d} are Gyroscope files:\").format(len(Raw_gyro_paths)))\nprint (\"The last file is a labels file\")\nprint (\"test labels file path is:\",Raw_data_paths[122])\nprint (\"train labels file path is:\",Raw_data_paths[123])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:42:40.792261Z","iopub.execute_input":"2023-03-12T11:42:40.793838Z","iopub.status.idle":"2023-03-12T11:42:40.80637Z","shell.execute_reply.started":"2023-03-12T11:42:40.793767Z","shell.execute_reply":"2023-03-12T11:42:40.804768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_dic={}\nraw_acc_columns=['acc_X','acc_Y','acc_Z']\nraw_gyro_columns=['gyro_X','gyro_Y','gyro_Z']\nfor path_index in range(0,61):\n        key= Raw_data_paths[path_index][-16:-4]\n        raw_acc_data_frame=import_raw_signals(Raw_data_paths[path_index],raw_acc_columns)\n        raw_gyro_data_frame=import_raw_signals(Raw_data_paths[path_index+61],raw_gyro_columns)\n        raw_signals_data_frame=pd.concat([raw_acc_data_frame, raw_gyro_data_frame], axis=1)\n        raw_dic[key]=raw_signals_data_frame","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:42:43.324362Z","iopub.execute_input":"2023-03-12T11:42:43.324778Z","iopub.status.idle":"2023-03-12T11:42:55.004294Z","shell.execute_reply.started":"2023-03-12T11:42:43.324739Z","shell.execute_reply":"2023-03-12T11:42:55.002877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('raw_dic contains %d DataFrame' % len(raw_dic))\ndisplay(raw_dic['exp01_user01'].head(3))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:42:56.515698Z","iopub.execute_input":"2023-03-12T11:42:56.516816Z","iopub.status.idle":"2023-03-12T11:42:56.551324Z","shell.execute_reply.started":"2023-03-12T11:42:56.516765Z","shell.execute_reply":"2023-03-12T11:42:56.54986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label 불러오기\n\nLabel 정보\n```\ntrain\nexperiment_number_ID : 실험 ID\nuser_number_ID : 유저 ID\nactivity_number_ID : 활동 ID\nLabel_start_point : Raw 데이터에서 행동이 시작하는 지점(시간)\nLabel_end_point : RAW 데이터에서 행동이 끝나는 지점(시간)\n\ntest\nexperiment_number_ID : 실험 ID\nuser_number_ID : 유저 ID\nLabel_start_point : Raw 데이터에서 행동이 시작하는 지점(시간)\nLabel_end_point : RAW 데이터에서 행동이 끝나는 지점(시간)\n```","metadata":{}},{"cell_type":"code","source":"train_raw_labels_columns=['experiment_number_ID','user_number_ID','activity_number_ID','Label_start_point','Label_end_point']\ntest_raw_labels_columns=['experiment_number_ID','user_number_ID','Label_start_point','Label_end_point']\n\ntest_labels_path=Raw_data_paths[122]\ntrain_labels_path=Raw_data_paths[123]\n\ntrain_Labels_Data_Frame=import_labels_file(train_labels_path,train_raw_labels_columns)\ntest_Labels_Data_Frame=import_labels_file(test_labels_path,test_raw_labels_columns)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:00.949794Z","iopub.execute_input":"2023-03-12T11:43:00.950296Z","iopub.status.idle":"2023-03-12T11:43:00.969687Z","shell.execute_reply.started":"2023-03-12T11:43:00.950243Z","shell.execute_reply":"2023-03-12T11:43:00.968185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"The first 3 rows of  train_Labels_Data_Frame:\" )\ndisplay(train_Labels_Data_Frame.head(3))\nprint(train_Labels_Data_Frame.shape)\ndisplay(test_Labels_Data_Frame.head(3))\nprint(test_Labels_Data_Frame.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:03.435667Z","iopub.execute_input":"2023-03-12T11:43:03.43611Z","iopub.status.idle":"2023-03-12T11:43:03.459559Z","shell.execute_reply.started":"2023-03-12T11:43:03.436075Z","shell.execute_reply":"2023-03-12T11:43:03.458167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터 전처리","metadata":{}},{"cell_type":"markdown","source":"### Median Filter","metadata":{}},{"cell_type":"code","source":"from scipy.signal import medfilt\n\ndef median(signal):\n    array=np.array(signal)   \n    med_filtered=sp.signal.medfilt(array, kernel_size=3)\n    return  med_filtered","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:06.299773Z","iopub.execute_input":"2023-03-12T11:43:06.300241Z","iopub.status.idle":"2023-03-12T11:43:06.854054Z","shell.execute_reply.started":"2023-03-12T11:43:06.300204Z","shell.execute_reply":"2023-03-12T11:43:06.852716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Butterworth Filter를 통한 성분분해","metadata":{}},{"cell_type":"code","source":"from scipy.fftpack import fft  \nfrom scipy.fftpack import fftfreq\nfrom scipy.fftpack import ifft\nimport math \n\nsampling_freq = 50\nnyq=sampling_freq/float(2)\nfreq1 = 0.3\nfreq2 = 20\n\n# Function name: components_selection_one_signal\n\n# Inputs: t_signal:1D numpy array (time domain signal); \n\n# Outputs: (total_component,t_DC_component , t_body_component, t_noise) \n#           type(1D array,1D array, 1D array)\n\n# cases to discuss: if the t_signal is an acceleration signal then the t_DC_component is the gravity component [Grav_acc]\n#                   if the t_signal is a gyro signal then the t_DC_component is not useful\n# t_noise component is not useful\n# if the t_signal is an acceleration signal then the t_body_component is the body's acceleration component [Body_acc]\n# if the t_signal is a gyro signal then the t_body_component is the body's angular velocity component [Body_gyro]\n\ndef components_selection_one_signal(t_signal,freq1,freq2):\n    t_signal=np.array(t_signal)\n    t_signal_length=len(t_signal)\n    f_signal=fft(t_signal)\n    freqs=np.array(sp.fftpack.fftfreq(t_signal_length, d=1/float(sampling_freq)))# frequency values between [-25hz:+25hz]\n    \n    # DC_component: f_signal values having freq between [-0.3 hz to 0 hz] and from [0 hz to 0.3hz] \n    #                                                             (-0.3 and 0.3 are included)\n    \n    # noise components: f_signal values having freq between [-25 hz to 20 hz[ and from ] 20 hz to 25 hz] \n    #                                                               (-25 and 25 hz inculded 20hz and -20hz not included)\n    \n    # selecting body_component: f_signal values having freq between [-20 hz to -0.3 hz] and from [0.3 hz to 20 hz] \n    #                                                               (-0.3 and 0.3 not included , -20hz and 20 hz included)\n    \n    \n    f_DC_signal=[] # DC_component in freq domain\n    f_body_signal=[] # body component in freq domain numpy.append(a, a[0])\n    f_noise_signal=[] # noise in freq domain\n    \n    for i in range(len(freqs)):# iterate over all available frequencies\n        \n        # selecting the frequency value\n        freq=freqs[i]\n        \n        # selecting the f_signal value associated to freq\n        value= f_signal[i]\n        \n        # Selecting DC_component values \n        if abs(freq)>0.3:# testing if freq is outside DC_component frequency ranges\n            f_DC_signal.append(float(0)) # add 0 to  the  list if it was the case (the value should not be added)                                       \n        else: # if freq is inside DC_component frequency ranges \n            f_DC_signal.append(value) # add f_signal value to f_DC_signal list\n    \n        # Selecting noise component values \n        if (abs(freq)<=20):# testing if freq is outside noise frequency ranges \n            f_noise_signal.append(float(0)) # # add 0 to  f_noise_signal list if it was the case \n        else:# if freq is inside noise frequency ranges \n            f_noise_signal.append(value) # add f_signal value to f_noise_signal\n\n        # Selecting body_component values \n        if (abs(freq)<=0.3 or abs(freq)>20):# testing if freq is outside Body_component frequency ranges\n            f_body_signal.append(float(0))# add 0 to  f_body_signal list\n        else:# if freq is inside Body_component frequency ranges\n            f_body_signal.append(value) # add f_signal value to f_body_signal list\n    \n    ################### Inverse the transformation of signals in freq domain ########################\n    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n    t_DC_component= ifft(np.array(f_DC_signal)).real\n    t_body_component= ifft(np.array(f_body_signal)).real\n    t_noise=ifft(np.array(f_noise_signal)).real\n    \n    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n                                     #  by substracting noise from t_signal (the original signal).\n    \n    # return outputs mentioned earlier\n    return (total_component,t_DC_component,t_body_component,t_noise) ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:07.693091Z","iopub.execute_input":"2023-03-12T11:43:07.694411Z","iopub.status.idle":"2023-03-12T11:43:07.7242Z","shell.execute_reply.started":"2023-03-12T11:43:07.694322Z","shell.execute_reply":"2023-03-12T11:43:07.722709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 피처모델링\n","metadata":{}},{"cell_type":"markdown","source":"### 1. 유클리드 Norm을 통한 가속도,자이로 센서 크기(Magnitude) 계산","metadata":{}},{"cell_type":"code","source":"import math\ndef mag_3_signals(x,y,z): # Euclidian magnitude\n    return [math.sqrt((x[i]**2+y[i]**2+z[i]**2)) for i in range(len(x))]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:12.740343Z","iopub.execute_input":"2023-03-12T11:43:12.740838Z","iopub.status.idle":"2023-03-12T11:43:12.747108Z","shell.execute_reply.started":"2023-03-12T11:43:12.740792Z","shell.execute_reply":"2023-03-12T11:43:12.746071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. 미분을 통한 변화량(Jerk) 계산","metadata":{}},{"cell_type":"code","source":"dt=0.02 # dt=1/50=0.02s time duration between two rows\n# Input: 1D array with lenght=N (N:unknown)\n# Output: 1D array with lenght=N-1\ndef jerk_one_signal(signal): \n        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:13.596415Z","iopub.execute_input":"2023-03-12T11:43:13.596812Z","iopub.status.idle":"2023-03-12T11:43:13.604129Z","shell.execute_reply.started":"2023-03-12T11:43:13.596778Z","shell.execute_reply":"2023-03-12T11:43:13.602566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_sig_dic={}\nraw_dic_keys=sorted(raw_dic.keys())\n\nfor key in tqdm.tqdm(raw_dic_keys):\n    raw_df=raw_dic[key]\n    time_sig_df=pd.DataFrame()\n    \n    for column in raw_df.columns:\n        t_signal=np.array(raw_df[column])\n        med_filtred=median(t_signal)\n        \n        if 'acc' in column:\n            _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred,freq1,freq2)\n            body_acc_jerk=jerk_one_signal(body_acc)\n            time_sig_df['t_body_'+column]=body_acc[:-1]\n            time_sig_df['t_grav_'+column]= grav_acc[:-1]\n            time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n        elif 'gyro' in column:\n            _,_,body_gyro,_=components_selection_one_signal(med_filtred,freq1,freq2)\n            body_gyro_jerk=jerk_one_signal(body_gyro)\n            time_sig_df['t_body_gyro_'+column[-1]]=body_gyro[:-1]\n            time_sig_df['t_body_gyro_jerk_'+column[-1]]=body_gyro_jerk\n            \n    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n                      't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n                      't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z',\n                      't_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z',\n                      't_body_gyro_jerk_X','t_body_gyro_jerk_Y','t_body_gyro_jerk_Z']\n        \n    ordered_time_sig_df=pd.DataFrame()\n        \n    for col in new_columns_ordered:\n        ordered_time_sig_df[col]=time_sig_df[col]\n        \n    for i in range(0,15,3):\n        mag_col_name=new_columns_ordered[i][:-1]+'mag'\n        col0=np.array(ordered_time_sig_df[new_columns_ordered[i]]) # copy X_component\n        col1=ordered_time_sig_df[new_columns_ordered[i+1]] # copy Y_component\n        col2=ordered_time_sig_df[new_columns_ordered[i+2]] # copy Z_component\n        mag_signal=mag_3_signals(col0,col1,col2)\n        ordered_time_sig_df[mag_col_name]=mag_signal\n        \n    time_sig_dic[key]=ordered_time_sig_df","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:43:14.373761Z","iopub.execute_input":"2023-03-12T11:43:14.374274Z","iopub.status.idle":"2023-03-12T11:44:28.346539Z","shell.execute_reply.started":"2023-03-12T11:43:14.374231Z","shell.execute_reply":"2023-03-12T11:44:28.345078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(time_sig_dic['exp01_user01'].shape)\ndisplay(time_sig_dic['exp01_user01'].describe())\ntime_sig_dic['exp01_user01'].head(3)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:45:26.682999Z","iopub.execute_input":"2023-03-12T11:45:26.683538Z","iopub.status.idle":"2023-03-12T11:45:26.819096Z","shell.execute_reply.started":"2023-03-12T11:45:26.683494Z","shell.execute_reply":"2023-03-12T11:45:26.81745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터 샘플링\n\n```\n2.56초를 기준으로 Raw 데이터 샘플링\n50%는 오버랩 하여 샘플링을 수행\n```","metadata":{}},{"cell_type":"code","source":"def Windowing_type(time_sig_dic,Labels_Data_Frame):\n    columns=time_sig_dic['exp01_user01'].columns\n    window_ID=0\n    time_dictionary_window={}\n    BA_array=np.array(Labels_Data_Frame)\n    \n    for line in tqdm.tqdm(BA_array):\n        file_key= 'exp' + normalize2(int(line[0]))  +  '_user' + normalize2(int(line[1]))\n        \n        if line.shape[0] == 5 :\n            act_ID=line[2]\n            start_point=line[3]\n            end_point = line[4]\n        else :\n            act_ID='None'\n            start_point = line[2]\n            end_point = line[3]\n        \n        for cursor in range(start_point,end_point-127,64):\n            end_point=cursor+128\n            data=np.array(time_sig_dic[file_key].iloc[cursor:end_point])\n            window=pd.DataFrame(data=data,columns=columns)\n            key='t_W'+normalize5(window_ID)+'_'+file_key+'_act'+normalize2(act_ID)\n            time_dictionary_window[key]=window\n            window_ID=window_ID+1\n    \n    return time_dictionary_window ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:45:48.194367Z","iopub.execute_input":"2023-03-12T11:45:48.194838Z","iopub.status.idle":"2023-03-12T11:45:48.206587Z","shell.execute_reply.started":"2023-03-12T11:45:48.194803Z","shell.execute_reply":"2023-03-12T11:45:48.205011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_time_dictionary_window  = Windowing_type(time_sig_dic,train_Labels_Data_Frame)\ntest_time_dictionary_window  = Windowing_type(time_sig_dic,test_Labels_Data_Frame)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:45:49.2237Z","iopub.execute_input":"2023-03-12T11:45:49.224181Z","iopub.status.idle":"2023-03-12T11:45:54.813072Z","shell.execute_reply.started":"2023-03-12T11:45:49.224143Z","shell.execute_reply":"2023-03-12T11:45:54.811599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_window = train_time_dictionary_window[sorted(train_time_dictionary_window.keys())[0]]\ntrain_window.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:46:00.369764Z","iopub.execute_input":"2023-03-12T11:46:00.37023Z","iopub.status.idle":"2023-03-12T11:46:00.399565Z","shell.execute_reply.started":"2023-03-12T11:46:00.370192Z","shell.execute_reply":"2023-03-12T11:46:00.397809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"시간 도메인 Train 데이터 수 : {}\".format(len(train_time_dictionary_window)))\nprint(\"시간 도메인 Test 데이터 수 : {}\".format(len(test_time_dictionary_window)))\nprint(\"윈도우 크기(2.56s => 128개) : {}\".format(len(train_window)))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:46:01.06249Z","iopub.execute_input":"2023-03-12T11:46:01.062913Z","iopub.status.idle":"2023-03-12T11:46:01.070264Z","shell.execute_reply.started":"2023-03-12T11:46:01.062877Z","shell.execute_reply":"2023-03-12T11:46:01.068964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.FFT(Fast Fourier Transform)을 통한 시간 도메인-> 주파수 도메인 변환","metadata":{}},{"cell_type":"code","source":"from scipy import fftpack\nfrom numpy.fft import *\n\ndef fast_fourier_transform_one_signal(t_signal):\n    complex_f_signal= fftpack.fft(t_signal)\n    amplitude_f_signal=np.abs(complex_f_signal)\n    \n    return amplitude_f_signal\n\ndef fast_fourier_transform(t_window):\n    f_window=pd.DataFrame()\n    for column in t_window.columns:\n        if 'grav' not in column:\n            t_signal=np.array(t_window[column])\n            f_signal= np.apply_along_axis(fast_fourier_transform_one_signal,0,t_signal)\n            f_window[\"f_\"+column[2:]]=f_signal\n    return f_window","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:46:01.886163Z","iopub.execute_input":"2023-03-12T11:46:01.887935Z","iopub.status.idle":"2023-03-12T11:46:01.896099Z","shell.execute_reply.started":"2023-03-12T11:46:01.887866Z","shell.execute_reply":"2023-03-12T11:46:01.894822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_frequent_dictionary_window = {'f'+key[1:] : train_t_df.pipe(fast_fourier_transform) for key, train_t_df in tqdm.tqdm(train_time_dictionary_window.items())}\ntest_frequent_dictionary_window = {'f'+key[1:] : test_t_df.pipe(fast_fourier_transform) for key, test_t_df in tqdm.tqdm(test_time_dictionary_window.items())}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T11:46:02.489695Z","iopub.execute_input":"2023-03-12T11:46:02.49011Z","iopub.status.idle":"2023-03-12T11:47:46.094636Z","shell.execute_reply.started":"2023-03-12T11:46:02.490076Z","shell.execute_reply":"2023-03-12T11:47:46.093365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_window = train_frequent_dictionary_window[sorted(train_frequent_dictionary_window.keys())[0]]\ntrain_window.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"주파수 도메인 Train 데이터 수 : {}\".format(len(train_frequent_dictionary_window)))\nprint(\"주파수 도메인 Test 데이터 수 : {}\".format(len(test_frequent_dictionary_window)))\nprint(\"피처의 갯수 : {}\".format(len(train_window)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Feature Extract","metadata":{}},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #1] Feature Engineering\n# -------------------------------------\n\n# -------------------------------------\n# Feature Engineering\n# -------------------------------------\n# 목적: 제공된 36개의 시퀀스 도메인 데이터를 기반으로 유의미한 피처를 추출한다.\n# 입력인자: 시간(time) 도메인 Feature 20개 , 주파수(frequency) 도메인 Feature 16개\n# 출력인자: 분류모델 학습을 위한 Feature\n# -------------------------------------\n\n# ------------------------------------------------------------\n# 논문에서 제안하는 Feature Engineering 방법을 따라 feature 추출\n# ------------------------------------------------------------\n#\n# mean(): Mean value\n# std(): Standard deviation\n# mad(): Median absolute deviation \n# max(): Largest value in array\n# min(): Smallest value in array\n# sma(): Signal magnitude area\n# energy(): Energy measure. Sum of the squares divided by the number of values. \n# iqr(): Interquartile range \n# entropy(): Signal entropy\n# arCoeff(): Autorregresion coefficients with Burg order equal to 4\n# correlation(): correlation coefficient between two signals\n# maxInds(): index of the frequency component with largest magnitude\n# meanFreq(): Weighted average of the frequency components to obtain a mean frequency\n# skewness(): skewness of the frequency domain signal \n# kurtosis(): kurtosis of the frequency domain signal \n# bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.\n# angle(): Angle between to vectors.\n\nimport sys\n\n# Time domain Feature Extract function\n\nfrom Feature_engineering import mean_axial,std_axial,mad_axial,max_axial,min_axial, t_sma_axial, t_energy_axial,IQR_axial,entropy_axial, t_arburg_axial, t_corr_axial\nfrom Feature_engineering import mean_mag,std_mag,mad_mag,max_mag,min_mag,t_sma_mag,t_energy_mag,IQR_mag,entropy_mag,t_arburg_mag\n\n# Frequency domain Feature Extract function\nfrom Feature_engineering import f_sma_axial,f_energy_axial,f_max_Inds_axial,f_mean_Freq_axial,f_skewness_and_kurtosis_axial,f_all_bands_energy_axial\nfrom Feature_engineering import f_sma_mag,f_energy_mag,f_max_Inds_mag,f_mean_Freq_mag,f_skewness_mag,f_kurtosis_mag\n\n# Additional Feature Extract function\nfrom Feature_engineering import angle_features\n\ndef feature_extractor(time_dictionary,freq_dictionary, condition='train') :\n    if condition is 'train' :\n        total_data = []\n        total_label = []\n    elif condition is 'test' :\n        total_data = []\n        \n    for i in tqdm.tqdm(range(len(time_dictionary))) :\n        \n        time_key = sorted(time_dictionary.keys())[i]\n        freq_key = sorted(freq_dictionary.keys())[i]\n        \n        time_window = time_dictionary[time_key]\n        freq_window = freq_dictionary[freq_key]\n        \n        if condition is 'train' :\n            window_user_id= int(time_key[-8:-6]) # extract the user id from window's key\n            window_activity_id=int(time_key[-2:]) # extract the activity id from the windows key\n        elif condition is 'test' :\n            window_user_id= int(time_key[-10:-8]) # extract the user id from window's key\n            window_activity_id= 0\n        else :\n            print(\"Error\")\n            sys.exit()\n            break;\n            \n        ##################################################################################\n        \n        \n        # Time domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n        \n        #[0,1,2] : 't_body_acc_X', 't_body_acc_Y', 't_body_acc_Z'\n        #[3,4,5] : 't_grav_acc_X','t_grav_acc_Y', 't_grav_acc_Z'\n        #[6,7,8] : 't_body_acc_jerk_X','t_body_acc_jerk_Y', 't_body_acc_jerk_Z'\n        #[9,10,11] : 't_body_gyro_X','t_body_gyro_Y', 't_body_gyro_Z'\n        #[12,13,14] : 't_body_gyro_jerk_X', 't_body_gyro_jerk_Y', 't_body_gyro_jerk_Z'\n        \n        axial_columns = time_window.columns[0:15]\n        axial_df = time_window[axial_columns] # X,Y,Z\n        \n        time_axial_features = []\n        \n        for col in range(0,15,3) : \n            \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 time_3axial_vector 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            \n            # 40 value per each 3-axial signals\n            time_3axial_vector= mean_vector + std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + AR_vector + corr_vector\n            \n            # append these features to the global list of features\n            time_axial_features= time_axial_features+ time_3axial_vector\n        \n        ##################################################################################\n        \n        # Time domain - Feature extractor - Part 2. Magnitude Features \n        \n        #[15]'t_body_acc_mag'\n        #[16]'t_grav_acc_mag'\n        #[17]'t_body_acc_jerk_mag'\n        #[18]'t_body_gyro_mag'\n        #[19]'t_body_gyro_jerk_mag'\n        \n        mag_columns = time_window.columns[15:]\n        mag_columns = time_window[mag_columns]\n        \n        time_mag_features = []\n        \n        for col in mag_columns :\n            \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 col_mag_values 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            \n            # 13 value per each t_mag_column\n            col_mag_values = [mean_value, std_value, mad_value, max_value, min_value, sma_value, \n                              energy_value,IQR_value, entropy_value]+ AR_vector\n\n            # col_mag_values will be added to the global list\n            time_mag_features= time_mag_features+ col_mag_values\n\n        \n        ##################################################################################\n        \n        # Frequency domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n        \n        #[0,1,2] : 'f_body_acc_X', 'f_body_acc_Y', 'f_body_acc_Z'\n        #[3,4,5] : 'f_body_acc_jerk_X','f_body_acc_jerk_Y', 'f_body_acc_jerk_Z'\n        #[6,7,8] : 'f_body_gyro_X','f_body_gyro_Y', 'f_body_gyro_Z'\n        #[9,10,11] : 'f_body_gyro_jerk_X','f_body_gyro_jerk_Y', 'f_body_gyro_jerk_Z'\n        \n        axial_columns=freq_window.columns[0:12]\n        axial_df=freq_window[axial_columns]\n        freq_axial_features=[]\n        \n        for col in range(0,12,3) :         \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 freq_3axial_features 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            \n            freq_3axial_features = mean_vector +std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + max_inds_vector + mean_Freq_vector + skewness_and_kurtosis_vector + bands_energy_vector\n            freq_axial_features = freq_axial_features+ freq_3axial_features\n        \n        ##################################################################################\n        \n        # Frequency domain - Feature extractor - Part 2. Magnitude Features\n        \n        #[12]'f_body_acc_mag'\n        #[13]'f_body_acc_jerk_mag'\n        #[14]'f_body_gyro_mag'\n        #[15]'f_body_gyro_jerk_mag'\n        \n        mag_columns=freq_window.columns[12:]\n        mag_columns=freq_window[mag_columns]\n        \n        freq_mag_features = []\n        \n        for col in mag_columns:\n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 col_mag_values에 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            \n            # 13 value per each t_mag_column\n            col_mag_values = [mean_value, std_value, mad_value, max_value, \n                              min_value, sma_value, energy_value,IQR_value, \n                              entropy_value, max_Inds_value, mean_Freq_value,\n                              skewness_value, kurtosis_value ]\n            \n            freq_mag_features= freq_mag_features+ col_mag_values\n        \n        ##################################################################################\n        \n        # Time domain - Feature extractor - Part 3. Additional Features \n        \n        additional_features = angle_features(time_window)\n                \n        ##################################################################################\n        \n        total_features = time_axial_features + time_mag_features + freq_axial_features + freq_mag_features + additional_features\n        \n        total_data.append(total_features)\n        if condition is 'train' :\n            total_label.append(window_activity_id)\n    \n    total_data = np.array(total_data)\n    if condition is 'train' :\n        total_label = np.array(total_label)\n    \n    if condition is 'train' :\n        return total_data, total_label\n    elif condition is 'test' :\n        return total_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, train_label = feature_extractor(train_time_dictionary_window,train_frequent_dictionary_window,condition='train')\ntest_data = feature_extractor(test_time_dictionary_window,test_frequent_dictionary_window,condition='test')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터 정규화","metadata":{}},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #2] Data Normalization\n# -------------------------------------\n\n# -------------------------------------\n# Data Normalization\n# -------------------------------------\n# 목적: 앞서 구축한 train,test 셋에 대한 Feature를 정규화한다.\n# 입력인자: train 셋에서 추출된 Feature, test 셋에서 추출된 Feature\n# 출력인자: 정규화된 Feature Vector\n# -------------------------------------\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# sklearn에서 제공하는 MinMaxScaler를 사용해 데이터 정규화를 진행한다.\n# (MinMaxScaler가 아닌 다른 정규화를 사용할 수 있다.)\n# ------------------------------------------------------------\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 분류 모델 학습 및 평가","metadata":{}},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #3] RandomForest를 이용한 분류\n# -------------------------------------\n\n# -------------------------------------\n# RandomForest를 이용한 분류\n# -------------------------------------\n# 목적: 앞서 완성한 train/test Feature를 RandomForest를 이용해 분류한다.\n# 추가 안내 : 리더보드의 베이스라인은 random_state를 0으로 하였다.\n# 입력인자: Feature vector(train/test)\n# 출력인자: 분류결과\n# -------------------------------------\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# sklearn에서 제공하는 RandomForest를 사용해 분류를 진행한다.\n# (RandomForest를가 아닌 다른 분류모델을 사용할 수 있다.)\n# ------------------------------------------------------------","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv = pd.read_csv('/kaggle/input/2023-ml-w10p1/submit.csv')\nsubmit_csv['Label'] = y_pred\nsubmit_csv['Label'] = submit_csv['Label'].astype(\"int\")\nsubmit_csv.to_csv(\"./result-rf.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}