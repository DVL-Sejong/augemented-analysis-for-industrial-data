{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_series, hidden):\n",
    "        '''\n",
    "        LSTM model with output layer to generate predictions.\n",
    "\n",
    "        Args:\n",
    "          num_series: number of input time series.\n",
    "          hidden: number of hidden units.\n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up network.\n",
    "        self.lstm = nn.LSTM(num_series, hidden, batch_first=True)\n",
    "        self.lstm.flatten_parameters()\n",
    "        self.linear = nn.Conv1d(hidden, 1, 1)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        '''Initialize hidden states for LSTM cell.'''\n",
    "        device = self.lstm.weight_ih_l0.device\n",
    "        return (torch.zeros(1, batch, self.hidden, device=device),\n",
    "                torch.zeros(1, batch, self.hidden, device=device))\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        # Set up hidden state.\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(X.shape[0])\n",
    "\n",
    "        # Apply LSTM.\n",
    "        X, hidden = self.lstm(X, hidden)\n",
    "\n",
    "        # Calculate predictions using output layer.\n",
    "        X = X.transpose(2, 1)\n",
    "        X = self.linear(X)\n",
    "        return X.transpose(2, 1), hidden\n",
    "\n",
    "\n",
    "class cLSTM(nn.Module):\n",
    "    def __init__(self, num_series, hidden):\n",
    "        '''\n",
    "        cLSTM model with one LSTM per time series.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          hidden: number of units in LSTM cell.\n",
    "        '''\n",
    "        super(cLSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(num_series, hidden) for _ in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X, hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "    def GC(self, threshold=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.\n",
    "\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "\n",
    "        Returns:\n",
    "          GC: (p x p) matrix. Entry (i, j) indicates whether variable j is\n",
    "            Granger causal of variable i.\n",
    "        '''\n",
    "        GC = [torch.norm(net.lstm.weight_ih_l0, dim=0)\n",
    "              for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC\n",
    "\n",
    "\n",
    "class cLSTMSparse(nn.Module):\n",
    "    def __init__(self, num_series, sparsity, hidden):\n",
    "        '''\n",
    "        cLSTM model that only uses specified interactions.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          sparsity: torch byte tensor indicating Granger causality, with size\n",
    "            (num_series, num_series).\n",
    "          hidden: number of units in LSTM cell.\n",
    "        '''\n",
    "        super(cLSTMSparse, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(int(torch.sum(sparsity[i].int())), hidden)\n",
    "            for i in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X[:, :, self.sparsity[i]], hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "def prox_update(network, lam, lr):\n",
    "    '''Perform in place proximal update on first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    norm = torch.norm(W, dim=0, keepdim=True)\n",
    "    W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n",
    "              * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    network.lstm.flatten_parameters()\n",
    "\n",
    "\n",
    "def regularize(network, lam):\n",
    "    '''Calculate regularization term for first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    return lam * torch.sum(torch.norm(W, dim=0))\n",
    "\n",
    "\n",
    "def ridge_regularize(network, lam):\n",
    "    '''Apply ridge penalty at linear layer and hidden-hidden weights.'''\n",
    "    return lam * (\n",
    "        torch.sum(network.linear.weight ** 2) +\n",
    "        torch.sum(network.lstm.weight_hh_l0 ** 2))\n",
    "\n",
    "\n",
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n",
    "\n",
    "\n",
    "def arrange_input(data, context):\n",
    "    '''\n",
    "    Arrange a single time series into overlapping short sequences.\n",
    "\n",
    "    Args:\n",
    "      data: time series of shape (T, dim).\n",
    "      context: length of short sequences.\n",
    "    '''\n",
    "    assert context >= 1 and isinstance(context, int)\n",
    "    input = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                        dtype=torch.float32, device=data.device)\n",
    "    target = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                         dtype=torch.float32, device=data.device)\n",
    "    for i in range(context):\n",
    "        start = i\n",
    "        end = len(data) - context + i\n",
    "        input[:, i, :] = data[start:end]\n",
    "        target[:, i, :] = data[start+1:end+1]\n",
    "    return input.detach(), target.detach()\n",
    "\n",
    "\n",
    "def train_model_gista(clstm, X, context, lam, lam_ridge, lr, max_iter,\n",
    "                      check_every=50, r=0.8, lr_min=1e-8, sigma=0.5,\n",
    "                      monotone=False, m=10, lr_decay=0.5,\n",
    "                      begin_line_search=True, switch_tol=1e-3, verbose=1):\n",
    "    '''\n",
    "    Train cLSTM model with GISTA.\n",
    "\n",
    "    Args:\n",
    "      clstm: clstm model.\n",
    "      X: tensor of data, shape (batch, T, p).\n",
    "      context: length for short overlapping subsequences.\n",
    "      lam: parameter for nonsmooth regularization.\n",
    "      lam_ridge: parameter for ridge regularization on output layer.\n",
    "      lr: learning rate.\n",
    "      max_iter: max number of GISTA iterations.\n",
    "      check_every: how frequently to record loss.\n",
    "      r: for line search.\n",
    "      lr_min: for line search.\n",
    "      sigma: for line search.\n",
    "      monotone: for line search.\n",
    "      m: for line search.\n",
    "      lr_decay: for adjusting initial learning rate of line search.\n",
    "      begin_line_search: whether to begin with line search.\n",
    "      switch_tol: tolerance for switching to line search.\n",
    "      verbose: level of verbosity (0, 1, 2).\n",
    "    '''\n",
    "    p = clstm.p\n",
    "    clstm_copy = deepcopy(clstm)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    lr_list = [lr for _ in range(p)]\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # Calculate full loss.\n",
    "    mse_list = []\n",
    "    smooth_list = []\n",
    "    loss_list = []\n",
    "    for i in range(p):\n",
    "        net = clstm.networks[i]\n",
    "        pred, _ = net(X)\n",
    "        mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "        ridge = ridge_regularize(net, lam_ridge)\n",
    "        smooth = mse + ridge\n",
    "        mse_list.append(mse)\n",
    "        smooth_list.append(smooth)\n",
    "        with torch.no_grad():\n",
    "            nonsmooth = regularize(net, lam)\n",
    "            loss = smooth + nonsmooth\n",
    "            loss_list.append(loss)\n",
    "\n",
    "    # Set up lists for loss and mse.\n",
    "    with torch.no_grad():\n",
    "        loss_mean = sum(loss_list) / p\n",
    "        mse_mean = sum(mse_list) / p\n",
    "    train_loss_list = [loss_mean]\n",
    "    train_mse_list = [mse_mean]\n",
    "\n",
    "    # For switching to line search.\n",
    "    line_search = begin_line_search\n",
    "\n",
    "    # For line search criterion.\n",
    "    done = [False for _ in range(p)]\n",
    "    assert 0 < sigma <= 1\n",
    "    assert m > 0\n",
    "    if not monotone:\n",
    "        last_losses = [[loss_list[i]] for i in range(p)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Backpropagate errors.\n",
    "        sum([smooth_list[i] for i in range(p) if not done[i]]).backward()\n",
    "\n",
    "        # For next iteration.\n",
    "        new_mse_list = []\n",
    "        new_smooth_list = []\n",
    "        new_loss_list = []\n",
    "\n",
    "        # Perform GISTA step for each network.\n",
    "        for i in range(p):\n",
    "            # Skip if network converged.\n",
    "            if done[i]:\n",
    "                new_mse_list.append(mse_list[i])\n",
    "                new_smooth_list.append(smooth_list[i])\n",
    "                new_loss_list.append(loss_list[i])\n",
    "                continue\n",
    "\n",
    "            # Prepare for line search.\n",
    "            step = False\n",
    "            lr_it = lr_list[i]\n",
    "            net = clstm.networks[i]\n",
    "            net_copy = clstm_copy.networks[i]\n",
    "\n",
    "            while not step:\n",
    "                # Perform tentative ISTA step.\n",
    "                for param, temp_param in zip(net.parameters(),\n",
    "                                             net_copy.parameters()):\n",
    "                    temp_param.data = param - lr_it * param.grad\n",
    "\n",
    "                # Proximal update.\n",
    "                prox_update(net_copy, lam, lr_it)\n",
    "\n",
    "                # Check line search criterion.\n",
    "                pred, _ = net_copy(X)\n",
    "                mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "                ridge = ridge_regularize(net_copy, lam_ridge)\n",
    "                smooth = mse + ridge\n",
    "                with torch.no_grad():\n",
    "                    nonsmooth = regularize(net_copy, lam)\n",
    "                    loss = smooth + nonsmooth\n",
    "                    tol = (0.5 * sigma / lr_it) * sum(\n",
    "                        [torch.sum((param - temp_param) ** 2)\n",
    "                         for param, temp_param in\n",
    "                         zip(net.parameters(), net_copy.parameters())])\n",
    "\n",
    "                comp = loss_list[i] if monotone else max(last_losses[i])\n",
    "                if not line_search or (comp - loss) > tol:\n",
    "                    step = True\n",
    "                    if verbose > 1:\n",
    "                        print('Taking step, network i = %d, lr = %f'\n",
    "                              % (i, lr_it))\n",
    "                        print('Gap = %f, tol = %f' % (comp - loss, tol))\n",
    "\n",
    "                    # For next iteration.\n",
    "                    new_mse_list.append(mse)\n",
    "                    new_smooth_list.append(smooth)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                    # Adjust initial learning rate.\n",
    "                    lr_list[i] = (\n",
    "                        (lr_list[i] ** (1 - lr_decay)) * (lr_it ** lr_decay))\n",
    "\n",
    "                    if not monotone:\n",
    "                        if len(last_losses[i]) == m:\n",
    "                            last_losses[i].pop(0)\n",
    "                        last_losses[i].append(loss)\n",
    "                else:\n",
    "                    # Reduce learning rate.\n",
    "                    lr_it *= r\n",
    "                    if lr_it < lr_min:\n",
    "                        done[i] = True\n",
    "                        new_mse_list.append(mse_list[i])\n",
    "                        new_smooth_list.append(smooth_list[i])\n",
    "                        new_loss_list.append(loss_list[i])\n",
    "                        if verbose > 0:\n",
    "                            print('Network %d converged' % (i + 1))\n",
    "                        break\n",
    "\n",
    "            # Clean up.\n",
    "            net.zero_grad()\n",
    "\n",
    "            if step:\n",
    "                # Swap network parameters.\n",
    "                clstm.networks[i], clstm_copy.networks[i] = net_copy, net\n",
    "\n",
    "        # For next iteration.\n",
    "        mse_list = new_mse_list\n",
    "        smooth_list = new_smooth_list\n",
    "        loss_list = new_loss_list\n",
    "\n",
    "        # Check if all networks have converged.\n",
    "        if sum(done) == p:\n",
    "            if verbose > 0:\n",
    "                print('Done at iteration = %d' % (it + 1))\n",
    "            break\n",
    "\n",
    "        # Check progress\n",
    "        if (it + 1) % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_mean = sum(loss_list) / p\n",
    "                mse_mean = sum(mse_list) / p\n",
    "                ridge_mean = (sum(smooth_list) - sum(mse_list)) / p\n",
    "                nonsmooth_mean = (sum(loss_list) - sum(smooth_list)) / p\n",
    "\n",
    "            train_loss_list.append(loss_mean)\n",
    "            train_mse_list.append(mse_mean)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Total loss = %f' % loss_mean)\n",
    "                print('MSE = %f, Ridge = %f, Nonsmooth = %f'\n",
    "                      % (mse_mean, ridge_mean, nonsmooth_mean))\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check whether loss has increased.\n",
    "            if not line_search:\n",
    "                if train_loss_list[-2] - train_loss_list[-1] < switch_tol:\n",
    "                    line_search = True\n",
    "                    if verbose > 0:\n",
    "                        print('Switching to line search')\n",
    "\n",
    "    return train_loss_list, train_mse_list\n",
    "\n",
    "\n",
    "def train_model_adam(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Add penalty term.\n",
    "        if lam > 0:\n",
    "            loss = loss + sum([regularize(net, lam) for net in clstm.networks])\n",
    "\n",
    "        if lam_ridge > 0:\n",
    "            loss = loss + sum([ridge_regularize(net, lam_ridge)\n",
    "                               for net in clstm.networks])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def train_model_ista(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "    loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in clstm.networks])\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in clstm.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in clstm.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge)\n",
    "                     for net in clstm.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in clstm.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def train_unregularized(clstm, X, context, lr, max_iter, lookback=5,\n",
    "                        check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred, hidden = clstm(X)\n",
    "        loss = sum([loss_fn(pred[:, :, i], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "def lorenz(x, t, F):\n",
    "    '''Partial derivatives for Lorenz-96 ODE.'''\n",
    "    p = len(x)\n",
    "    dxdt = np.zeros(p)\n",
    "    for i in range(p):\n",
    "        dxdt[i] = (x[(i+1) % p] - x[(i-2) % p]) * x[(i-1) % p] - x[i] + F\n",
    "\n",
    "    return dxdt\n",
    "\n",
    "def simulate_lorenz_96(p, T, F=10.0, delta_t=0.1, sd=0.1, burn_in=1000,\n",
    "                       seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Use scipy to solve ODE.\n",
    "    x0 = np.random.normal(scale=0.01, size=p)\n",
    "    t = np.linspace(0, (T + burn_in) * delta_t, T + burn_in)\n",
    "    X = odeint(lorenz, x0, t, args=(F,))\n",
    "    X += np.random.normal(scale=sd, size=(T + burn_in, p))\n",
    "\n",
    "    # Set up Granger causality ground truth.\n",
    "    GC = np.zeros((p, p), dtype=int)\n",
    "    for i in range(p):\n",
    "        GC[i, i] = 1\n",
    "        GC[i, (i + 1) % p] = 1\n",
    "        GC[i, (i - 1) % p] = 1\n",
    "        GC[i, (i - 2) % p] = 1\n",
    "\n",
    "    return X[burn_in:], GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n",
    "X = torch.tensor(X_np[np.newaxis], dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm = cLSTM(X.shape[-1], hidden=100).cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 50----------\n",
      "Loss = 0.158456\n",
      "----------Iter = 100----------\n",
      "Loss = 0.098552\n",
      "----------Iter = 150----------\n",
      "Loss = 0.067615\n",
      "----------Iter = 200----------\n",
      "Loss = 0.049876\n",
      "----------Iter = 250----------\n",
      "Loss = 0.038691\n",
      "----------Iter = 300----------\n",
      "Loss = 0.031109\n",
      "----------Iter = 350----------\n",
      "Loss = 0.025751\n",
      "----------Iter = 400----------\n",
      "Loss = 0.021685\n",
      "----------Iter = 450----------\n",
      "Loss = 0.018612\n",
      "----------Iter = 500----------\n",
      "Loss = 0.016332\n",
      "----------Iter = 550----------\n",
      "Loss = 0.014211\n",
      "----------Iter = 600----------\n",
      "Loss = 0.012658\n",
      "----------Iter = 650----------\n",
      "Loss = 0.011422\n",
      "----------Iter = 700----------\n",
      "Loss = 0.010225\n",
      "----------Iter = 750----------\n",
      "Loss = 0.009281\n",
      "----------Iter = 800----------\n",
      "Loss = 0.008628\n",
      "----------Iter = 850----------\n",
      "Loss = 0.008032\n",
      "----------Iter = 900----------\n",
      "Loss = 0.007571\n",
      "----------Iter = 950----------\n",
      "Loss = 0.006702\n",
      "----------Iter = 1000----------\n",
      "Loss = 0.006232\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = train_model_adam(\n",
    "    clstm, X, context=10, lr=1e-3, max_iter=1000,\n",
    "    check_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6956, 2.7441, 1.9018, 2.0652, 2.3065, 2.2541, 2.1255, 2.9637, 3.1259,\n",
       "         3.3755],\n",
       "        [3.4322, 2.7063, 2.7679, 2.1783, 2.5418, 2.4124, 2.0616, 2.3412, 2.9757,\n",
       "         3.4749],\n",
       "        [3.6531, 3.2276, 2.7412, 2.8318, 2.2289, 2.1844, 2.2241, 1.9818, 2.4396,\n",
       "         2.7696],\n",
       "        [2.9068, 3.2677, 3.3391, 2.6606, 2.6436, 2.3330, 2.1423, 2.3301, 2.2422,\n",
       "         2.3006],\n",
       "        [2.5477, 2.7785, 3.4658, 3.4745, 2.7660, 2.5806, 2.2545, 2.2974, 2.4871,\n",
       "         2.4009],\n",
       "        [2.1405, 2.3643, 2.7418, 3.3990, 3.2385, 2.6169, 2.7682, 2.0289, 2.5711,\n",
       "         2.1901],\n",
       "        [2.8793, 2.5597, 2.5356, 3.1366, 3.5985, 3.4516, 2.7866, 2.5999, 2.2356,\n",
       "         2.4670],\n",
       "        [2.5529, 1.9663, 2.3075, 2.4162, 2.8391, 3.8710, 3.6402, 2.7158, 2.7192,\n",
       "         2.2006],\n",
       "        [2.3954, 2.3670, 2.3575, 2.6172, 2.3812, 2.9542, 3.2169, 3.2566, 2.6057,\n",
       "         2.8479],\n",
       "        [2.9827, 2.2937, 2.1863, 2.4428, 2.2580, 2.3933, 2.8669, 3.4155, 3.6129,\n",
       "         2.8656]], device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.norm(net.lstm.weight_ih_l0, dim =0) for net in clstm.networks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8675e-01, 1.9006e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.2316e-03, 4.8468e-03],\n",
       "        [7.6135e-03, 2.0047e-01, 1.9706e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4282e-03],\n",
       "        [9.2250e-03, 9.8703e-03, 2.0462e-01, 1.4379e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 7.7232e-03, 4.0370e-03, 1.9580e-01, 1.3483e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 7.5062e-03, 2.4191e-03, 2.0544e-01, 1.4117e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 3.8159e-05, 0.0000e+00, 8.9998e-03, 0.0000e+00, 2.0346e-01,\n",
       "         1.6426e-02, 0.0000e+00, 0.0000e+00, 8.5028e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9841e-03, 9.9640e-04,\n",
       "         2.0332e-01, 1.9530e-02, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8096e-03,\n",
       "         4.5364e-03, 1.9358e-01, 2.3517e-02, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         6.0592e-03, 5.9731e-03, 2.0634e-01, 1.2981e-02],\n",
       "        [1.3032e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4635e-02, 1.0178e-02, 2.0047e-01]], device='cuda:0',\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.norm(net.lstm.weight_ih_l0, dim =0) for net in clstm.networks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBF_X = torch.load('lorenz_rbf_10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBF_X = RBF_X.T.reshape(1,1000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 50----------\n",
      "Loss = 71.258904\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 100----------\n",
      "Loss = 21.321062\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 150----------\n",
      "Loss = 5.888548\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 200----------\n",
      "Loss = 5.883781\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 250----------\n",
      "Loss = 5.879501\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 300----------\n",
      "Loss = 5.875596\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 350----------\n",
      "Loss = 5.871982\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 400----------\n",
      "Loss = 5.868596\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 450----------\n",
      "Loss = 5.865385\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 500----------\n",
      "Loss = 5.862312\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 550----------\n",
      "Loss = 5.859348\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 600----------\n",
      "Loss = 5.856467\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 650----------\n",
      "Loss = 5.853653\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 700----------\n",
      "Loss = 5.850891\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 750----------\n",
      "Loss = 5.848171\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 800----------\n",
      "Loss = 5.845485\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 850----------\n",
      "Loss = 5.842825\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 900----------\n",
      "Loss = 5.840187\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 950----------\n",
      "Loss = 5.837569\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1000----------\n",
      "Loss = 5.834964\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1050----------\n",
      "Loss = 5.832373\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1100----------\n",
      "Loss = 5.829793\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1150----------\n",
      "Loss = 5.827222\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1200----------\n",
      "Loss = 5.824661\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1250----------\n",
      "Loss = 5.822108\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1300----------\n",
      "Loss = 5.819561\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1350----------\n",
      "Loss = 5.817022\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1400----------\n",
      "Loss = 5.814488\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1450----------\n",
      "Loss = 5.811962\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1500----------\n",
      "Loss = 5.809440\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1550----------\n",
      "Loss = 5.806924\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1600----------\n",
      "Loss = 5.804414\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1650----------\n",
      "Loss = 5.801909\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1700----------\n",
      "Loss = 5.799409\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1750----------\n",
      "Loss = 5.796915\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1800----------\n",
      "Loss = 5.794425\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1850----------\n",
      "Loss = 5.791942\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1900----------\n",
      "Loss = 5.789463\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1950----------\n",
      "Loss = 5.786989\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2000----------\n",
      "Loss = 5.784519\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2050----------\n",
      "Loss = 5.782055\n",
      "Variable usage = 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chanyoung\\Desktop\\seqRBF\\RBFGC\\clstm.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clstm2 \u001b[39m=\u001b[39m cLSTM(RBF_X\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], hidden\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mcuda(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_loss_list2 \u001b[39m=\u001b[39m train_model_ista(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     clstm2, RBF_X, context\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, lam\u001b[39m=\u001b[39;49m\u001b[39m10.0\u001b[39;49m, lam_ridge\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, max_iter\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     check_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\chanyoung\\Desktop\\seqRBF\\RBFGC\\clstm.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain_model_ista\u001b[1;34m(clstm, X, context, lr, max_iter, lam, lam_ridge, lookback, check_every, verbose)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=462'>463</a>\u001b[0m smooth \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m ridge\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=464'>465</a>\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=465'>466</a>\u001b[0m     \u001b[39m# Take gradient step.\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=466'>467</a>\u001b[0m     smooth\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=467'>468</a>\u001b[0m     \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m clstm\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/seqRBF/RBFGC/clstm.ipynb#X10sZmlsZQ%3D%3D?line=468'>469</a>\u001b[0m         param\u001b[39m.\u001b[39mdata \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m param\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\chanyoung\\anaconda3\\envs\\cooling\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\chanyoung\\anaconda3\\envs\\cooling\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clstm2 = cLSTM(RBF_X.shape[-1], hidden=100).cuda(device=device)\n",
    "train_loss_list2 = train_model_ista(\n",
    "    clstm2, RBF_X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n",
    "    check_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
