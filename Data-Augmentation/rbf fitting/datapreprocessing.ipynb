{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def PrepareDataset(speed_matrix, \\\n",
    "                   BATCH_SIZE = 64, \\\n",
    "                   seq_len = 36, \\\n",
    "                   pred_len = 1, \\\n",
    "                   train_propotion = 0.7, \\\n",
    "                   valid_propotion = 0.2, \\\n",
    "                   masking = False, \\\n",
    "                   mask_ones_proportion = 0.8):\n",
    "\n",
    "    time_len = speed_matrix.shape[0]\n",
    "    \n",
    "    speed_matrix = speed_matrix.clip(0, 100)\n",
    "    \n",
    "    max_speed = speed_matrix.max().max()\n",
    "    speed_matrix =  speed_matrix / max_speed\n",
    "    \n",
    "    speed_sequences, speed_labels = [], []\n",
    "    for i in range(time_len - seq_len - pred_len):\n",
    "        speed_sequences.append(speed_matrix.iloc[i:i+seq_len].values)\n",
    "        speed_labels.append(speed_matrix.iloc[i+seq_len:i+seq_len+pred_len].values)\n",
    "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
    "    \n",
    "    # using zero-one mask to randomly set elements to zeros\n",
    "    if masking:\n",
    "        print('Split Speed finished. Start to generate Mask, Delta, Last_observed_X ...')\n",
    "        np.random.seed(1024)\n",
    "        Mask = np.random.choice([0,1], size=(speed_sequences.shape), p = [1 - mask_ones_proportion, mask_ones_proportion])\n",
    "        speed_sequences = np.multiply(speed_sequences, Mask)\n",
    "        \n",
    "        # temporal information\n",
    "        interval = 5 # 5 minutes\n",
    "        S = np.zeros_like(speed_sequences) # time stamps\n",
    "        for i in range(S.shape[1]):\n",
    "            S[:,i,:] = interval * i\n",
    "\n",
    "        Delta = np.zeros_like(speed_sequences) # time intervals\n",
    "        for i in range(1, S.shape[1]):\n",
    "            Delta[:,i,:] = S[:,i,:] - S[:,i-1,:]\n",
    "\n",
    "        missing_index = np.where(Mask == 0)\n",
    "\n",
    "        X_last_obsv = np.copy(speed_sequences)\n",
    "        for idx in range(missing_index[0].shape[0]):\n",
    "            i = missing_index[0][idx] \n",
    "            j = missing_index[1][idx]\n",
    "            k = missing_index[2][idx]\n",
    "            if j != 0 and j != 9:\n",
    "                Delta[i,j+1,k] = Delta[i,j+1,k] + Delta[i,j,k]\n",
    "            if j != 0:\n",
    "                X_last_obsv[i,j,k] = X_last_obsv[i,j-1,k] # last observation\n",
    "        Delta = Delta / Delta.max() # normalize\n",
    "    \n",
    "    # shuffle and split the dataset to training and testing datasets\n",
    "    print('Generate Mask, Delta, Last_observed_X finished. Start to shuffle and split dataset ...')\n",
    "    sample_size = speed_sequences.shape[0]\n",
    "    index = np.arange(sample_size, dtype = int)\n",
    "    np.random.seed(1024)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    speed_sequences = speed_sequences[index]\n",
    "    speed_labels = speed_labels[index]\n",
    "    \n",
    "    if masking:\n",
    "        X_last_obsv = X_last_obsv[index]\n",
    "        Mask = Mask[index]\n",
    "        Delta = Delta[index]\n",
    "        speed_sequences = np.expand_dims(speed_sequences, axis=1)\n",
    "        X_last_obsv = np.expand_dims(X_last_obsv, axis=1)\n",
    "        Mask = np.expand_dims(Mask, axis=1)\n",
    "        Delta = np.expand_dims(Delta, axis=1)\n",
    "        dataset_agger = np.concatenate((speed_sequences, X_last_obsv, Mask, Delta), axis = 1)\n",
    "        \n",
    "    train_index = int(np.floor(sample_size * train_propotion))\n",
    "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
    "    \n",
    "    if masking:\n",
    "        train_data, train_label = dataset_agger[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = dataset_agger[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = dataset_agger[valid_index:], speed_labels[valid_index:]\n",
    "    else:\n",
    "        train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
    "    \n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "    \n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "    \n",
    "    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    \n",
    "    X_mean = np.mean(speed_sequences, axis = 0)\n",
    "    \n",
    "    print('Finished')\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader, max_speed, X_mean"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
