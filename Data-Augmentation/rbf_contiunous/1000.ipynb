{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc22b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cd1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('C:/Users/chanyoung/Desktop/RBFfitting/data/sach/saches_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d202f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>praf</th>\n",
       "      <th>pmek</th>\n",
       "      <th>plcg</th>\n",
       "      <th>PIP2</th>\n",
       "      <th>PIP3</th>\n",
       "      <th>p44/42</th>\n",
       "      <th>pakts473</th>\n",
       "      <th>PKA</th>\n",
       "      <th>PKC</th>\n",
       "      <th>P38</th>\n",
       "      <th>pjnk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.4</td>\n",
       "      <td>54.20</td>\n",
       "      <td>15.40</td>\n",
       "      <td>24.40</td>\n",
       "      <td>12.90</td>\n",
       "      <td>9.91</td>\n",
       "      <td>27.9</td>\n",
       "      <td>449.0</td>\n",
       "      <td>35.90</td>\n",
       "      <td>93.9</td>\n",
       "      <td>48.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.9</td>\n",
       "      <td>24.60</td>\n",
       "      <td>8.66</td>\n",
       "      <td>6.49</td>\n",
       "      <td>26.20</td>\n",
       "      <td>22.70</td>\n",
       "      <td>26.7</td>\n",
       "      <td>219.0</td>\n",
       "      <td>10.70</td>\n",
       "      <td>39.6</td>\n",
       "      <td>71.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>21.10</td>\n",
       "      <td>244.00</td>\n",
       "      <td>70.40</td>\n",
       "      <td>10.40</td>\n",
       "      <td>16.8</td>\n",
       "      <td>470.0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>38.5</td>\n",
       "      <td>19.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.1</td>\n",
       "      <td>19.30</td>\n",
       "      <td>9.39</td>\n",
       "      <td>18.40</td>\n",
       "      <td>33.10</td>\n",
       "      <td>27.60</td>\n",
       "      <td>30.2</td>\n",
       "      <td>407.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.7</td>\n",
       "      <td>25.90</td>\n",
       "      <td>24.80</td>\n",
       "      <td>104.00</td>\n",
       "      <td>37.50</td>\n",
       "      <td>20.50</td>\n",
       "      <td>35.5</td>\n",
       "      <td>308.0</td>\n",
       "      <td>23.30</td>\n",
       "      <td>46.1</td>\n",
       "      <td>31.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>55.2</td>\n",
       "      <td>1.01</td>\n",
       "      <td>11.40</td>\n",
       "      <td>215.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>16.80</td>\n",
       "      <td>18.8</td>\n",
       "      <td>375.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13.9</td>\n",
       "      <td>2.48</td>\n",
       "      <td>66.10</td>\n",
       "      <td>264.00</td>\n",
       "      <td>95.60</td>\n",
       "      <td>8.98</td>\n",
       "      <td>11.9</td>\n",
       "      <td>362.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>44.1</td>\n",
       "      <td>3.52</td>\n",
       "      <td>15.70</td>\n",
       "      <td>115.00</td>\n",
       "      <td>35.90</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12.6</td>\n",
       "      <td>704.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>49.6</td>\n",
       "      <td>1.89</td>\n",
       "      <td>30.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>5.14</td>\n",
       "      <td>14.10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>19.8</td>\n",
       "      <td>3.52</td>\n",
       "      <td>27.10</td>\n",
       "      <td>43.70</td>\n",
       "      <td>8.66</td>\n",
       "      <td>15.80</td>\n",
       "      <td>83.5</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>26.4</td>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    praf   pmek   plcg    PIP2   PIP3  p44/42  pakts473     PKA    PKC   P38  \\\n",
       "0   70.4  54.20  15.40   24.40  12.90    9.91      27.9   449.0  35.90  93.9   \n",
       "1   38.9  24.60   8.66    6.49  26.20   22.70      26.7   219.0  10.70  39.6   \n",
       "2   41.0  17.00  21.10  244.00  70.40   10.40      16.8   470.0  19.60  38.5   \n",
       "3   33.1  19.30   9.39   18.40  33.10   27.60      30.2   407.0   9.56  16.0   \n",
       "4   71.7  25.90  24.80  104.00  37.50   20.50      35.5   308.0  23.30  46.1   \n",
       "..   ...    ...    ...     ...    ...     ...       ...     ...    ...   ...   \n",
       "95  55.2   1.01  11.40  215.00  47.00   16.80      18.8   375.0   1.00   1.0   \n",
       "96  13.9   2.48  66.10  264.00  95.60    8.98      11.9   362.0   1.83  14.3   \n",
       "97  44.1   3.52  15.70  115.00  35.90    5.00      12.6   704.0   7.84  12.7   \n",
       "98  49.6   1.89  30.00   24.80   5.14   14.10      16.0   241.0   1.00   1.0   \n",
       "99  19.8   3.52  27.10   43.70   8.66   15.80      83.5  1186.0   4.70  26.4   \n",
       "\n",
       "     pjnk  \n",
       "0   48.30  \n",
       "1   71.70  \n",
       "2   19.60  \n",
       "3    9.91  \n",
       "4   31.60  \n",
       "..    ...  \n",
       "95   2.09  \n",
       "96   1.37  \n",
       "97   2.76  \n",
       "98   2.50  \n",
       "99   6.04  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b4ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_row = pd.DataFrame(np.zeros((9, 11)), columns=data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e412a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_df)):\n",
    "    if i == 0:\n",
    "        con_df = pd.concat([data_df[i:i+1], empty_row])\n",
    "    else:\n",
    "        con_df2 = pd.concat([data_df[i:i+1], empty_row])\n",
    "        con_df = pd.concat([con_df, con_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56b877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df.reset_index(drop= True,inplace = True)\n",
    "con_df.to_csv('./data/GAIN_1000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cccf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_gpu = False  # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6831c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.5\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = con_df.values\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "# def xavier_init(size):\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f5ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu is True:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")       # Output is multi-variate\n",
    "else:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 2. Generator\n",
    "if use_gpu is True:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")\n",
    "else:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "\n",
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = F.relu(torch.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = F.relu(torch.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = torch.sigmoid(torch.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = F.relu(torch.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = F.relu(torch.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = torch.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = torch.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "def discriminator_loss(M, New_X, H):\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29a76165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▋                                                                           | 29/500 [00:00<00:03, 155.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Train_loss: 0.5057\n",
      "Test_loss: 0.5139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▋                                                          | 131/500 [00:00<00:01, 188.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n",
      "Train_loss: 0.3112\n",
      "Test_loss: 0.3571\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▏                                          | 229/500 [00:01<00:01, 167.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "Train_loss: 0.07864\n",
      "Test_loss: 0.116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████▏                           | 324/500 [00:01<00:01, 173.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 300\n",
      "Train_loss: 0.05712\n",
      "Test_loss: 0.06785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████▊           | 429/500 [00:02<00:00, 238.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 400\n",
      "Train_loss: 0.05327\n",
      "Test_loss: 0.05702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 185.93it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer_D = torch.optim.Adam(params=theta_D)\n",
    "optimizer_G = torch.optim.Adam(params=theta_G)\n",
    "#%% Start Iterations\n",
    "for it in tqdm(range(500)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device=\"cuda\")\n",
    "        M_mb = torch.tensor(M_mb, device=\"cuda\")\n",
    "        H_mb = torch.tensor(H_mb, device=\"cuda\")\n",
    "        New_X_mb = torch.tensor(New_X_mb, device=\"cuda\")\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb)\n",
    "        M_mb = torch.tensor(M_mb)\n",
    "        H_mb = torch.tensor(H_mb)\n",
    "        New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    optimizer_G.zero_grad()\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baf0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentationX = Data\n",
    "augmentationM = np.zeros_like(Data)\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    augmentationM[:,i] = 1.*B\n",
    "Z_mb = sample_Z(1000, 11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626b549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = augmentationM\n",
    "X_mb = augmentationX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "if use_gpu is True:\n",
    "    X_mb = torch.tensor(X_mb, device='cuda')\n",
    "    M_mb = torch.tensor(M_mb, device='cuda')\n",
    "    New_X_mb = torch.tensor(New_X_mb, device='cuda')\n",
    "else:\n",
    "    X_mb = torch.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21da330",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample\n",
    "imputed_df = pd.DataFrame(imputed_data.detach().numpy(), columns = con_df.columns)\n",
    "imputed_df.to_csv('./data/augmentation_sach_1000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6acc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>praf</th>\n",
       "      <th>pmek</th>\n",
       "      <th>plcg</th>\n",
       "      <th>PIP2</th>\n",
       "      <th>PIP3</th>\n",
       "      <th>p44/42</th>\n",
       "      <th>pakts473</th>\n",
       "      <th>PKA</th>\n",
       "      <th>PKC</th>\n",
       "      <th>P38</th>\n",
       "      <th>pjnk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.030760</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.061722</td>\n",
       "      <td>0.072336</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.189372</td>\n",
       "      <td>0.045386</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.022486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.019721</td>\n",
       "      <td>0.052258</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.044780</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.021751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.023429</td>\n",
       "      <td>0.033180</td>\n",
       "      <td>0.043784</td>\n",
       "      <td>0.042784</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.016696</td>\n",
       "      <td>0.077869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028697</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.030376</td>\n",
       "      <td>0.037435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         praf      pmek      plcg      PIP2      PIP3    p44/42  pakts473  \\\n",
       "0    0.000086  0.030760  0.009222  0.009071  0.061722  0.072336  0.000886   \n",
       "1    0.017073  0.000000  0.003032  0.017467  0.019721  0.052258  0.052455   \n",
       "2    0.009533  0.044780  0.001051  0.013617  0.005911  0.050047  0.000000   \n",
       "3    0.001068  0.005469  0.000138  0.000000  0.000000  0.009191  0.000000   \n",
       "4    0.002714  0.000000  0.000690  0.000000  0.017401  0.034198  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.001094  0.005631  0.000095  0.000000  0.000000  0.008704  0.000000   \n",
       "996  0.004476  0.000000  0.000415  0.000000  0.005245  0.000000  0.015180   \n",
       "997  0.000000  0.002750  0.000010  0.000000  0.000000  0.000000  0.002506   \n",
       "998  0.021751  0.000000  0.003051  0.023429  0.033180  0.043784  0.042784   \n",
       "999  0.016696  0.077869  0.000000  0.028697  0.014440  0.030376  0.037435   \n",
       "\n",
       "          PKA       PKC       P38      pjnk  \n",
       "0    0.189372  0.045386  0.000083  0.022486  \n",
       "1    0.000000  0.021069  0.000000  0.010703  \n",
       "2    0.000000  0.016389  0.000000  0.003523  \n",
       "3    0.000000  0.000867  0.000509  0.000000  \n",
       "4    0.000979  0.003374  0.004534  0.000000  \n",
       "..        ...       ...       ...       ...  \n",
       "995  0.000443  0.001024  0.000000  0.000436  \n",
       "996  0.000000  0.004929  0.000000  0.003313  \n",
       "997  0.000261  0.000000  0.000000  0.000000  \n",
       "998  0.019590  0.020978  0.000000  0.000000  \n",
       "999  0.000000  0.018822  0.013257  0.009467  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a940b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b1153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e319429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
