{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureRegression, self).__init__()\n",
    "        self.build(input_size)\n",
    "\n",
    "    def build(self, input_size):\n",
    "        self.W = Parameter(torch.Tensor(input_size, input_size))\n",
    "        self.b = Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        m = torch.ones(input_size, input_size).cuda() - torch.eye(input_size, input_size).cuda()\n",
    "        self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_h = F.linear(x, self.W * Variable(self.m), self.b)\n",
    "        return z_h\n",
    "\n",
    "class TemporalDecay(nn.Module):\n",
    "    def __init__(self, input_size, output_size, diag = False):\n",
    "        super(TemporalDecay, self).__init__()\n",
    "        self.diag = diag\n",
    "\n",
    "        self.build(input_size, output_size)\n",
    "\n",
    "    def build(self, input_size, output_size):\n",
    "        self.W = Parameter(torch.Tensor(output_size, input_size)).cuda()\n",
    "        self.b = Parameter(torch.Tensor(output_size)).cuda()\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        if self.diag == True:\n",
    "            assert(input_size == output_size)\n",
    "            m = torch.eye(input_size, input_size).cuda()\n",
    "            self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, d):\n",
    "        gamma = self.relu(F.linear(d, self.W, self.b))\n",
    "        gamma = torch.exp(-gamma)\n",
    "        return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator 모델\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.temp_decay_h = TemporalDecay(input_size, output_size = hidden_size, diag = False)\n",
    "        self.temp_decay_r = TemporalDecay(input_size, input_size, diag = True)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.input_size, bias=True)\n",
    "        \n",
    "        self.z_layer = FeatureRegression(self.input_size)\n",
    "        self.beta_layer = nn.Linear(self.input_size * 2, self.input_size)\n",
    "        self.grucell = nn.GRUCell(self.input_size * 2, self.hidden_size)\n",
    "        self.hidden_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def loss(self, hat, y, m):\n",
    "        return torch.sum(torch.abs((y - hat)) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        rbfs = input[:,0,::]\n",
    "        delta = input[:,1,::]\n",
    "        masks = input[:,2,::]\n",
    "\n",
    "        hid = torch.zeros((rbfs.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        x_loss = 0.0\n",
    "        imputations = []\n",
    "        c_hat_list = []\n",
    "        for i in range(rbfs.size(1)):\n",
    "\n",
    "            r = rbfs[:,i,:]\n",
    "            d = delta[:,i,:]\n",
    "            m = masks[:,i,:]\n",
    "\n",
    "            gamma_r = self.temp_decay_r(d)\n",
    "            gamma_h = self.temp_decay_h(d)\n",
    "\n",
    "            hid = hid * gamma_h\n",
    "            \n",
    "            x_hat = self.output_layer(hid)\n",
    "            x_loss += torch.sum(torch.abs(r - x_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            r_c = m * r + (1 - m) * x_hat\n",
    "\n",
    "            r_hat = self.z_layer(r_c)\n",
    "\n",
    "            x_loss += torch.sum(torch.abs(r - r_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            beta_weight = torch.cat([gamma_r, m], dim = 1)\n",
    "            beta = torch.sigmoid(self.beta_layer(beta_weight))\n",
    "\n",
    "            c_hat = beta * r_hat + (1 - beta) * x_hat\n",
    "            x_loss += torch.sum(torch.abs(r - c_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            c_c = m * r + (1 - m) * c_hat\n",
    "\n",
    "            gru_input = torch.cat([c_c, m], dim = 1)\n",
    "            c_hat_list.append(c_hat.unsqueeze(1))\n",
    "            imputations.append(c_c.unsqueeze(1))\n",
    "            \n",
    "            # GRU cell\n",
    "            self.hidden_dropout(hid)\n",
    "            hid = self.grucell(gru_input, hid)\n",
    "            \n",
    "        imputations = torch.cat(imputations, dim = 1)\n",
    "        c_hat_list = torch.cat(c_hat_list, dim = 1)\n",
    "\n",
    "        return  c_hat_list, imputations, x_loss / rbfs.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def make_deltas(masks):\n",
    "    deltas = []\n",
    "    for h in range(len(masks)):\n",
    "        if h == 0:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])])\n",
    "        else:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])] + (1-masks[h]) * deltas[-1])\n",
    "    \n",
    "    return list(deltas)\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, q):\n",
    "        self.data = dataset\n",
    "        self.q = q\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] // self.q\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,index * self.q : index * self.q + self.q,:]\n",
    "\n",
    "def missing_data_rbf(df,rbf, batch_size):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "\n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    values = torch.nan_to_num(torch.from_numpy(values).to(torch.float32))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([values.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0), rbf_x.unsqueeze_(0)], dim = 0)\n",
    "    \n",
    "    mydata  = MyDataset(dataset, batch_size)\n",
    "    data = DataLoader(mydata, batch_size, shuffle=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def val_missing_data_rbf(df,rbf):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    \n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "\n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    values = torch.nan_to_num(torch.from_numpy(values).to(torch.float32))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([values.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0), rbf_x.unsqueeze_(0)], dim = 0).unsqueeze_(0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def eval_model(model, rbf, realpath, dfpath):\n",
    "    \n",
    "    df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)\n",
    "    dataset = val_missing_data_rbf(df,rbf)\n",
    "    dataset = dataset.to(device)\n",
    "\n",
    "    real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "    real_scaler = (real - df.mean()) / df.std()\n",
    "\n",
    "    df_scaler = ((df-df.mean()) / df.std()).values\n",
    "    masks = ~np.isnan(df_scaler)\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    \n",
    "    eval_masks = ~np.isnan(real_scaler.values)\n",
    "    eval_masks = torch.from_numpy(eval_masks).to(torch.float32)\n",
    "\n",
    "    test_masks = eval_masks - masks\n",
    "    real_scaler = torch.nan_to_num(torch.from_numpy(real_scaler.values).to(torch.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    imputations, x_loss, c_hat_list = model(dataset)\n",
    "\n",
    "    Nonscale_imputataion = pd.DataFrame(imputations[0].cpu().detach() , columns= df.columns)\n",
    "    Nonscale_imputataion = (Nonscale_imputataion * df.std()) + df.mean()\n",
    "    \n",
    "    real = real.fillna(0)\n",
    "    print(\"Scale MAE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(test_masks))\n",
    "    print(\"Scale MRE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(torch.abs(real_scaler * test_masks)))\n",
    "\n",
    "    print(\"Original MAE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"Original MRE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    return Nonscale_imputataion\n",
    "\n",
    "def eval_bi_model(model,rbf, realpath, dfpath):\n",
    "    \n",
    "    df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)\n",
    "    dataset = val_missing_data_rbf(df,rbf, 1)\n",
    "    dataset = dataset.to(device)\n",
    "\n",
    "    real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "    real_scaler = (real - df.mean()) / df.std()\n",
    "\n",
    "    df_scaler = ((df-df.mean()) / df.std()).values\n",
    "    masks = ~np.isnan(df_scaler)\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    \n",
    "    eval_masks = ~np.isnan(real_scaler.values)\n",
    "    eval_masks = torch.from_numpy(eval_masks).to(torch.float32)\n",
    "\n",
    "    test_masks = eval_masks - masks\n",
    "    real_scaler = torch.nan_to_num(torch.from_numpy(real_scaler.values).to(torch.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    loss, x_loss, back_x_loss, loss_c, biimputataion = model(dataset)\n",
    "\n",
    "    Nonscale_imputataion = pd.DataFrame(biimputataion[0].cpu().detach() , columns= df.columns)\n",
    "    Nonscale_imputataion = (Nonscale_imputataion * df.std()) + df.mean()\n",
    "    \n",
    "    real = real.fillna(0)\n",
    "    print(\"Scale MAE :\", torch.sum(torch.abs(biimputataion[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(test_masks))\n",
    "    print(\"Scale MRE :\", torch.sum(torch.abs(biimputataion[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(torch.abs(real_scaler * test_masks)))\n",
    "\n",
    "    print(\"Original MAE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"Original MRE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    return Nonscale_imputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = 'air_1000_0.05_time.csv'\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = missing_data_rbf(df, rbfpath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MGRU, self).__init__()\n",
    "\n",
    "        self.temp_decay_h = TemporalDecay(input_size, output_size = hidden_size, diag = False)\n",
    "        self.temp_decay_x = TemporalDecay(input_size, input_size, diag = True)\n",
    "        self.temp_decay_r = TemporalDecay(input_size, input_size, diag = True)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.input_size, bias=True)\n",
    "        \n",
    "        self.z_layer = FeatureRegression(self.input_size)\n",
    "        self.beta_layer = nn.Linear(self.input_size * 2, self.input_size)\n",
    "        self.grucell = nn.GRUCell(self.input_size * 2, self.hidden_size)\n",
    "        self.concat_lyaer = nn.Linear(self.input_size * 2, self.input_size)\n",
    "        \n",
    "\n",
    "    def loss(self, hat, y, m):\n",
    "        return torch.sum(torch.abs((y - hat)) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        values = input[:,0,::]\n",
    "        delta = input[:,1,::]\n",
    "        masks = input[:,2,::]\n",
    "        rbfs = input[:,3,::]\n",
    "\n",
    "        hid = torch.zeros((values.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        x_loss = 0.0\n",
    "        imputations = []\n",
    "        c_hat_list = []\n",
    "        for i in range(values.size(1)):\n",
    "\n",
    "            v = values[:,i,:]\n",
    "            d = delta[:,i,:]\n",
    "            m = masks[:,i,:]\n",
    "            r = rbfs[:,i,:]\n",
    "\n",
    "            gamma_x = self.temp_decay_x(d)\n",
    "            gamma_h = self.temp_decay_h(d)\n",
    "            \n",
    "            hid = hid * gamma_h\n",
    "\n",
    "            r_hat = self.temp_decay_r(r)\n",
    "            \n",
    "            x_hat = self.output_layer(hid)\n",
    "            x_loss += torch.sum(torch.abs(v - x_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            RG = torch.cat([x_hat, r_hat], dim = 1)\n",
    "            concat_hat = self.concat_lyaer(RG)\n",
    "            x_loss += torch.sum(torch.abs(v - concat_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            x_c = m * v + (1 - m) * x_hat\n",
    "\n",
    "            z_hat = self.z_layer(x_c)\n",
    "            x_loss += torch.sum(torch.abs(v - z_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            beta_weight = torch.cat([gamma_x, m], dim = 1)\n",
    "            beta = torch.sigmoid(self.beta_layer(beta_weight))\n",
    "\n",
    "            c_hat = beta * z_hat + (1 - beta) * x_hat\n",
    "            x_loss += torch.sum(torch.abs(v - c_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            c_c = m * v + (1 - m) * c_hat\n",
    "\n",
    "            gru_input = torch.cat([c_c, m], dim = 1)\n",
    "            imputations.append(c_c.unsqueeze(dim = 1))\n",
    "            c_hat_list.append(c_hat.unsqueeze(1))\n",
    "            \n",
    "            # GRU cell\n",
    "            hid = self.grucell(gru_input, hid)\n",
    "\n",
    "        c_hat_list = torch.cat(c_hat_list, dim = 1)\n",
    "        imputations = torch.cat(imputations, dim = 1)\n",
    "        return imputations, x_loss, c_hat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MGRU(model, lr, epochs, dataset, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    progress = tqdm(range(epochs))\n",
    "    \n",
    "    imputation_list = []\n",
    "    loss_list = []\n",
    "    c_hat_list2 = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in progress:\n",
    "        batch_loss = 0.0\n",
    "        for data in dataset:\n",
    "            data = data.to(device)\n",
    "            imputations, x_loss, c_hat_list = model(data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            x_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss += x_loss\n",
    "        progress.set_description(\"loss: {}\".format(batch_loss))\n",
    "\n",
    "    return x_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = 'air_1000_0.05_time.csv'\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 110.54060363769531: 100%|██████████| 1000/1000 [15:12<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1815)\n",
      "Scale MRE : tensor(0.2620)\n",
      "Original MAE : 14.64047915524505\n",
      "Original MRE : 0.20565254274388195\n"
     ]
    }
   ],
   "source": [
    "model = MGRU(36, 64)\n",
    "model.to(device)\n",
    "loss_list = train_MGRU(model, 0.001, 1000, dataset, device)\n",
    "Nonscale_imputataion = eval_model(model,rbfpath, \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.2011)\n",
      "Scale MRE : tensor(0.2903)\n",
      "Original MAE : 16.20589607710959\n",
      "Original MRE : 0.22764171174730274\n"
     ]
    }
   ],
   "source": [
    "Nonscale_imputataion = eval_model(model,\"air_1000_0.05_time.csv\", \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1815)\n",
      "Scale MRE : tensor(0.2620)\n",
      "Original MAE : 14.64047915524505\n",
      "Original MRE : 0.20565254274388195\n"
     ]
    }
   ],
   "source": [
    "Nonscale_imputataion = eval_model(model,\"air_1000_0.05_time.csv\", \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 161.7032012939453: 100%|██████████| 1000/1000 [13:48<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1987)\n",
      "Scale MRE : tensor(0.2869)\n",
      "Original MAE : 16.07232064329993\n",
      "Original MRE : 0.22576539832068707\n"
     ]
    }
   ],
   "source": [
    "model = MGRU(36, 64)\n",
    "model.to(device)\n",
    "loss_list = train_MGRU(model, 0.0001, 1000, dataset, device)\n",
    "Nonscale_imputataion = eval_model(model,\"air_1000_0.05_time.csv\", \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 110.17478942871094: 100%|██████████| 500/500 [07:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1806)\n",
      "Scale MRE : tensor(0.2608)\n",
      "Original MAE : 14.601126131568007\n",
      "Original MRE : 0.2050997569164492\n"
     ]
    }
   ],
   "source": [
    "model = MGRU(36, 64)\n",
    "model.to(device)\n",
    "loss_list = train_MGRU(model, 0.001, 500, dataset, device)\n",
    "Nonscale_imputataion = eval_model(model,\"air_1000_0.05_time.csv\", \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfdf = pd.read_csv(\"./RBFresult/air_20_8.0_drop_0.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfdf = (rbfdf - rbfdf.mean()) / rbfdf.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfdf.to_csv(\"./RBFresult/air_20_8.0_scale.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfdf = pd.read_csv(\"./RBFresult/air_20_8.0_scale.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.833702</td>\n",
       "      <td>0.074850</td>\n",
       "      <td>0.347782</td>\n",
       "      <td>0.198880</td>\n",
       "      <td>0.178503</td>\n",
       "      <td>0.126357</td>\n",
       "      <td>0.182780</td>\n",
       "      <td>0.115592</td>\n",
       "      <td>0.094246</td>\n",
       "      <td>-0.043076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053620</td>\n",
       "      <td>0.222963</td>\n",
       "      <td>0.748346</td>\n",
       "      <td>-0.311618</td>\n",
       "      <td>0.216050</td>\n",
       "      <td>0.458980</td>\n",
       "      <td>0.099035</td>\n",
       "      <td>-0.035186</td>\n",
       "      <td>-0.343594</td>\n",
       "      <td>-0.420327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.884523</td>\n",
       "      <td>0.229706</td>\n",
       "      <td>0.530275</td>\n",
       "      <td>0.321159</td>\n",
       "      <td>0.199503</td>\n",
       "      <td>0.277548</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>0.300777</td>\n",
       "      <td>0.255071</td>\n",
       "      <td>0.051002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137924</td>\n",
       "      <td>0.259383</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>-0.233680</td>\n",
       "      <td>0.393282</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>0.245915</td>\n",
       "      <td>0.040810</td>\n",
       "      <td>-0.204572</td>\n",
       "      <td>-0.305951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.854783</td>\n",
       "      <td>0.307665</td>\n",
       "      <td>0.723294</td>\n",
       "      <td>0.406509</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.383615</td>\n",
       "      <td>0.421295</td>\n",
       "      <td>0.514432</td>\n",
       "      <td>0.393861</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379266</td>\n",
       "      <td>0.376184</td>\n",
       "      <td>0.641818</td>\n",
       "      <td>-0.129991</td>\n",
       "      <td>0.522623</td>\n",
       "      <td>0.475514</td>\n",
       "      <td>0.332519</td>\n",
       "      <td>0.239367</td>\n",
       "      <td>-0.029407</td>\n",
       "      <td>-0.161838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797969</td>\n",
       "      <td>0.382020</td>\n",
       "      <td>0.837884</td>\n",
       "      <td>0.464174</td>\n",
       "      <td>0.124071</td>\n",
       "      <td>0.477686</td>\n",
       "      <td>0.530302</td>\n",
       "      <td>0.717413</td>\n",
       "      <td>0.529407</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672896</td>\n",
       "      <td>0.510132</td>\n",
       "      <td>0.564856</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.674533</td>\n",
       "      <td>0.428275</td>\n",
       "      <td>0.443649</td>\n",
       "      <td>0.455692</td>\n",
       "      <td>0.183708</td>\n",
       "      <td>0.013497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680214</td>\n",
       "      <td>0.576192</td>\n",
       "      <td>0.800103</td>\n",
       "      <td>0.426261</td>\n",
       "      <td>0.231408</td>\n",
       "      <td>0.586270</td>\n",
       "      <td>0.669133</td>\n",
       "      <td>0.814170</td>\n",
       "      <td>0.649634</td>\n",
       "      <td>0.325653</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017126</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>0.569529</td>\n",
       "      <td>0.169826</td>\n",
       "      <td>0.824753</td>\n",
       "      <td>0.329915</td>\n",
       "      <td>0.607453</td>\n",
       "      <td>0.527583</td>\n",
       "      <td>0.433548</td>\n",
       "      <td>0.219047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>0.190555</td>\n",
       "      <td>-0.068424</td>\n",
       "      <td>-0.175941</td>\n",
       "      <td>-0.172720</td>\n",
       "      <td>-0.262101</td>\n",
       "      <td>-0.253210</td>\n",
       "      <td>-0.212252</td>\n",
       "      <td>-0.198784</td>\n",
       "      <td>-0.133938</td>\n",
       "      <td>-0.321118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178321</td>\n",
       "      <td>-0.296517</td>\n",
       "      <td>0.159499</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.162669</td>\n",
       "      <td>1.012385</td>\n",
       "      <td>0.570752</td>\n",
       "      <td>-0.349070</td>\n",
       "      <td>-0.310987</td>\n",
       "      <td>-0.580916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>0.145157</td>\n",
       "      <td>-0.140374</td>\n",
       "      <td>-0.166147</td>\n",
       "      <td>-0.169738</td>\n",
       "      <td>-0.211978</td>\n",
       "      <td>-0.234623</td>\n",
       "      <td>-0.205259</td>\n",
       "      <td>-0.168571</td>\n",
       "      <td>-0.112179</td>\n",
       "      <td>-0.292837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112672</td>\n",
       "      <td>-0.296055</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.192919</td>\n",
       "      <td>1.060401</td>\n",
       "      <td>0.780759</td>\n",
       "      <td>-0.016766</td>\n",
       "      <td>-0.257222</td>\n",
       "      <td>-0.580590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>0.093035</td>\n",
       "      <td>-0.186031</td>\n",
       "      <td>-0.111841</td>\n",
       "      <td>-0.133577</td>\n",
       "      <td>-0.143286</td>\n",
       "      <td>-0.174276</td>\n",
       "      <td>-0.049471</td>\n",
       "      <td>-0.117937</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>-0.230917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055072</td>\n",
       "      <td>-0.295588</td>\n",
       "      <td>0.096265</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.268784</td>\n",
       "      <td>1.174698</td>\n",
       "      <td>1.114924</td>\n",
       "      <td>0.572203</td>\n",
       "      <td>-0.168022</td>\n",
       "      <td>-0.580264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>0.043069</td>\n",
       "      <td>-0.138666</td>\n",
       "      <td>-0.043633</td>\n",
       "      <td>-0.115748</td>\n",
       "      <td>-0.132931</td>\n",
       "      <td>-0.144740</td>\n",
       "      <td>0.291237</td>\n",
       "      <td>-0.108921</td>\n",
       "      <td>-0.020099</td>\n",
       "      <td>-0.163647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>-0.295117</td>\n",
       "      <td>0.094204</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>0.279773</td>\n",
       "      <td>1.406187</td>\n",
       "      <td>1.501001</td>\n",
       "      <td>1.218251</td>\n",
       "      <td>-0.055078</td>\n",
       "      <td>-0.579938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.051307</td>\n",
       "      <td>0.081811</td>\n",
       "      <td>-0.109469</td>\n",
       "      <td>-0.170510</td>\n",
       "      <td>-0.125312</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>-0.131353</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>-0.052164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042971</td>\n",
       "      <td>-0.294642</td>\n",
       "      <td>0.122101</td>\n",
       "      <td>0.235977</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>1.471956</td>\n",
       "      <td>1.590799</td>\n",
       "      <td>1.611849</td>\n",
       "      <td>-0.009876</td>\n",
       "      <td>-0.579613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        001001    001002    001003    001004    001005    001006    001007  \\\n",
       "0     0.833702  0.074850  0.347782  0.198880  0.178503  0.126357  0.182780   \n",
       "1     0.884523  0.229706  0.530275  0.321159  0.199503  0.277548  0.320911   \n",
       "2     0.854783  0.307665  0.723294  0.406509  0.139408  0.383615  0.421295   \n",
       "3     0.797969  0.382020  0.837884  0.464174  0.124071  0.477686  0.530302   \n",
       "4     0.680214  0.576192  0.800103  0.426261  0.231408  0.586270  0.669133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8754  0.190555 -0.068424 -0.175941 -0.172720 -0.262101 -0.253210 -0.212252   \n",
       "8755  0.145157 -0.140374 -0.166147 -0.169738 -0.211978 -0.234623 -0.205259   \n",
       "8756  0.093035 -0.186031 -0.111841 -0.133577 -0.143286 -0.174276 -0.049471   \n",
       "8757  0.043069 -0.138666 -0.043633 -0.115748 -0.132931 -0.144740  0.291237   \n",
       "8758  0.002258 -0.051307  0.081811 -0.109469 -0.170510 -0.125312  0.701550   \n",
       "\n",
       "        001008    001009    001010  ...    001027    001028    001029  \\\n",
       "0     0.115592  0.094246 -0.043076  ... -0.053620  0.222963  0.748346   \n",
       "1     0.300777  0.255071  0.051002  ...  0.137924  0.259383  0.751663   \n",
       "2     0.514432  0.393861  0.173122  ...  0.379266  0.376184  0.641818   \n",
       "3     0.717413  0.529407  0.288889  ...  0.672896  0.510132  0.564856   \n",
       "4     0.814170  0.649634  0.325653  ...  1.017126  0.516252  0.569529   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8754 -0.198784 -0.133938 -0.321118  ...  0.178321 -0.296517  0.159499   \n",
       "8755 -0.168571 -0.112179 -0.292837  ...  0.112672 -0.296055  0.109013   \n",
       "8756 -0.117937 -0.060297 -0.230917  ...  0.055072 -0.295588  0.096265   \n",
       "8757 -0.108921 -0.020099 -0.163647  ...  0.003419 -0.295117  0.094204   \n",
       "8758 -0.131353 -0.012837 -0.052164  ... -0.042971 -0.294642  0.122101   \n",
       "\n",
       "        001030    001031    001032    001033    001034    001035    001036  \n",
       "0    -0.311618  0.216050  0.458980  0.099035 -0.035186 -0.343594 -0.420327  \n",
       "1    -0.233680  0.393282  0.513269  0.245915  0.040810 -0.204572 -0.305951  \n",
       "2    -0.129991  0.522623  0.475514  0.332519  0.239367 -0.029407 -0.161838  \n",
       "3     0.003552  0.674533  0.428275  0.443649  0.455692  0.183708  0.013497  \n",
       "4     0.169826  0.824753  0.329915  0.607453  0.527583  0.433548  0.219047  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8754 -0.006098  0.162669  1.012385  0.570752 -0.349070 -0.310987 -0.580916  \n",
       "8755 -0.013756  0.192919  1.060401  0.780759 -0.016766 -0.257222 -0.580590  \n",
       "8756  0.042617  0.268784  1.174698  1.114924  0.572203 -0.168022 -0.580264  \n",
       "8757  0.133922  0.279773  1.406187  1.501001  1.218251 -0.055078 -0.579938  \n",
       "8758  0.235977  0.240800  1.471956  1.590799  1.611849 -0.009876 -0.579613  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = 'air_20_8.0_scale.csv'\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 113.62510681152344: 100%|██████████| 1000/1000 [13:40<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1776)\n",
      "Scale MRE : tensor(0.2564)\n",
      "Original MAE : 14.322701748137124\n",
      "Original MRE : 0.20118877273298785\n"
     ]
    }
   ],
   "source": [
    "model = MGRU(36, 64)\n",
    "model.to(device)\n",
    "loss_list = train_MGRU(model, 0.001, 1000, dataset, device)\n",
    "Nonscale_imputataion = eval_model(model, rbfpath, \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 97.78351593017578: 100%|██████████| 1000/1000 [13:38<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1771)\n",
      "Scale MRE : tensor(0.2556)\n",
      "Original MAE : 14.275960990012232\n",
      "Original MRE : 0.2005322125441967\n"
     ]
    }
   ],
   "source": [
    "model = MGRU(36, 64)\n",
    "model.to(device)\n",
    "loss_list = train_MGRU(model, 0.001, 1000, dataset, device)\n",
    "Nonscale_imputataion = eval_model(model, rbfpath, \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiMGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiMGRU, self).__init__()\n",
    "        \n",
    "        self.fmGRU = MGRU(input_size, hidden_size)\n",
    "        self.bmGRU = MGRU(input_size, hidden_size)\n",
    "    \n",
    "    def get_consistency_loss(self, pred_f, pred_b):\n",
    "        loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
    "        return loss\n",
    "    \n",
    "    def backdirect_data(self, tensor_):\n",
    "        if tensor_.dim() <= 1:\n",
    "            return tensor_\n",
    "    \n",
    "        indices = range(tensor_.size()[2])[::-1]\n",
    "        indices = Variable(torch.LongTensor(indices), requires_grad = False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            indices = indices.cuda()\n",
    "\n",
    "        return tensor_.index_select(2, indices)\n",
    "    \n",
    "    def backdirect_imputation(self, tensor_):\n",
    "        if tensor_.dim() <= 1:\n",
    "            return tensor_\n",
    "    \n",
    "        indices = range(tensor_.size()[1])[::-1]\n",
    "        indices = Variable(torch.LongTensor(indices), requires_grad = False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            indices = indices.cuda()\n",
    "\n",
    "        return tensor_.index_select(1, indices)\n",
    "\n",
    "    def forward(self, dataset):\n",
    "        back_dataset = self.backdirect_data(dataset)\n",
    "\n",
    "        x_imputataions, x_loss, C_hat = self.fmGRU(dataset)\n",
    "        back_x_imputataions, back_x_loss, back_C_hat = self.bmGRU(back_dataset)\n",
    "\n",
    "        loss_c = self.get_consistency_loss(x_imputataions, self.backdirect_imputation(back_x_imputataions))\n",
    "        loss = x_loss + back_x_loss + loss_c\n",
    "\n",
    "        biimputataion = (x_imputataions +  self.backdirect_imputation(back_x_imputataions)) / 2\n",
    "\n",
    "        return  loss, x_loss, back_x_loss, loss_c, biimputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BiMGRU(model, lr, epochs, dataset, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    progress = tqdm(range(epochs))\n",
    "    \n",
    "    imputation_list = []\n",
    "    loss_list = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in progress:\n",
    "        batch_loss = 0.0\n",
    "        batch_f_loss = 0.0\n",
    "        batch_b_loss = 0.0\n",
    "        batch_c_loss = 0.0\n",
    "        for data in dataset:\n",
    "            data = data.to(device)\n",
    "            loss, x_loss, back_x_loss, loss_c, biimputataion = model(data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            imputation_list.append(biimputataion)\n",
    "\n",
    "            batch_loss += loss\n",
    "            batch_f_loss += x_loss\n",
    "            batch_c_loss += loss_c\n",
    "            batch_b_loss += back_x_loss\n",
    "            \n",
    "        progress.set_description(\"loss: {}, f_MGRU loss : {}, b_MGRU loss : {}, consistency Loss : {}\".format(batch_loss, batch_f_loss, batch_b_loss, batch_c_loss))\n",
    "        loss_list.append(loss)\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bieval_model(model, rbf, realpath, dfpath):\n",
    "    \n",
    "    df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)\n",
    "    dataset = val_missing_data_rbf(df,rbf)\n",
    "    dataset = dataset.to(device)\n",
    "\n",
    "    real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "    real_scaler = (real - df.mean()) / df.std()\n",
    "\n",
    "    df_scaler = ((df-df.mean()) / df.std()).values\n",
    "    masks = ~np.isnan(df_scaler)\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    \n",
    "    eval_masks = ~np.isnan(real_scaler.values)\n",
    "    eval_masks = torch.from_numpy(eval_masks).to(torch.float32)\n",
    "\n",
    "    test_masks = eval_masks - masks\n",
    "    real_scaler = torch.nan_to_num(torch.from_numpy(real_scaler.values).to(torch.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    loss, x_loss, back_x_loss, loss_c, biimputataion = model(dataset)\n",
    "\n",
    "    Nonscale_imputataion = pd.DataFrame(biimputataion[0].cpu().detach() , columns= df.columns)\n",
    "    Nonscale_imputataion = (Nonscale_imputataion * df.std()) + df.mean()\n",
    "    \n",
    "    real = real.fillna(0)\n",
    "    print(\"Scale MAE :\", torch.sum(torch.abs(biimputataion[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(test_masks))\n",
    "    print(\"Scale MRE :\", torch.sum(torch.abs(biimputataion[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(torch.abs(real_scaler * test_masks)))\n",
    "\n",
    "    print(\"Original MAE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"Original MRE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    return Nonscale_imputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = 'air_1000_0.05_time.csv'\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 233.22569274902344, f_MGRU loss : 116.21894836425781, b_MGRU loss : 116.94601440429688, consistency Loss : 0.060727380216121674: 100%|██████████| 600/600 [15:42<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.1779)\n",
      "Scale MRE : tensor(0.2568)\n",
      "Original MAE : 14.371936851229467\n",
      "Original MRE : 0.201880370599139\n"
     ]
    }
   ],
   "source": [
    "bimodel = BiMGRU(36, 64)\n",
    "bimodel.to(device)\n",
    "biloss_list = train_BiMGRU(bimodel, 0.001, 600, dataset, device)\n",
    "biNonscale_imputataion = Bieval_model(bimodel, rbfpath, \"pm25_ground.csv\", \"pm25_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
