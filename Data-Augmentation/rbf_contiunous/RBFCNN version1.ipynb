{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c14b8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bc8f267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_feature, image_height, time_lag, device):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.num_feature = num_feature\n",
    "        self.time_lag = time_lag\n",
    "        self.image_height = image_height\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.num_feature, out_channels = 1,\n",
    "                               kernel_size = (1, self.time_lag) , stride=(self.image_height, 1), device = device)\n",
    "        \n",
    "        #self.networks = nn.ModuleList([self.conv1 for _ in range(self.num_feature)])\n",
    "        \n",
    "        # self.fc1 = nn.Linear()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.conv1(x)\n",
    "        pred = pred.flatten() # size : (target_num)\n",
    "        \n",
    "            \n",
    "        return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "de20de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFCNN(nn.Module):\n",
    "    def __init__(self, num_feature, image_height, time_lag, device):\n",
    "        super(RBFCNN, self).__init__()\n",
    "        \n",
    "        self.num_feature = num_feature\n",
    "        self.time_lag = time_lag\n",
    "        self.image_height = image_height\n",
    "        \n",
    "        self.networks = nn.ModuleList([\n",
    "            CNN(self.num_feature, self.image_height, time_lag, device) for _ in range(self.num_feature)])\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        pred = [self.networks[i](X).flatten() for i in range(self.num_feature)]\n",
    "        pred = torch.stack(pred)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def GC(self, threshold=True):\n",
    "\n",
    "        GC = [torch.norm(net.conv1.weight, dim=-1)\n",
    "              for net in self.networks]\n",
    "        \n",
    "        GC = torch.stack(GC)\n",
    "        GC = GC.view((self.num_feature, self.num_feature))\n",
    "        \n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d2b0693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "def image_tensor(path, data_number):\n",
    "    x_list = []\n",
    "    for i in range(data_number):\n",
    "        #img = np.asarray(Image.open(path.format(i)), dtype = float)\n",
    "        img = Image.open(path.format(i))\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        tensor_image = convert_tensor(img)\n",
    "        x_list.append(tensor_image[0])\n",
    "    data = torch.stack(x_list)\n",
    "    data = data.view(1, 10, tensor_image.size()[-2], tensor_image.size()[-1]).to(device)\n",
    "    \n",
    "    return data\n",
    "\n",
    "path = './data/{}_value.jpg'\n",
    "data = image_tensor(path, 10)\n",
    "data = data.to(device= device)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "06fb4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(network, lam): # 아직 적용하지 않음\n",
    "    W = network.conv1.weight\n",
    "    return lam * torch.sum(torch.norm(W, dim=0))\n",
    "\n",
    "def restore_parameters(model, best_model):\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n",
    "        \n",
    "def prox_update(network, lam, lr):\n",
    "    W = network.conv1.weight\n",
    "    norm = torch.norm(W, dim=0, keepdim=True)\n",
    "    W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n",
    "              * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "\n",
    "def ridge_regularize(network, lam):  # 적용 -> 이게 맞나?\n",
    "    \n",
    "    return lam * (torch.sum(network.conv1.weight ** 2, dtype = float))   \n",
    "    '''\n",
    "    return lam * (\n",
    "        torch.sum(network.linear.weight ** 2) +\n",
    "        torch.sum(network.lstm.weight_hh_l0 ** 2))\n",
    "    '''\n",
    "\n",
    "def train_model_ista(rbfcnn, X, Y, lr, max_iter, lam=0.0, lam_ridge=0.0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "    feature_num = X.size()[-3]\n",
    "    \n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [rbfcnn.networks[i](X) for i in range(feature_num)]\n",
    "    loss = sum([loss_fn(pred[i], Y[i]) for i in range(feature_num)])\n",
    "               \n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in rbfcnn.networks])\n",
    "    smooth = loss + ridge # loss : differentiable, ridge non-differentiable\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in rbfcnn.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in rbfcnn.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        rbfcnn.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [rbfcnn.networks[i](X) for i in range(feature_num)]\n",
    "        loss = sum([loss_fn(pred[i], Y[i]) for i in range(feature_num)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge) for net in rbfcnn.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in rbfcnn.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / feature_num\n",
    "            \n",
    "            # mean_loss = loss / feature_num\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(rbfcnn.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(rbfcnn)\n",
    "                \n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(rbfcnn, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b978fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_data(dataflame, time_lag, device):\n",
    "    df = dataflame.iloc()[time_lag - 1:]\n",
    "    target = df.values.T\n",
    "    target = torch.tensor(target, device = device, dtype = torch.float32)\n",
    "    \n",
    "    return target\n",
    "df = pd.read_csv('./data/simulate_lorenz_96_100.csv')\n",
    "target = target_data(df, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89bb3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RBFCNN(10, 33, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e26a9af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 50----------\n",
      "Loss = 22.831360\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 100----------\n",
      "Loss = 21.862729\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 150----------\n",
      "Loss = 21.069839\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 200----------\n",
      "Loss = 20.420808\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 250----------\n",
      "Loss = 19.889532\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 300----------\n",
      "Loss = 19.454646\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 350----------\n",
      "Loss = 19.098666\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 400----------\n",
      "Loss = 18.807269\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 450----------\n",
      "Loss = 18.568744\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 500----------\n",
      "Loss = 18.373492\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 550----------\n",
      "Loss = 18.213669\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 600----------\n",
      "Loss = 18.082840\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 650----------\n",
      "Loss = 17.975754\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 700----------\n",
      "Loss = 17.888089\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 750----------\n",
      "Loss = 17.816333\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 800----------\n",
      "Loss = 17.757594\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 850----------\n",
      "Loss = 17.709517\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 900----------\n",
      "Loss = 17.670158\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 950----------\n",
      "Loss = 17.637941\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1000----------\n",
      "Loss = 17.611571\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1050----------\n",
      "Loss = 17.589983\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1100----------\n",
      "Loss = 17.572314\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1150----------\n",
      "Loss = 17.557849\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1200----------\n",
      "Loss = 17.546010\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1250----------\n",
      "Loss = 17.536319\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1300----------\n",
      "Loss = 17.528384\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1350----------\n",
      "Loss = 17.521892\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1400----------\n",
      "Loss = 17.516576\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1450----------\n",
      "Loss = 17.512224\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1500----------\n",
      "Loss = 17.508661\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1550----------\n",
      "Loss = 17.505745\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1600----------\n",
      "Loss = 17.503358\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1650----------\n",
      "Loss = 17.501407\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1700----------\n",
      "Loss = 17.499806\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1750----------\n",
      "Loss = 17.498499\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1800----------\n",
      "Loss = 17.497424\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1850----------\n",
      "Loss = 17.496552\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1900----------\n",
      "Loss = 17.495831\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 1950----------\n",
      "Loss = 17.495242\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2000----------\n",
      "Loss = 17.494763\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2050----------\n",
      "Loss = 17.494370\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2100----------\n",
      "Loss = 17.494044\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2150----------\n",
      "Loss = 17.493784\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2200----------\n",
      "Loss = 17.493565\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2250----------\n",
      "Loss = 17.493390\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2300----------\n",
      "Loss = 17.493246\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2350----------\n",
      "Loss = 17.493126\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2400----------\n",
      "Loss = 17.493027\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2450----------\n",
      "Loss = 17.492949\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2500----------\n",
      "Loss = 17.492885\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2550----------\n",
      "Loss = 17.492831\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2600----------\n",
      "Loss = 17.492787\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2650----------\n",
      "Loss = 17.492754\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2700----------\n",
      "Loss = 17.492725\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2750----------\n",
      "Loss = 17.492702\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2800----------\n",
      "Loss = 17.492679\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2850----------\n",
      "Loss = 17.492662\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2900----------\n",
      "Loss = 17.492651\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 2950----------\n",
      "Loss = 17.492641\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3000----------\n",
      "Loss = 17.492632\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3050----------\n",
      "Loss = 17.492625\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3100----------\n",
      "Loss = 17.492616\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3150----------\n",
      "Loss = 17.492613\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3200----------\n",
      "Loss = 17.492609\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3250----------\n",
      "Loss = 17.492606\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3300----------\n",
      "Loss = 17.492604\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3350----------\n",
      "Loss = 17.492599\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3400----------\n",
      "Loss = 17.492599\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3450----------\n",
      "Loss = 17.492598\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3500----------\n",
      "Loss = 17.492596\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3550----------\n",
      "Loss = 17.492596\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3600----------\n",
      "Loss = 17.492595\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3650----------\n",
      "Loss = 17.492595\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3700----------\n",
      "Loss = 17.492592\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3750----------\n",
      "Loss = 17.492593\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3800----------\n",
      "Loss = 17.492593\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3850----------\n",
      "Loss = 17.492590\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3900----------\n",
      "Loss = 17.492592\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 3950----------\n",
      "Loss = 17.492590\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 4000----------\n",
      "Loss = 17.492590\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 4050----------\n",
      "Loss = 17.492590\n",
      "Variable usage = 0.00%\n",
      "----------Iter = 4100----------\n",
      "Loss = 17.492590\n",
      "Variable usage = 0.00%\n",
      "Stopping early\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(22.8314, device='cuda:0', dtype=torch.float64),\n",
       " tensor(21.8627, device='cuda:0', dtype=torch.float64),\n",
       " tensor(21.0698, device='cuda:0', dtype=torch.float64),\n",
       " tensor(20.4208, device='cuda:0', dtype=torch.float64),\n",
       " tensor(19.8895, device='cuda:0', dtype=torch.float64),\n",
       " tensor(19.4546, device='cuda:0', dtype=torch.float64),\n",
       " tensor(19.0987, device='cuda:0', dtype=torch.float64),\n",
       " tensor(18.8073, device='cuda:0', dtype=torch.float64),\n",
       " tensor(18.5687, device='cuda:0', dtype=torch.float64),\n",
       " tensor(18.3735, device='cuda:0', dtype=torch.float64),\n",
       " tensor(18.2137, device='cuda:0', dtype=torch.float64),\n",
       " tensor(18.0828, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.9758, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.8881, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.8163, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.7576, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.7095, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.6702, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.6379, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.6116, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5900, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5723, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5578, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5460, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5363, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5284, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5219, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5166, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5122, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5087, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5057, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5034, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.5014, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4998, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4985, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4974, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4966, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4958, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4952, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4948, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4944, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4940, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4938, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4936, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4934, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4932, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4931, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4930, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4929, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4929, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4928, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4928, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4928, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4927, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4927, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4927, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4927, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4927, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64),\n",
       " tensor(17.4926, device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_ista(model, data, target, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n",
    "    check_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95ca77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
