{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class FeatureRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureRegression, self).__init__()\n",
    "        self.build(input_size)\n",
    "\n",
    "    def build(self, input_size):\n",
    "        self.W = Parameter(torch.Tensor(input_size, input_size))\n",
    "        self.b = Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        m = torch.ones(input_size, input_size).cuda() - torch.eye(input_size, input_size).cuda()\n",
    "        self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_h = F.linear(x, self.W * Variable(self.m), self.b)\n",
    "        return z_h\n",
    "\n",
    "class TemporalDecay(nn.Module):\n",
    "    def __init__(self, input_size, output_size, diag = False):\n",
    "        super(TemporalDecay, self).__init__()\n",
    "        self.diag = diag\n",
    "\n",
    "        self.build(input_size, output_size)\n",
    "\n",
    "    def build(self, input_size, output_size):\n",
    "        self.W = Parameter(torch.Tensor(output_size, input_size)).cuda()\n",
    "        self.b = Parameter(torch.Tensor(output_size)).cuda()\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        if self.diag == True:\n",
    "            assert(input_size == output_size)\n",
    "            m = torch.eye(input_size, input_size).cuda()\n",
    "            self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, d):\n",
    "        gamma = self.relu(F.linear(d, self.W, self.b))\n",
    "        gamma = torch.exp(-gamma)\n",
    "        return gamma\n",
    "\n",
    "# Generator 모델\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.temp_decay_h = TemporalDecay(input_size, output_size = hidden_size, diag = False)\n",
    "        self.temp_decay_r = TemporalDecay(input_size, input_size, diag = True)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.input_size, bias=True)\n",
    "        \n",
    "        self.z_layer = FeatureRegression(self.input_size)\n",
    "        self.beta_layer = nn.Linear(self.input_size * 2, self.input_size)\n",
    "        self.grucell = nn.GRUCell(self.input_size * 2, self.hidden_size)\n",
    "        self.hidden_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def loss(self, hat, y, m):\n",
    "        return torch.sum(torch.abs((y - hat)) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        rbfs = input[:,0,::]\n",
    "        delta = input[:,1,::]\n",
    "        masks = input[:,2,::]\n",
    "\n",
    "        hid = torch.zeros((rbfs.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        x_loss = 0.0\n",
    "        imputations = []\n",
    "        c_hat_list = []\n",
    "        for i in range(rbfs.size(1)):\n",
    "\n",
    "            r = rbfs[:,i,:]\n",
    "            d = delta[:,i,:]\n",
    "            m = masks[:,i,:]\n",
    "\n",
    "            gamma_r = self.temp_decay_r(d)\n",
    "            gamma_h = self.temp_decay_h(d)\n",
    "\n",
    "            hid = hid * gamma_h\n",
    "            \n",
    "            x_hat = self.output_layer(hid)\n",
    "            x_loss += torch.sum(torch.abs(r - x_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            r_c = m * r + (1 - m) * x_hat\n",
    "\n",
    "            r_hat = self.z_layer(r_c)\n",
    "\n",
    "            x_loss += torch.sum(torch.abs(r - r_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            beta_weight = torch.cat([gamma_r, m], dim = 1)\n",
    "            beta = torch.sigmoid(self.beta_layer(beta_weight))\n",
    "\n",
    "            c_hat = beta * r_hat + (1 - beta) * x_hat\n",
    "            x_loss += torch.sum(torch.abs(r - c_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            c_c = m * r + (1 - m) * c_hat\n",
    "\n",
    "            gru_input = torch.cat([c_c, m], dim = 1)\n",
    "            c_hat_list.append(c_hat.unsqueeze(1))\n",
    "            imputations.append(c_c.unsqueeze(1))\n",
    "            \n",
    "            # GRU cell\n",
    "            self.hidden_dropout(hid)\n",
    "            hid = self.grucell(gru_input, hid)\n",
    "            \n",
    "        imputations = torch.cat(imputations, dim = 1)\n",
    "        c_hat_list = torch.cat(c_hat_list, dim = 1)\n",
    "\n",
    "        return  c_hat_list, imputations, x_loss / rbfs.size(1)\n",
    "\n",
    "\n",
    "# Discriminator 모델\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim, h_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D_W1 = nn.Parameter(torch.Tensor(dim*2, h_dim))\n",
    "        self.D_b1 = nn.Parameter(torch.zeros(h_dim))\n",
    "        self.D_W2 = nn.Parameter(torch.Tensor(h_dim, h_dim))\n",
    "        self.D_b2 = nn.Parameter(torch.zeros(h_dim))\n",
    "        self.D_W3 = nn.Parameter(torch.Tensor(h_dim, dim))\n",
    "        self.D_b3 = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.D_W1.size(0))\n",
    "        self.D_W1.data.uniform_(-stdv, stdv)\n",
    "        self.D_b1.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        stdv_DW2 = 1. / math.sqrt(self.D_W2.size(0))\n",
    "        self.D_W2.data.uniform_(-stdv_DW2, stdv_DW2)\n",
    "        self.D_b2.data.uniform_(-stdv_DW2, stdv_DW2)\n",
    "\n",
    "        stdv_DW3 = 1. / math.sqrt(self.D_W3.size(0))\n",
    "        self.D_W3.data.uniform_(-stdv_DW3, stdv_DW3)\n",
    "        self.D_b3.data.uniform_(-stdv_DW3, stdv_DW3)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Concatenate Data and Hint\n",
    "        inputs = torch.cat((x, h), dim=2)\n",
    "        D_h1 = torch.relu(torch.matmul(inputs, self.D_W1) + self.D_b1)\n",
    "        D_h2 = torch.relu(torch.matmul(D_h1, self.D_W2) + self.D_b2)\n",
    "        D_logit = torch.matmul(D_h2, self.D_W3) + self.D_b3\n",
    "        D_prob = torch.sigmoid(D_logit)\n",
    "        return D_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, q):\n",
    "        self.data = dataset\n",
    "        self.q = q\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] // self.q\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,index * self.q : index * self.q + self.q,:]\n",
    "\n",
    "def make_deltas(masks):\n",
    "    deltas = []\n",
    "    for h in range(len(masks)):\n",
    "        if h == 0:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])])\n",
    "        else:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])] + (1-masks[h]) * deltas[-1])\n",
    "    \n",
    "    return list(deltas)\n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "    unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "    binary_random_matrix = 1. * (unif_random_matrix < p)\n",
    "\n",
    "    return binary_random_matrix\n",
    "\n",
    "def hint_matrix(B, Masks):\n",
    "    Hint = B * Masks + 0.5 * (1-B)\n",
    "\n",
    "    Hint = torch.from_numpy(Hint).to(torch.float32)\n",
    "    return Hint\n",
    "    \n",
    "def missing_data_rbf(df,rbf, batch_size, p):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    binary_random_matrix = binary_sampler(p, shp[0], shp[1])\n",
    "\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "    Hint = hint_matrix(binary_random_matrix, masks)\n",
    "    \n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([rbf_x.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0), Hint.unsqueeze_(0)], dim = 0)\n",
    "\n",
    "    mydata  = MyDataset(dataset, batch_size)\n",
    "    data = DataLoader(mydata, batch_size, shuffle=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = 'air_1000_0.05_time.csv'\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, Generator, Discriminator, lr, epochs, alpha):\n",
    "    imputation_list = []\n",
    "\n",
    "    optimizer_G = optim.Adam(Generator.parameters(), lr=lr)\n",
    "    optimizer_D = optim.Adam(Discriminator.parameters(), lr=lr)\n",
    "\n",
    "    progress = tqdm(range(epochs))\n",
    "    Generator.to(device)\n",
    "    Discriminator.to(device)\n",
    "\n",
    "    for epoch in progress:\n",
    "        batch_loss_G = 0.0\n",
    "        batch_loss_D = 0.0\n",
    "\n",
    "        for data in dataset:\n",
    "\n",
    "            data = data.to(device)\n",
    "            hint = data[:,3,::].clone().detach()\n",
    "            Mask = data[:,2,::].clone().detach()\n",
    "\n",
    "            c_hat_list, imputations, x_loss = Generator(data)\n",
    "            D_prob = Discriminator(imputations, hint)\n",
    "            D_loss = -torch.mean(Mask * torch.log(D_prob + 1e-8) + (1-Mask) * torch.log(1. - D_prob + 1e-8))\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            D_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "            D_prob2 = Discriminator(imputations, hint)\n",
    "            G_loss = -torch.mean((1-Mask) * torch.log(D_prob2 + 1e-8)) + x_loss * alpha\n",
    "            optimizer_G.zero_grad()\n",
    "            G_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "\n",
    "            imputation_list.append(imputations)\n",
    "            batch_loss_G += G_loss\n",
    "            batch_loss_D += D_loss\n",
    "\n",
    "        progress.set_description(\"G_loss: {}, D_loss : {}\".format(batch_loss_G, batch_loss_D))\n",
    "    \n",
    "    return imputation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(36, 64)\n",
    "D = Discriminator(36, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G_loss: 17.324520111083984, D_loss : 0.28490614891052246: 100%|██████████| 300/300 [05:23<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "imputation_list = train(dataset, G, D, 1e-3, 300, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_missing_data_rbf(df,rbf):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    \n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "\n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    values = torch.nan_to_num(torch.from_numpy(values).to(torch.float32))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([rbf_x.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0)], dim = 0).unsqueeze_(0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def eval_model(model, rbf, realpath, dfpath):\n",
    "    \n",
    "    df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)\n",
    "    dataset = val_missing_data_rbf(df, rbf)\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    dataset = dataset.to(device)\n",
    "\n",
    "    real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "    real_scaler = (real - df.mean()) / df.std()\n",
    "\n",
    "    df_scaler = ((df-df.mean()) / df.std()).values\n",
    "    masks = ~np.isnan(df_scaler)\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    \n",
    "    eval_masks = ~np.isnan(real_scaler.values)\n",
    "    eval_masks = torch.from_numpy(eval_masks).to(torch.float32)\n",
    "\n",
    "    test_masks = eval_masks - masks\n",
    "    real_scaler = torch.nan_to_num(torch.from_numpy(real_scaler.values).to(torch.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    c_hat_list, imputations, x_loss = model(dataset)\n",
    "\n",
    "    Nonscale_imputataion = pd.DataFrame(imputations[0].cpu().detach() , columns= df.columns)\n",
    "    Nonscale_imputataion = (Nonscale_imputataion * df.std()) + df.mean()\n",
    "    rbf_df = pd.DataFrame(rbf_df.values, columns= df.columns)\n",
    "    rbf_df = (rbf_df * df.std()) + df.mean()\n",
    "    \n",
    "    real = real.fillna(0)\n",
    "    df = df.fillna(0)\n",
    "    print(\"Scale MAE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(test_masks))\n",
    "    print(\"Scale MRE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(torch.abs(real_scaler * test_masks)))\n",
    "\n",
    "    print(\"Original MAE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"Original MRE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    print(\"RBF MAE :\", np.sum(np.abs((rbf_df - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"RBF MRE :\", np.sum(np.abs((rbf_df - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    print('observation MAE :', np.sum(np.abs((Nonscale_imputataion - df).values * masks.cpu().numpy())) / np.sum(masks.cpu().numpy()))\n",
    "\n",
    "    return Nonscale_imputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "realpath = 'pm25_ground.txt'\n",
    "real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.2640)\n",
      "Scale MRE : tensor(0.3811)\n",
      "Original MAE : 21.31256209026835\n",
      "Original MRE : 0.29937425816287694\n",
      "RBF MAE : 26.244345089085357\n",
      "RBF MRE : 0.36865024996703916\n",
      "observation MAE : 8.382986230262004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.979632</td>\n",
       "      <td>94.558028</td>\n",
       "      <td>109.012972</td>\n",
       "      <td>105.292300</td>\n",
       "      <td>104.623389</td>\n",
       "      <td>98.806747</td>\n",
       "      <td>102.341168</td>\n",
       "      <td>99.229637</td>\n",
       "      <td>100.945096</td>\n",
       "      <td>96.546109</td>\n",
       "      <td>...</td>\n",
       "      <td>76.712119</td>\n",
       "      <td>78.509137</td>\n",
       "      <td>93.531377</td>\n",
       "      <td>75.436973</td>\n",
       "      <td>89.525562</td>\n",
       "      <td>74.103350</td>\n",
       "      <td>88.368280</td>\n",
       "      <td>109.708148</td>\n",
       "      <td>97.749255</td>\n",
       "      <td>120.733266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.773533</td>\n",
       "      <td>97.222771</td>\n",
       "      <td>112.847540</td>\n",
       "      <td>109.052867</td>\n",
       "      <td>106.300672</td>\n",
       "      <td>101.935345</td>\n",
       "      <td>106.501526</td>\n",
       "      <td>101.818824</td>\n",
       "      <td>104.554178</td>\n",
       "      <td>98.539958</td>\n",
       "      <td>...</td>\n",
       "      <td>91.890872</td>\n",
       "      <td>80.232474</td>\n",
       "      <td>96.495995</td>\n",
       "      <td>87.667646</td>\n",
       "      <td>93.647284</td>\n",
       "      <td>76.142415</td>\n",
       "      <td>91.739778</td>\n",
       "      <td>111.762013</td>\n",
       "      <td>104.154877</td>\n",
       "      <td>128.011903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.546584</td>\n",
       "      <td>98.990375</td>\n",
       "      <td>116.139787</td>\n",
       "      <td>113.003751</td>\n",
       "      <td>108.182353</td>\n",
       "      <td>105.169956</td>\n",
       "      <td>110.835437</td>\n",
       "      <td>105.005373</td>\n",
       "      <td>108.455424</td>\n",
       "      <td>101.542161</td>\n",
       "      <td>...</td>\n",
       "      <td>98.136107</td>\n",
       "      <td>81.931966</td>\n",
       "      <td>98.443932</td>\n",
       "      <td>90.834839</td>\n",
       "      <td>97.075322</td>\n",
       "      <td>78.074774</td>\n",
       "      <td>95.394895</td>\n",
       "      <td>114.699009</td>\n",
       "      <td>105.383608</td>\n",
       "      <td>129.231866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.463774</td>\n",
       "      <td>99.790112</td>\n",
       "      <td>119.029576</td>\n",
       "      <td>117.346926</td>\n",
       "      <td>110.447253</td>\n",
       "      <td>108.663489</td>\n",
       "      <td>115.472482</td>\n",
       "      <td>109.059993</td>\n",
       "      <td>112.779486</td>\n",
       "      <td>105.811113</td>\n",
       "      <td>...</td>\n",
       "      <td>99.612019</td>\n",
       "      <td>83.787046</td>\n",
       "      <td>99.414389</td>\n",
       "      <td>92.096992</td>\n",
       "      <td>99.833949</td>\n",
       "      <td>80.079464</td>\n",
       "      <td>99.463593</td>\n",
       "      <td>118.629706</td>\n",
       "      <td>107.473838</td>\n",
       "      <td>133.860040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112.106772</td>\n",
       "      <td>99.871445</td>\n",
       "      <td>121.933568</td>\n",
       "      <td>122.414836</td>\n",
       "      <td>113.389459</td>\n",
       "      <td>112.696303</td>\n",
       "      <td>120.620953</td>\n",
       "      <td>114.270940</td>\n",
       "      <td>117.706829</td>\n",
       "      <td>111.515048</td>\n",
       "      <td>...</td>\n",
       "      <td>104.045623</td>\n",
       "      <td>86.126502</td>\n",
       "      <td>99.783051</td>\n",
       "      <td>93.022497</td>\n",
       "      <td>102.235314</td>\n",
       "      <td>82.504918</td>\n",
       "      <td>104.119098</td>\n",
       "      <td>123.537302</td>\n",
       "      <td>107.458295</td>\n",
       "      <td>134.063385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>85.517552</td>\n",
       "      <td>66.028307</td>\n",
       "      <td>70.094838</td>\n",
       "      <td>65.534252</td>\n",
       "      <td>73.493956</td>\n",
       "      <td>66.055605</td>\n",
       "      <td>64.616278</td>\n",
       "      <td>64.528036</td>\n",
       "      <td>70.011654</td>\n",
       "      <td>69.110241</td>\n",
       "      <td>...</td>\n",
       "      <td>90.454438</td>\n",
       "      <td>99.399387</td>\n",
       "      <td>84.389473</td>\n",
       "      <td>76.380683</td>\n",
       "      <td>86.552459</td>\n",
       "      <td>119.705787</td>\n",
       "      <td>111.664071</td>\n",
       "      <td>96.144254</td>\n",
       "      <td>76.555851</td>\n",
       "      <td>94.116831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>82.107537</td>\n",
       "      <td>66.425545</td>\n",
       "      <td>73.556263</td>\n",
       "      <td>67.037746</td>\n",
       "      <td>74.498008</td>\n",
       "      <td>65.691271</td>\n",
       "      <td>72.138699</td>\n",
       "      <td>64.140210</td>\n",
       "      <td>70.108171</td>\n",
       "      <td>70.659920</td>\n",
       "      <td>...</td>\n",
       "      <td>97.684677</td>\n",
       "      <td>109.433097</td>\n",
       "      <td>77.498082</td>\n",
       "      <td>74.398222</td>\n",
       "      <td>80.346946</td>\n",
       "      <td>129.726792</td>\n",
       "      <td>125.189406</td>\n",
       "      <td>125.314947</td>\n",
       "      <td>75.624343</td>\n",
       "      <td>86.125582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>81.051560</td>\n",
       "      <td>69.216705</td>\n",
       "      <td>79.531669</td>\n",
       "      <td>70.265830</td>\n",
       "      <td>76.753433</td>\n",
       "      <td>67.533268</td>\n",
       "      <td>83.520990</td>\n",
       "      <td>65.965787</td>\n",
       "      <td>72.694449</td>\n",
       "      <td>74.851661</td>\n",
       "      <td>...</td>\n",
       "      <td>109.155502</td>\n",
       "      <td>122.580940</td>\n",
       "      <td>74.110290</td>\n",
       "      <td>75.558825</td>\n",
       "      <td>78.026563</td>\n",
       "      <td>139.256283</td>\n",
       "      <td>140.241324</td>\n",
       "      <td>160.064574</td>\n",
       "      <td>77.533807</td>\n",
       "      <td>82.975097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>87.653015</td>\n",
       "      <td>66.563618</td>\n",
       "      <td>85.776118</td>\n",
       "      <td>71.924839</td>\n",
       "      <td>84.343692</td>\n",
       "      <td>70.316736</td>\n",
       "      <td>95.181907</td>\n",
       "      <td>92.213810</td>\n",
       "      <td>76.202058</td>\n",
       "      <td>93.339675</td>\n",
       "      <td>...</td>\n",
       "      <td>112.299218</td>\n",
       "      <td>123.555588</td>\n",
       "      <td>79.823029</td>\n",
       "      <td>78.028144</td>\n",
       "      <td>65.881984</td>\n",
       "      <td>104.524212</td>\n",
       "      <td>132.610261</td>\n",
       "      <td>131.240331</td>\n",
       "      <td>81.575055</td>\n",
       "      <td>88.246833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>81.722431</td>\n",
       "      <td>75.521197</td>\n",
       "      <td>90.576436</td>\n",
       "      <td>77.149673</td>\n",
       "      <td>82.326852</td>\n",
       "      <td>73.203146</td>\n",
       "      <td>103.996087</td>\n",
       "      <td>72.364971</td>\n",
       "      <td>79.477517</td>\n",
       "      <td>85.122152</td>\n",
       "      <td>...</td>\n",
       "      <td>122.552167</td>\n",
       "      <td>133.071006</td>\n",
       "      <td>73.046655</td>\n",
       "      <td>80.161137</td>\n",
       "      <td>78.151603</td>\n",
       "      <td>145.536892</td>\n",
       "      <td>158.697031</td>\n",
       "      <td>212.763437</td>\n",
       "      <td>86.089627</td>\n",
       "      <td>103.319942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          001001     001002      001003      001004      001005      001006  \\\n",
       "0     112.979632  94.558028  109.012972  105.292300  104.623389   98.806747   \n",
       "1     116.773533  97.222771  112.847540  109.052867  106.300672  101.935345   \n",
       "2     119.546584  98.990375  116.139787  113.003751  108.182353  105.169956   \n",
       "3     121.463774  99.790112  119.029576  117.346926  110.447253  108.663489   \n",
       "4     112.106772  99.871445  121.933568  122.414836  113.389459  112.696303   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "8754   85.517552  66.028307   70.094838   65.534252   73.493956   66.055605   \n",
       "8755   82.107537  66.425545   73.556263   67.037746   74.498008   65.691271   \n",
       "8756   81.051560  69.216705   79.531669   70.265830   76.753433   67.533268   \n",
       "8757   87.653015  66.563618   85.776118   71.924839   84.343692   70.316736   \n",
       "8758   81.722431  75.521197   90.576436   77.149673   82.326852   73.203146   \n",
       "\n",
       "          001007      001008      001009      001010  ...      001027  \\\n",
       "0     102.341168   99.229637  100.945096   96.546109  ...   76.712119   \n",
       "1     106.501526  101.818824  104.554178   98.539958  ...   91.890872   \n",
       "2     110.835437  105.005373  108.455424  101.542161  ...   98.136107   \n",
       "3     115.472482  109.059993  112.779486  105.811113  ...   99.612019   \n",
       "4     120.620953  114.270940  117.706829  111.515048  ...  104.045623   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "8754   64.616278   64.528036   70.011654   69.110241  ...   90.454438   \n",
       "8755   72.138699   64.140210   70.108171   70.659920  ...   97.684677   \n",
       "8756   83.520990   65.965787   72.694449   74.851661  ...  109.155502   \n",
       "8757   95.181907   92.213810   76.202058   93.339675  ...  112.299218   \n",
       "8758  103.996087   72.364971   79.477517   85.122152  ...  122.552167   \n",
       "\n",
       "          001028     001029     001030      001031      001032      001033  \\\n",
       "0      78.509137  93.531377  75.436973   89.525562   74.103350   88.368280   \n",
       "1      80.232474  96.495995  87.667646   93.647284   76.142415   91.739778   \n",
       "2      81.931966  98.443932  90.834839   97.075322   78.074774   95.394895   \n",
       "3      83.787046  99.414389  92.096992   99.833949   80.079464   99.463593   \n",
       "4      86.126502  99.783051  93.022497  102.235314   82.504918  104.119098   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "8754   99.399387  84.389473  76.380683   86.552459  119.705787  111.664071   \n",
       "8755  109.433097  77.498082  74.398222   80.346946  129.726792  125.189406   \n",
       "8756  122.580940  74.110290  75.558825   78.026563  139.256283  140.241324   \n",
       "8757  123.555588  79.823029  78.028144   65.881984  104.524212  132.610261   \n",
       "8758  133.071006  73.046655  80.161137   78.151603  145.536892  158.697031   \n",
       "\n",
       "          001034      001035      001036  \n",
       "0     109.708148   97.749255  120.733266  \n",
       "1     111.762013  104.154877  128.011903  \n",
       "2     114.699009  105.383608  129.231866  \n",
       "3     118.629706  107.473838  133.860040  \n",
       "4     123.537302  107.458295  134.063385  \n",
       "...          ...         ...         ...  \n",
       "8754   96.144254   76.555851   94.116831  \n",
       "8755  125.314947   75.624343   86.125582  \n",
       "8756  160.064574   77.533807   82.975097  \n",
       "8757  131.240331   81.575055   88.246833  \n",
       "8758  212.763437   86.089627  103.319942  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputation\n",
    "eval_model(G, rbfpath, 'pm25_ground.txt', dfpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.2604)\n",
      "Scale MRE : tensor(0.3760)\n",
      "Original MAE : 21.00848766572539\n",
      "Original MRE : 0.2951029718253502\n",
      "RBF MAE : 26.244345089085357\n",
      "RBF MRE : 0.36865024996703916\n",
      "observation MAE : 8.382986230262004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.979632</td>\n",
       "      <td>94.558028</td>\n",
       "      <td>109.012972</td>\n",
       "      <td>105.292300</td>\n",
       "      <td>104.623389</td>\n",
       "      <td>98.806747</td>\n",
       "      <td>102.341168</td>\n",
       "      <td>99.229637</td>\n",
       "      <td>100.945096</td>\n",
       "      <td>96.546109</td>\n",
       "      <td>...</td>\n",
       "      <td>79.074474</td>\n",
       "      <td>78.509137</td>\n",
       "      <td>93.531377</td>\n",
       "      <td>72.966515</td>\n",
       "      <td>89.525562</td>\n",
       "      <td>74.103350</td>\n",
       "      <td>88.368280</td>\n",
       "      <td>109.708148</td>\n",
       "      <td>105.481484</td>\n",
       "      <td>123.406619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.773533</td>\n",
       "      <td>97.222771</td>\n",
       "      <td>112.847540</td>\n",
       "      <td>109.052867</td>\n",
       "      <td>106.300672</td>\n",
       "      <td>101.935345</td>\n",
       "      <td>106.501526</td>\n",
       "      <td>101.818824</td>\n",
       "      <td>104.554178</td>\n",
       "      <td>98.539958</td>\n",
       "      <td>...</td>\n",
       "      <td>91.561790</td>\n",
       "      <td>80.232474</td>\n",
       "      <td>96.495995</td>\n",
       "      <td>92.394345</td>\n",
       "      <td>93.647284</td>\n",
       "      <td>76.142415</td>\n",
       "      <td>91.739778</td>\n",
       "      <td>111.762013</td>\n",
       "      <td>105.008387</td>\n",
       "      <td>134.513446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.546584</td>\n",
       "      <td>98.990375</td>\n",
       "      <td>116.139787</td>\n",
       "      <td>113.003751</td>\n",
       "      <td>108.182353</td>\n",
       "      <td>105.169956</td>\n",
       "      <td>110.835437</td>\n",
       "      <td>105.005373</td>\n",
       "      <td>108.455424</td>\n",
       "      <td>101.542161</td>\n",
       "      <td>...</td>\n",
       "      <td>96.995659</td>\n",
       "      <td>81.931966</td>\n",
       "      <td>98.443932</td>\n",
       "      <td>89.657708</td>\n",
       "      <td>97.075322</td>\n",
       "      <td>78.074774</td>\n",
       "      <td>95.394895</td>\n",
       "      <td>114.699009</td>\n",
       "      <td>110.871501</td>\n",
       "      <td>138.300555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.463774</td>\n",
       "      <td>99.790112</td>\n",
       "      <td>119.029576</td>\n",
       "      <td>117.346926</td>\n",
       "      <td>110.447253</td>\n",
       "      <td>108.663489</td>\n",
       "      <td>115.472482</td>\n",
       "      <td>109.059993</td>\n",
       "      <td>112.779486</td>\n",
       "      <td>105.811113</td>\n",
       "      <td>...</td>\n",
       "      <td>99.452154</td>\n",
       "      <td>83.787046</td>\n",
       "      <td>99.414389</td>\n",
       "      <td>88.090290</td>\n",
       "      <td>99.833949</td>\n",
       "      <td>80.079464</td>\n",
       "      <td>99.463593</td>\n",
       "      <td>118.629706</td>\n",
       "      <td>116.924954</td>\n",
       "      <td>143.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113.827661</td>\n",
       "      <td>99.871445</td>\n",
       "      <td>121.933568</td>\n",
       "      <td>122.414836</td>\n",
       "      <td>113.389459</td>\n",
       "      <td>112.696303</td>\n",
       "      <td>120.620953</td>\n",
       "      <td>114.270940</td>\n",
       "      <td>117.706829</td>\n",
       "      <td>111.515048</td>\n",
       "      <td>...</td>\n",
       "      <td>104.048894</td>\n",
       "      <td>86.126502</td>\n",
       "      <td>99.783051</td>\n",
       "      <td>89.740898</td>\n",
       "      <td>102.235314</td>\n",
       "      <td>82.504918</td>\n",
       "      <td>104.119098</td>\n",
       "      <td>123.537302</td>\n",
       "      <td>123.434968</td>\n",
       "      <td>150.469390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>85.517552</td>\n",
       "      <td>66.028307</td>\n",
       "      <td>70.094838</td>\n",
       "      <td>65.534252</td>\n",
       "      <td>73.493956</td>\n",
       "      <td>66.055605</td>\n",
       "      <td>64.616278</td>\n",
       "      <td>64.528036</td>\n",
       "      <td>70.011654</td>\n",
       "      <td>69.110241</td>\n",
       "      <td>...</td>\n",
       "      <td>102.305976</td>\n",
       "      <td>102.483719</td>\n",
       "      <td>84.389473</td>\n",
       "      <td>76.380683</td>\n",
       "      <td>86.552459</td>\n",
       "      <td>119.705787</td>\n",
       "      <td>111.664071</td>\n",
       "      <td>96.144254</td>\n",
       "      <td>76.555851</td>\n",
       "      <td>104.854518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>82.107537</td>\n",
       "      <td>66.425545</td>\n",
       "      <td>73.556263</td>\n",
       "      <td>67.037746</td>\n",
       "      <td>74.498008</td>\n",
       "      <td>65.691271</td>\n",
       "      <td>72.138699</td>\n",
       "      <td>64.140210</td>\n",
       "      <td>70.108171</td>\n",
       "      <td>70.659920</td>\n",
       "      <td>...</td>\n",
       "      <td>110.105233</td>\n",
       "      <td>113.897149</td>\n",
       "      <td>77.498082</td>\n",
       "      <td>74.398222</td>\n",
       "      <td>80.346946</td>\n",
       "      <td>129.726792</td>\n",
       "      <td>125.189406</td>\n",
       "      <td>125.314947</td>\n",
       "      <td>75.624343</td>\n",
       "      <td>99.625134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>81.051560</td>\n",
       "      <td>69.216705</td>\n",
       "      <td>79.531669</td>\n",
       "      <td>70.265830</td>\n",
       "      <td>76.753433</td>\n",
       "      <td>67.533268</td>\n",
       "      <td>83.520990</td>\n",
       "      <td>65.965787</td>\n",
       "      <td>72.694449</td>\n",
       "      <td>74.851661</td>\n",
       "      <td>...</td>\n",
       "      <td>122.394518</td>\n",
       "      <td>129.151693</td>\n",
       "      <td>74.110290</td>\n",
       "      <td>75.558825</td>\n",
       "      <td>78.026563</td>\n",
       "      <td>139.256283</td>\n",
       "      <td>140.241324</td>\n",
       "      <td>160.064574</td>\n",
       "      <td>77.533807</td>\n",
       "      <td>96.273548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>98.486822</td>\n",
       "      <td>78.911588</td>\n",
       "      <td>85.776118</td>\n",
       "      <td>90.013126</td>\n",
       "      <td>91.644665</td>\n",
       "      <td>70.316736</td>\n",
       "      <td>95.181907</td>\n",
       "      <td>93.776060</td>\n",
       "      <td>76.202058</td>\n",
       "      <td>91.275036</td>\n",
       "      <td>...</td>\n",
       "      <td>122.006243</td>\n",
       "      <td>129.238375</td>\n",
       "      <td>81.155703</td>\n",
       "      <td>78.028144</td>\n",
       "      <td>85.374034</td>\n",
       "      <td>113.069793</td>\n",
       "      <td>113.175050</td>\n",
       "      <td>126.007220</td>\n",
       "      <td>105.082755</td>\n",
       "      <td>103.485884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>81.722431</td>\n",
       "      <td>75.521197</td>\n",
       "      <td>90.576436</td>\n",
       "      <td>77.149673</td>\n",
       "      <td>82.326852</td>\n",
       "      <td>73.203146</td>\n",
       "      <td>103.996087</td>\n",
       "      <td>72.364971</td>\n",
       "      <td>79.477517</td>\n",
       "      <td>85.122152</td>\n",
       "      <td>...</td>\n",
       "      <td>128.928185</td>\n",
       "      <td>138.879403</td>\n",
       "      <td>73.046655</td>\n",
       "      <td>80.161137</td>\n",
       "      <td>78.151603</td>\n",
       "      <td>145.536892</td>\n",
       "      <td>158.697031</td>\n",
       "      <td>212.763437</td>\n",
       "      <td>86.089627</td>\n",
       "      <td>96.628396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          001001     001002      001003      001004      001005      001006  \\\n",
       "0     112.979632  94.558028  109.012972  105.292300  104.623389   98.806747   \n",
       "1     116.773533  97.222771  112.847540  109.052867  106.300672  101.935345   \n",
       "2     119.546584  98.990375  116.139787  113.003751  108.182353  105.169956   \n",
       "3     121.463774  99.790112  119.029576  117.346926  110.447253  108.663489   \n",
       "4     113.827661  99.871445  121.933568  122.414836  113.389459  112.696303   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "8754   85.517552  66.028307   70.094838   65.534252   73.493956   66.055605   \n",
       "8755   82.107537  66.425545   73.556263   67.037746   74.498008   65.691271   \n",
       "8756   81.051560  69.216705   79.531669   70.265830   76.753433   67.533268   \n",
       "8757   98.486822  78.911588   85.776118   90.013126   91.644665   70.316736   \n",
       "8758   81.722431  75.521197   90.576436   77.149673   82.326852   73.203146   \n",
       "\n",
       "          001007      001008      001009      001010  ...      001027  \\\n",
       "0     102.341168   99.229637  100.945096   96.546109  ...   79.074474   \n",
       "1     106.501526  101.818824  104.554178   98.539958  ...   91.561790   \n",
       "2     110.835437  105.005373  108.455424  101.542161  ...   96.995659   \n",
       "3     115.472482  109.059993  112.779486  105.811113  ...   99.452154   \n",
       "4     120.620953  114.270940  117.706829  111.515048  ...  104.048894   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "8754   64.616278   64.528036   70.011654   69.110241  ...  102.305976   \n",
       "8755   72.138699   64.140210   70.108171   70.659920  ...  110.105233   \n",
       "8756   83.520990   65.965787   72.694449   74.851661  ...  122.394518   \n",
       "8757   95.181907   93.776060   76.202058   91.275036  ...  122.006243   \n",
       "8758  103.996087   72.364971   79.477517   85.122152  ...  128.928185   \n",
       "\n",
       "          001028     001029     001030      001031      001032      001033  \\\n",
       "0      78.509137  93.531377  72.966515   89.525562   74.103350   88.368280   \n",
       "1      80.232474  96.495995  92.394345   93.647284   76.142415   91.739778   \n",
       "2      81.931966  98.443932  89.657708   97.075322   78.074774   95.394895   \n",
       "3      83.787046  99.414389  88.090290   99.833949   80.079464   99.463593   \n",
       "4      86.126502  99.783051  89.740898  102.235314   82.504918  104.119098   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "8754  102.483719  84.389473  76.380683   86.552459  119.705787  111.664071   \n",
       "8755  113.897149  77.498082  74.398222   80.346946  129.726792  125.189406   \n",
       "8756  129.151693  74.110290  75.558825   78.026563  139.256283  140.241324   \n",
       "8757  129.238375  81.155703  78.028144   85.374034  113.069793  113.175050   \n",
       "8758  138.879403  73.046655  80.161137   78.151603  145.536892  158.697031   \n",
       "\n",
       "          001034      001035      001036  \n",
       "0     109.708148  105.481484  123.406619  \n",
       "1     111.762013  105.008387  134.513446  \n",
       "2     114.699009  110.871501  138.300555  \n",
       "3     118.629706  116.924954  143.958600  \n",
       "4     123.537302  123.434968  150.469390  \n",
       "...          ...         ...         ...  \n",
       "8754   96.144254   76.555851  104.854518  \n",
       "8755  125.314947   75.624343   99.625134  \n",
       "8756  160.064574   77.533807   96.273548  \n",
       "8757  126.007220  105.082755  103.485884  \n",
       "8758  212.763437   86.089627   96.628396  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C_hat\n",
    "eval_model(G, rbfpath, 'pm25_ground.txt', dfpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
