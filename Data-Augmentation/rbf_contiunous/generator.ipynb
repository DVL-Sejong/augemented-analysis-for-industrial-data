{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class FeatureRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureRegression, self).__init__()\n",
    "        self.build(input_size)\n",
    "\n",
    "    def build(self, input_size):\n",
    "        self.W = Parameter(torch.Tensor(input_size, input_size))\n",
    "        self.b = Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        m = torch.ones(input_size, input_size).cuda() - torch.eye(input_size, input_size).cuda()\n",
    "        self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_h = F.linear(x, self.W * Variable(self.m), self.b)\n",
    "        return z_h\n",
    "\n",
    "class TemporalDecay(nn.Module):\n",
    "    def __init__(self, input_size, output_size, diag = False):\n",
    "        super(TemporalDecay, self).__init__()\n",
    "        self.diag = diag\n",
    "\n",
    "        self.build(input_size, output_size)\n",
    "\n",
    "    def build(self, input_size, output_size):\n",
    "        self.W = Parameter(torch.Tensor(output_size, input_size)).cuda()\n",
    "        self.b = Parameter(torch.Tensor(output_size)).cuda()\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        if self.diag == True:\n",
    "            assert(input_size == output_size)\n",
    "            m = torch.eye(input_size, input_size).cuda()\n",
    "            self.register_buffer('m', m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, d):\n",
    "        gamma = self.relu(F.linear(d, self.W, self.b))\n",
    "        gamma = torch.exp(-gamma)\n",
    "        return gamma\n",
    "\n",
    "# Generator 모델\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.temp_decay_h = TemporalDecay(input_size, output_size = hidden_size, diag = False)\n",
    "        self.temp_decay_r = TemporalDecay(input_size, input_size, diag = True)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.input_size, bias=True)\n",
    "        \n",
    "        self.z_layer = FeatureRegression(self.input_size)\n",
    "        self.beta_layer = nn.Linear(self.input_size * 2, self.input_size)\n",
    "        self.grucell = nn.GRUCell(self.input_size * 2, self.hidden_size)\n",
    "        self.hidden_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def loss(self, hat, y, m):\n",
    "        return torch.sum(torch.abs((y - hat)) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        rbfs = input[:,0,::]\n",
    "        delta = input[:,1,::]\n",
    "        masks = input[:,2,::]\n",
    "\n",
    "        hid = torch.zeros((rbfs.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        x_loss = 0.0\n",
    "        imputations = []\n",
    "        c_hat_list = []\n",
    "        for i in range(rbfs.size(1)):\n",
    "\n",
    "            r = rbfs[:,i,:]\n",
    "            d = delta[:,i,:]\n",
    "            m = masks[:,i,:]\n",
    "\n",
    "            gamma_r = self.temp_decay_r(d)\n",
    "            gamma_h = self.temp_decay_h(d)\n",
    "\n",
    "            hid = hid * gamma_h\n",
    "            \n",
    "            x_hat = self.output_layer(hid)\n",
    "            x_loss += torch.sum(torch.abs(r - x_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            r_c = m * r + (1 - m) * x_hat\n",
    "\n",
    "            r_hat = self.z_layer(r_c)\n",
    "\n",
    "            x_loss += torch.sum(torch.abs(r - r_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            beta_weight = torch.cat([gamma_r, m], dim = 1)\n",
    "            beta = torch.sigmoid(self.beta_layer(beta_weight))\n",
    "\n",
    "            c_hat = beta * r_hat + (1 - beta) * x_hat\n",
    "            x_loss += torch.sum(torch.abs(r - c_hat) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "            c_c = m * r + (1 - m) * c_hat\n",
    "\n",
    "            gru_input = torch.cat([c_c, m], dim = 1)\n",
    "            c_hat_list.append(c_hat.unsqueeze(1))\n",
    "            imputations.append(c_c.unsqueeze(1))\n",
    "            \n",
    "            # GRU cell\n",
    "            self.hidden_dropout(hid)\n",
    "            hid = self.grucell(gru_input, hid)\n",
    "            \n",
    "        imputations = torch.cat(imputations, dim = 1)\n",
    "        c_hat_list = torch.cat(c_hat_list, dim = 1)\n",
    "\n",
    "        return  c_hat_list, imputations, x_loss / rbfs.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, q):\n",
    "        self.data = dataset\n",
    "        self.q = q\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] // self.q\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,index * self.q : index * self.q + self.q,:]\n",
    "\n",
    "def make_deltas(masks):\n",
    "    deltas = []\n",
    "    for h in range(len(masks)):\n",
    "        if h == 0:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])])\n",
    "        else:\n",
    "            deltas.append([1 for _ in range(masks.shape[1])] + (1-masks[h]) * deltas[-1])\n",
    "    \n",
    "    return list(deltas)\n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "    unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "    binary_random_matrix = 1. * (unif_random_matrix < p)\n",
    "\n",
    "    return binary_random_matrix\n",
    "\n",
    "def hint_matrix(B, Masks):\n",
    "    Hint = B * Masks + 0.5 * (1-B)\n",
    "\n",
    "    Hint = torch.from_numpy(Hint).to(torch.float32)\n",
    "    return Hint\n",
    "    \n",
    "def missing_data_rbf(df,rbf, batch_size, p):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    binary_random_matrix = binary_sampler(p, shp[0], shp[1])\n",
    "\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "    Hint = hint_matrix(binary_random_matrix, masks)\n",
    "    \n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([rbf_x.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0), Hint.unsqueeze_(0)], dim = 0)\n",
    "\n",
    "    mydata  = MyDataset(dataset, batch_size)\n",
    "    data = DataLoader(mydata, batch_size, shuffle=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_G(dataset, Generator, lr, epochs):\n",
    "    imputation_list = []\n",
    "\n",
    "    optimizer_G = optim.Adam(Generator.parameters(), lr=lr)\n",
    "\n",
    "    progress = tqdm(range(epochs))\n",
    "    Generator.to(device)\n",
    "\n",
    "    for epoch in progress:\n",
    "        batch_loss_G = 0.0\n",
    "        batch_loss_D = 0.0\n",
    "\n",
    "        for data in dataset:\n",
    "\n",
    "            data = data.to(device)\n",
    "            hint = data[:,3,::].clone().detach()\n",
    "            Mask = data[:,2,::].clone().detach()\n",
    "\n",
    "            c_hat_list, imputations, x_loss = Generator(data)\n",
    "            optimizer_G.zero_grad()\n",
    "            x_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "\n",
    "            imputation_list.append(imputations)\n",
    "            batch_loss_G += x_loss\n",
    "\n",
    "        progress.set_description(\"G_loss: {}\".format(batch_loss_G / len(dataset)))\n",
    "    \n",
    "    return imputation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = 'pm25_missing.txt'\n",
    "df = pd.read_csv(\"./dataset/\"+dfpath).drop([\"datetime\"], axis = 1)\n",
    "rbfpath = \"air\"\n",
    "batch_size = 64\n",
    "dataset = missing_data_rbf(df, rbfpath, batch_size, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_df = pd.read_csv(\"./RBFresult/air_20_8.0_drop_0.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.729387</td>\n",
       "      <td>0.202664</td>\n",
       "      <td>0.305591</td>\n",
       "      <td>0.150003</td>\n",
       "      <td>0.135624</td>\n",
       "      <td>0.057313</td>\n",
       "      <td>0.094235</td>\n",
       "      <td>0.024987</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>-0.075675</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231177</td>\n",
       "      <td>0.756275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580793</td>\n",
       "      <td>0.415816</td>\n",
       "      <td>0.021781</td>\n",
       "      <td>-0.129339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546073</td>\n",
       "      <td>0.147144</td>\n",
       "      <td>0.525986</td>\n",
       "      <td>0.264572</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.216440</td>\n",
       "      <td>0.333405</td>\n",
       "      <td>0.160917</td>\n",
       "      <td>0.248561</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129904</td>\n",
       "      <td>0.633067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580793</td>\n",
       "      <td>0.367683</td>\n",
       "      <td>0.177156</td>\n",
       "      <td>-0.059496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.585355</td>\n",
       "      <td>0.188784</td>\n",
       "      <td>0.649958</td>\n",
       "      <td>0.366411</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.349046</td>\n",
       "      <td>0.460024</td>\n",
       "      <td>0.235061</td>\n",
       "      <td>0.366278</td>\n",
       "      <td>0.183491</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317983</td>\n",
       "      <td>0.571463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679165</td>\n",
       "      <td>0.351639</td>\n",
       "      <td>0.425754</td>\n",
       "      <td>0.050257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.611542</td>\n",
       "      <td>0.355345</td>\n",
       "      <td>0.746381</td>\n",
       "      <td>0.468250</td>\n",
       "      <td>0.124378</td>\n",
       "      <td>0.468391</td>\n",
       "      <td>0.516299</td>\n",
       "      <td>0.247418</td>\n",
       "      <td>0.431677</td>\n",
       "      <td>0.207052</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.375853</td>\n",
       "      <td>0.571463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744746</td>\n",
       "      <td>0.431860</td>\n",
       "      <td>0.472367</td>\n",
       "      <td>0.130077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480267</td>\n",
       "      <td>0.842804</td>\n",
       "      <td>0.544629</td>\n",
       "      <td>0.203102</td>\n",
       "      <td>0.547955</td>\n",
       "      <td>0.685125</td>\n",
       "      <td>0.333919</td>\n",
       "      <td>0.523234</td>\n",
       "      <td>0.242393</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173307</td>\n",
       "      <td>0.556062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.810327</td>\n",
       "      <td>0.383727</td>\n",
       "      <td>0.550054</td>\n",
       "      <td>0.179965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>0.048507</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>-0.176523</td>\n",
       "      <td>-0.180974</td>\n",
       "      <td>-0.269242</td>\n",
       "      <td>-0.260941</td>\n",
       "      <td>-0.229347</td>\n",
       "      <td>-0.246873</td>\n",
       "      <td>-0.130750</td>\n",
       "      <td>-0.323061</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.171037</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.203702</td>\n",
       "      <td>1.025498</td>\n",
       "      <td>0.332530</td>\n",
       "      <td>-0.398732</td>\n",
       "      <td>-0.429225</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>0.061601</td>\n",
       "      <td>-0.019418</td>\n",
       "      <td>-0.135199</td>\n",
       "      <td>-0.130055</td>\n",
       "      <td>-0.156780</td>\n",
       "      <td>-0.260941</td>\n",
       "      <td>-0.285622</td>\n",
       "      <td>-0.333374</td>\n",
       "      <td>-0.287707</td>\n",
       "      <td>-0.299501</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.059977</td>\n",
       "      <td>-0.018071</td>\n",
       "      <td>0.220097</td>\n",
       "      <td>1.009454</td>\n",
       "      <td>0.860802</td>\n",
       "      <td>-0.039541</td>\n",
       "      <td>-0.377575</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>-0.030056</td>\n",
       "      <td>-0.005538</td>\n",
       "      <td>-0.038776</td>\n",
       "      <td>-0.206434</td>\n",
       "      <td>-0.145533</td>\n",
       "      <td>-0.181378</td>\n",
       "      <td>-0.130865</td>\n",
       "      <td>-0.172729</td>\n",
       "      <td>-0.091511</td>\n",
       "      <td>-0.240599</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>0.334864</td>\n",
       "      <td>1.025498</td>\n",
       "      <td>1.031714</td>\n",
       "      <td>0.589044</td>\n",
       "      <td>-0.284604</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.088554</td>\n",
       "      <td>-0.032384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>-0.043150</td>\n",
       "      <td>-0.019418</td>\n",
       "      <td>0.140295</td>\n",
       "      <td>-0.117325</td>\n",
       "      <td>-0.190518</td>\n",
       "      <td>-0.128335</td>\n",
       "      <td>0.699193</td>\n",
       "      <td>-0.185086</td>\n",
       "      <td>-0.026113</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.171037</td>\n",
       "      <td>0.258676</td>\n",
       "      <td>0.302074</td>\n",
       "      <td>1.378472</td>\n",
       "      <td>1.420150</td>\n",
       "      <td>1.367293</td>\n",
       "      <td>-0.119322</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        001001    001002    001003    001004    001005    001006    001007  \\\n",
       "0     0.729387  0.202664  0.305591  0.150003  0.135624  0.057313  0.094235   \n",
       "1     0.546073  0.147144  0.525986  0.264572  0.045654  0.216440  0.333405   \n",
       "2     0.585355  0.188784  0.649958  0.366411  0.056900  0.349046  0.460024   \n",
       "3     0.611542  0.355345  0.746381  0.468250  0.124378  0.468391  0.516299   \n",
       "4          NaN  0.480267  0.842804  0.544629  0.203102  0.547955  0.685125   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8754  0.048507 -0.033298 -0.176523 -0.180974 -0.269242 -0.260941 -0.229347   \n",
       "8755  0.061601 -0.019418 -0.135199 -0.130055 -0.156780 -0.260941 -0.285622   \n",
       "8756 -0.030056 -0.005538 -0.038776 -0.206434 -0.145533 -0.181378 -0.130865   \n",
       "8757       NaN       NaN  0.057646       NaN       NaN -0.088554 -0.032384   \n",
       "8758 -0.043150 -0.019418  0.140295 -0.117325 -0.190518 -0.128335  0.699193   \n",
       "\n",
       "        001008    001009    001010  ...  001027    001028    001029    001030  \\\n",
       "0     0.024987  0.039286 -0.075675  ...     NaN  0.231177  0.756275       NaN   \n",
       "1     0.160917  0.248561  0.006787  ...     NaN  0.129904  0.633067       NaN   \n",
       "2     0.235061  0.366278  0.183491  ...     NaN  0.317983  0.571463       NaN   \n",
       "3     0.247418  0.431677  0.207052  ...     NaN  0.375853  0.571463       NaN   \n",
       "4     0.333919  0.523234  0.242393  ...     NaN  0.173307  0.556062       NaN   \n",
       "...        ...       ...       ...  ...     ...       ...       ...       ...   \n",
       "8754 -0.246873 -0.130750 -0.323061  ...     NaN       NaN  0.171037  0.009604   \n",
       "8755 -0.333374 -0.287707 -0.299501  ...     NaN       NaN -0.059977 -0.018071   \n",
       "8756 -0.172729 -0.091511 -0.240599  ...     NaN       NaN  0.063231  0.023441   \n",
       "8757       NaN -0.026113       NaN  ...     NaN       NaN       NaN  0.078791   \n",
       "8758 -0.185086 -0.026113  0.006787  ...     NaN       NaN  0.171037  0.258676   \n",
       "\n",
       "        001031    001032    001033    001034    001035  001036  \n",
       "0     0.580793  0.415816  0.021781 -0.129339       NaN     NaN  \n",
       "1     0.580793  0.367683  0.177156 -0.059496       NaN     NaN  \n",
       "2     0.679165  0.351639  0.425754  0.050257       NaN     NaN  \n",
       "3     0.744746  0.431860  0.472367  0.130077       NaN     NaN  \n",
       "4     0.810327  0.383727  0.550054  0.179965       NaN     NaN  \n",
       "...        ...       ...       ...       ...       ...     ...  \n",
       "8754  0.203702  1.025498  0.332530 -0.398732 -0.429225     NaN  \n",
       "8755  0.220097  1.009454  0.860802 -0.039541 -0.377575     NaN  \n",
       "8756  0.334864  1.025498  1.031714  0.589044 -0.284604     NaN  \n",
       "8757       NaN       NaN       NaN       NaN       NaN     NaN  \n",
       "8758  0.302074  1.378472  1.420150  1.367293 -0.119322     NaN  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.833702</td>\n",
       "      <td>0.074850</td>\n",
       "      <td>0.347782</td>\n",
       "      <td>0.198880</td>\n",
       "      <td>0.178503</td>\n",
       "      <td>0.126357</td>\n",
       "      <td>0.182780</td>\n",
       "      <td>0.115592</td>\n",
       "      <td>0.094246</td>\n",
       "      <td>-0.043076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053620</td>\n",
       "      <td>0.222963</td>\n",
       "      <td>0.748346</td>\n",
       "      <td>-0.311618</td>\n",
       "      <td>0.216050</td>\n",
       "      <td>0.458980</td>\n",
       "      <td>0.099035</td>\n",
       "      <td>-0.035186</td>\n",
       "      <td>-0.343594</td>\n",
       "      <td>-0.420327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.884523</td>\n",
       "      <td>0.229706</td>\n",
       "      <td>0.530275</td>\n",
       "      <td>0.321159</td>\n",
       "      <td>0.199503</td>\n",
       "      <td>0.277548</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>0.300777</td>\n",
       "      <td>0.255071</td>\n",
       "      <td>0.051002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137924</td>\n",
       "      <td>0.259383</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>-0.233680</td>\n",
       "      <td>0.393282</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>0.245915</td>\n",
       "      <td>0.040810</td>\n",
       "      <td>-0.204572</td>\n",
       "      <td>-0.305951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.854783</td>\n",
       "      <td>0.307665</td>\n",
       "      <td>0.723294</td>\n",
       "      <td>0.406509</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.383615</td>\n",
       "      <td>0.421295</td>\n",
       "      <td>0.514432</td>\n",
       "      <td>0.393861</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379266</td>\n",
       "      <td>0.376184</td>\n",
       "      <td>0.641818</td>\n",
       "      <td>-0.129991</td>\n",
       "      <td>0.522623</td>\n",
       "      <td>0.475514</td>\n",
       "      <td>0.332519</td>\n",
       "      <td>0.239367</td>\n",
       "      <td>-0.029407</td>\n",
       "      <td>-0.161838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797969</td>\n",
       "      <td>0.382020</td>\n",
       "      <td>0.837884</td>\n",
       "      <td>0.464174</td>\n",
       "      <td>0.124071</td>\n",
       "      <td>0.477686</td>\n",
       "      <td>0.530302</td>\n",
       "      <td>0.717413</td>\n",
       "      <td>0.529407</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672896</td>\n",
       "      <td>0.510132</td>\n",
       "      <td>0.564856</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.674533</td>\n",
       "      <td>0.428275</td>\n",
       "      <td>0.443649</td>\n",
       "      <td>0.455692</td>\n",
       "      <td>0.183708</td>\n",
       "      <td>0.013497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680214</td>\n",
       "      <td>0.576192</td>\n",
       "      <td>0.800103</td>\n",
       "      <td>0.426261</td>\n",
       "      <td>0.231408</td>\n",
       "      <td>0.586270</td>\n",
       "      <td>0.669133</td>\n",
       "      <td>0.814170</td>\n",
       "      <td>0.649634</td>\n",
       "      <td>0.325653</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017126</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>0.569529</td>\n",
       "      <td>0.169826</td>\n",
       "      <td>0.824753</td>\n",
       "      <td>0.329915</td>\n",
       "      <td>0.607453</td>\n",
       "      <td>0.527583</td>\n",
       "      <td>0.433548</td>\n",
       "      <td>0.219047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>0.190555</td>\n",
       "      <td>-0.068424</td>\n",
       "      <td>-0.175941</td>\n",
       "      <td>-0.172720</td>\n",
       "      <td>-0.262101</td>\n",
       "      <td>-0.253210</td>\n",
       "      <td>-0.212252</td>\n",
       "      <td>-0.198784</td>\n",
       "      <td>-0.133938</td>\n",
       "      <td>-0.321118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178321</td>\n",
       "      <td>-0.296517</td>\n",
       "      <td>0.159499</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.162669</td>\n",
       "      <td>1.012385</td>\n",
       "      <td>0.570752</td>\n",
       "      <td>-0.349070</td>\n",
       "      <td>-0.310987</td>\n",
       "      <td>-0.580916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>0.145157</td>\n",
       "      <td>-0.140374</td>\n",
       "      <td>-0.166147</td>\n",
       "      <td>-0.169738</td>\n",
       "      <td>-0.211978</td>\n",
       "      <td>-0.234623</td>\n",
       "      <td>-0.205259</td>\n",
       "      <td>-0.168571</td>\n",
       "      <td>-0.112179</td>\n",
       "      <td>-0.292837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112672</td>\n",
       "      <td>-0.296055</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.192919</td>\n",
       "      <td>1.060401</td>\n",
       "      <td>0.780759</td>\n",
       "      <td>-0.016766</td>\n",
       "      <td>-0.257222</td>\n",
       "      <td>-0.580590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>0.093035</td>\n",
       "      <td>-0.186031</td>\n",
       "      <td>-0.111841</td>\n",
       "      <td>-0.133577</td>\n",
       "      <td>-0.143286</td>\n",
       "      <td>-0.174276</td>\n",
       "      <td>-0.049471</td>\n",
       "      <td>-0.117937</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>-0.230917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055072</td>\n",
       "      <td>-0.295588</td>\n",
       "      <td>0.096265</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.268784</td>\n",
       "      <td>1.174698</td>\n",
       "      <td>1.114924</td>\n",
       "      <td>0.572203</td>\n",
       "      <td>-0.168022</td>\n",
       "      <td>-0.580264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>0.043069</td>\n",
       "      <td>-0.138666</td>\n",
       "      <td>-0.043633</td>\n",
       "      <td>-0.115748</td>\n",
       "      <td>-0.132931</td>\n",
       "      <td>-0.144740</td>\n",
       "      <td>0.291237</td>\n",
       "      <td>-0.108921</td>\n",
       "      <td>-0.020099</td>\n",
       "      <td>-0.163647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>-0.295117</td>\n",
       "      <td>0.094204</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>0.279773</td>\n",
       "      <td>1.406187</td>\n",
       "      <td>1.501001</td>\n",
       "      <td>1.218251</td>\n",
       "      <td>-0.055078</td>\n",
       "      <td>-0.579938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.051307</td>\n",
       "      <td>0.081811</td>\n",
       "      <td>-0.109469</td>\n",
       "      <td>-0.170510</td>\n",
       "      <td>-0.125312</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>-0.131353</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>-0.052164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042971</td>\n",
       "      <td>-0.294642</td>\n",
       "      <td>0.122101</td>\n",
       "      <td>0.235977</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>1.471956</td>\n",
       "      <td>1.590799</td>\n",
       "      <td>1.611849</td>\n",
       "      <td>-0.009876</td>\n",
       "      <td>-0.579613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        001001    001002    001003    001004    001005    001006    001007  \\\n",
       "0     0.833702  0.074850  0.347782  0.198880  0.178503  0.126357  0.182780   \n",
       "1     0.884523  0.229706  0.530275  0.321159  0.199503  0.277548  0.320911   \n",
       "2     0.854783  0.307665  0.723294  0.406509  0.139408  0.383615  0.421295   \n",
       "3     0.797969  0.382020  0.837884  0.464174  0.124071  0.477686  0.530302   \n",
       "4     0.680214  0.576192  0.800103  0.426261  0.231408  0.586270  0.669133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8754  0.190555 -0.068424 -0.175941 -0.172720 -0.262101 -0.253210 -0.212252   \n",
       "8755  0.145157 -0.140374 -0.166147 -0.169738 -0.211978 -0.234623 -0.205259   \n",
       "8756  0.093035 -0.186031 -0.111841 -0.133577 -0.143286 -0.174276 -0.049471   \n",
       "8757  0.043069 -0.138666 -0.043633 -0.115748 -0.132931 -0.144740  0.291237   \n",
       "8758  0.002258 -0.051307  0.081811 -0.109469 -0.170510 -0.125312  0.701550   \n",
       "\n",
       "        001008    001009    001010  ...    001027    001028    001029  \\\n",
       "0     0.115592  0.094246 -0.043076  ... -0.053620  0.222963  0.748346   \n",
       "1     0.300777  0.255071  0.051002  ...  0.137924  0.259383  0.751663   \n",
       "2     0.514432  0.393861  0.173122  ...  0.379266  0.376184  0.641818   \n",
       "3     0.717413  0.529407  0.288889  ...  0.672896  0.510132  0.564856   \n",
       "4     0.814170  0.649634  0.325653  ...  1.017126  0.516252  0.569529   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8754 -0.198784 -0.133938 -0.321118  ...  0.178321 -0.296517  0.159499   \n",
       "8755 -0.168571 -0.112179 -0.292837  ...  0.112672 -0.296055  0.109013   \n",
       "8756 -0.117937 -0.060297 -0.230917  ...  0.055072 -0.295588  0.096265   \n",
       "8757 -0.108921 -0.020099 -0.163647  ...  0.003419 -0.295117  0.094204   \n",
       "8758 -0.131353 -0.012837 -0.052164  ... -0.042971 -0.294642  0.122101   \n",
       "\n",
       "        001030    001031    001032    001033    001034    001035    001036  \n",
       "0    -0.311618  0.216050  0.458980  0.099035 -0.035186 -0.343594 -0.420327  \n",
       "1    -0.233680  0.393282  0.513269  0.245915  0.040810 -0.204572 -0.305951  \n",
       "2    -0.129991  0.522623  0.475514  0.332519  0.239367 -0.029407 -0.161838  \n",
       "3     0.003552  0.674533  0.428275  0.443649  0.455692  0.183708  0.013497  \n",
       "4     0.169826  0.824753  0.329915  0.607453  0.527583  0.433548  0.219047  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8754 -0.006098  0.162669  1.012385  0.570752 -0.349070 -0.310987 -0.580916  \n",
       "8755 -0.013756  0.192919  1.060401  0.780759 -0.016766 -0.257222 -0.580590  \n",
       "8756  0.042617  0.268784  1.174698  1.114924  0.572203 -0.168022 -0.580264  \n",
       "8757  0.133922  0.279773  1.406187  1.501001  1.218251 -0.055078 -0.579938  \n",
       "8758  0.235977  0.240800  1.471956  1.590799  1.611849 -0.009876 -0.579613  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rbf_df - rbf_df.mean()) / rbf_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G_loss: 1.968428373336792:   1%|▏         | 4/300 [00:04<05:39,  1.15s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chanyoung\\Desktop\\RBFGAN\\generator.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m G \u001b[39m=\u001b[39m Generator(\u001b[39m36\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m imputation_list \u001b[39m=\u001b[39m train_G(dataset, G,  \u001b[39m1e-3\u001b[39;49m, \u001b[39m300\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\chanyoung\\Desktop\\RBFGAN\\generator.ipynb Cell 5\u001b[0m in \u001b[0;36mtrain_G\u001b[1;34m(dataset, Generator, lr, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m c_hat_list, imputations, x_loss \u001b[39m=\u001b[39m Generator(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer_G\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x_loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer_G\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chanyoung/Desktop/RBFGAN/generator.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m imputation_list\u001b[39m.\u001b[39mappend(imputations)\n",
      "File \u001b[1;32mc:\\Users\\chanyoung\\anaconda3\\envs\\cooling\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\chanyoung\\anaconda3\\envs\\cooling\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = Generator(36, 64)\n",
    "imputation_list = train_G(dataset, G,  1e-3, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_missing_data_rbf(df,rbf):\n",
    "    \n",
    "    values = ((df - df.mean()) / df.std()).values\n",
    "    shp = values.shape\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    \n",
    "    masks = ~np.isnan(values)\n",
    "    \n",
    "    masks = masks.reshape(shp)\n",
    "\n",
    "    deltas = np.array(make_deltas(masks))\n",
    "    values = torch.nan_to_num(torch.from_numpy(values).to(torch.float32))\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    deltas = torch.from_numpy(deltas).to(torch.float32)\n",
    "    rbf_x = torch.from_numpy(rbf_df.values).to(torch.float32)\n",
    "    dataset = torch.cat([rbf_x.unsqueeze_(0), deltas.unsqueeze_(0), masks.unsqueeze_(0)], dim = 0).unsqueeze_(0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def eval_model(model, rbf, realpath, dfpath):\n",
    "    \n",
    "    df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)\n",
    "    dataset = val_missing_data_rbf(df, rbf)\n",
    "    rbf_df = pd.read_csv(\"./RBFresult/\" + rbf)\n",
    "    dataset = dataset.to(device)\n",
    "\n",
    "    real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "    real_scaler = (real - df.mean()) / df.std()\n",
    "\n",
    "    df_scaler = ((df-df.mean()) / df.std()).values\n",
    "    masks = ~np.isnan(df_scaler)\n",
    "    masks = torch.from_numpy(masks).to(torch.float32)\n",
    "    \n",
    "    eval_masks = ~np.isnan(real_scaler.values)\n",
    "    eval_masks = torch.from_numpy(eval_masks).to(torch.float32)\n",
    "\n",
    "    test_masks = eval_masks - masks\n",
    "    real_scaler = torch.nan_to_num(torch.from_numpy(real_scaler.values).to(torch.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    c_hat_list, imputations, x_loss = model(dataset)\n",
    "\n",
    "    Nonscale_imputataion = pd.DataFrame(imputations[0].cpu().detach() , columns= df.columns)\n",
    "    Nonscale_imputataion = (Nonscale_imputataion * df.std()) + df.mean()\n",
    "    rbf_df = pd.DataFrame(rbf_df.values, columns= df.columns)\n",
    "    rbf_df = (rbf_df * df.std()) + df.mean()\n",
    "    \n",
    "    real = real.fillna(0)\n",
    "    df = df.fillna(0)\n",
    "    print(\"Scale MAE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(test_masks))\n",
    "    print(\"Scale MRE :\", torch.sum(torch.abs(imputations[0].cpu().detach() - real_scaler) * test_masks) / torch.sum(torch.abs(real_scaler * test_masks)))\n",
    "\n",
    "    print(\"Original MAE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"Original MRE :\", np.sum(np.abs((Nonscale_imputataion - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    print(\"RBF MAE :\", np.sum(np.abs((rbf_df - real).values * test_masks.cpu().numpy())) / np.sum(test_masks.cpu().numpy()))\n",
    "    print(\"RBF MRE :\", np.sum(np.abs((rbf_df - real).values * test_masks.cpu().numpy())) / np.sum(np.abs(real.values * test_masks.cpu().numpy())))\n",
    "\n",
    "    print('observation MAE :', np.sum(np.abs((Nonscale_imputataion - df).values * masks.cpu().numpy())) / np.sum(masks.cpu().numpy()))\n",
    "\n",
    "    return Nonscale_imputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "realpath = 'pm25_ground.txt'\n",
    "real = pd.read_csv(\"./dataset/\" + realpath).drop(['datetime'], axis = 1)\n",
    "df = pd.read_csv(\"./dataset/\" + dfpath).drop(['datetime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale MAE : tensor(0.2228)\n",
      "Scale MRE : tensor(0.3217)\n",
      "Original MAE : 17.954595193516763\n",
      "Original MRE : 0.2522054173453041\n",
      "RBF MAE : 26.244345089085357\n",
      "RBF MRE : 0.36865024996703916\n",
      "observation MAE : 8.382986230262004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001001</th>\n",
       "      <th>001002</th>\n",
       "      <th>001003</th>\n",
       "      <th>001004</th>\n",
       "      <th>001005</th>\n",
       "      <th>001006</th>\n",
       "      <th>001007</th>\n",
       "      <th>001008</th>\n",
       "      <th>001009</th>\n",
       "      <th>001010</th>\n",
       "      <th>...</th>\n",
       "      <th>001027</th>\n",
       "      <th>001028</th>\n",
       "      <th>001029</th>\n",
       "      <th>001030</th>\n",
       "      <th>001031</th>\n",
       "      <th>001032</th>\n",
       "      <th>001033</th>\n",
       "      <th>001034</th>\n",
       "      <th>001035</th>\n",
       "      <th>001036</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.979632</td>\n",
       "      <td>94.558028</td>\n",
       "      <td>109.012972</td>\n",
       "      <td>105.292300</td>\n",
       "      <td>104.623389</td>\n",
       "      <td>98.806747</td>\n",
       "      <td>102.341168</td>\n",
       "      <td>99.229637</td>\n",
       "      <td>100.945096</td>\n",
       "      <td>96.546109</td>\n",
       "      <td>...</td>\n",
       "      <td>76.657814</td>\n",
       "      <td>78.509137</td>\n",
       "      <td>93.531377</td>\n",
       "      <td>74.435169</td>\n",
       "      <td>89.525562</td>\n",
       "      <td>74.103350</td>\n",
       "      <td>88.368280</td>\n",
       "      <td>109.708148</td>\n",
       "      <td>100.071964</td>\n",
       "      <td>118.165729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.773533</td>\n",
       "      <td>97.222771</td>\n",
       "      <td>112.847540</td>\n",
       "      <td>109.052867</td>\n",
       "      <td>106.300672</td>\n",
       "      <td>101.935345</td>\n",
       "      <td>106.501526</td>\n",
       "      <td>101.818824</td>\n",
       "      <td>104.554178</td>\n",
       "      <td>98.539958</td>\n",
       "      <td>...</td>\n",
       "      <td>93.285501</td>\n",
       "      <td>80.232474</td>\n",
       "      <td>96.495995</td>\n",
       "      <td>89.972203</td>\n",
       "      <td>93.647284</td>\n",
       "      <td>76.142415</td>\n",
       "      <td>91.739778</td>\n",
       "      <td>111.762013</td>\n",
       "      <td>97.526683</td>\n",
       "      <td>109.480425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.546584</td>\n",
       "      <td>98.990375</td>\n",
       "      <td>116.139787</td>\n",
       "      <td>113.003751</td>\n",
       "      <td>108.182353</td>\n",
       "      <td>105.169956</td>\n",
       "      <td>110.835437</td>\n",
       "      <td>105.005373</td>\n",
       "      <td>108.455424</td>\n",
       "      <td>101.542161</td>\n",
       "      <td>...</td>\n",
       "      <td>101.235739</td>\n",
       "      <td>81.931966</td>\n",
       "      <td>98.443932</td>\n",
       "      <td>93.121400</td>\n",
       "      <td>97.075322</td>\n",
       "      <td>78.074774</td>\n",
       "      <td>95.394895</td>\n",
       "      <td>114.699009</td>\n",
       "      <td>102.464654</td>\n",
       "      <td>114.007791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121.463774</td>\n",
       "      <td>99.790112</td>\n",
       "      <td>119.029576</td>\n",
       "      <td>117.346926</td>\n",
       "      <td>110.447253</td>\n",
       "      <td>108.663489</td>\n",
       "      <td>115.472482</td>\n",
       "      <td>109.059993</td>\n",
       "      <td>112.779486</td>\n",
       "      <td>105.811113</td>\n",
       "      <td>...</td>\n",
       "      <td>102.226006</td>\n",
       "      <td>83.787046</td>\n",
       "      <td>99.414389</td>\n",
       "      <td>92.917473</td>\n",
       "      <td>99.833949</td>\n",
       "      <td>80.079464</td>\n",
       "      <td>99.463593</td>\n",
       "      <td>118.629706</td>\n",
       "      <td>105.957574</td>\n",
       "      <td>115.321597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115.965042</td>\n",
       "      <td>99.871445</td>\n",
       "      <td>121.933568</td>\n",
       "      <td>122.414836</td>\n",
       "      <td>113.389459</td>\n",
       "      <td>112.696303</td>\n",
       "      <td>120.620953</td>\n",
       "      <td>114.270940</td>\n",
       "      <td>117.706829</td>\n",
       "      <td>111.515048</td>\n",
       "      <td>...</td>\n",
       "      <td>104.394425</td>\n",
       "      <td>86.126502</td>\n",
       "      <td>99.783051</td>\n",
       "      <td>93.858217</td>\n",
       "      <td>102.235314</td>\n",
       "      <td>82.504918</td>\n",
       "      <td>104.119098</td>\n",
       "      <td>123.537302</td>\n",
       "      <td>109.087697</td>\n",
       "      <td>114.799340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>85.517552</td>\n",
       "      <td>66.028307</td>\n",
       "      <td>70.094838</td>\n",
       "      <td>65.534252</td>\n",
       "      <td>73.493956</td>\n",
       "      <td>66.055605</td>\n",
       "      <td>64.616278</td>\n",
       "      <td>64.528036</td>\n",
       "      <td>70.011654</td>\n",
       "      <td>69.110241</td>\n",
       "      <td>...</td>\n",
       "      <td>101.756341</td>\n",
       "      <td>110.175764</td>\n",
       "      <td>84.389473</td>\n",
       "      <td>76.380683</td>\n",
       "      <td>86.552459</td>\n",
       "      <td>119.705787</td>\n",
       "      <td>111.664071</td>\n",
       "      <td>96.144254</td>\n",
       "      <td>76.555851</td>\n",
       "      <td>65.435776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>82.107537</td>\n",
       "      <td>66.425545</td>\n",
       "      <td>73.556263</td>\n",
       "      <td>67.037746</td>\n",
       "      <td>74.498008</td>\n",
       "      <td>65.691271</td>\n",
       "      <td>72.138699</td>\n",
       "      <td>64.140210</td>\n",
       "      <td>70.108171</td>\n",
       "      <td>70.659920</td>\n",
       "      <td>...</td>\n",
       "      <td>109.730342</td>\n",
       "      <td>119.738275</td>\n",
       "      <td>77.498082</td>\n",
       "      <td>74.398222</td>\n",
       "      <td>80.346946</td>\n",
       "      <td>129.726792</td>\n",
       "      <td>125.189406</td>\n",
       "      <td>125.314947</td>\n",
       "      <td>75.624343</td>\n",
       "      <td>66.337765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>81.051560</td>\n",
       "      <td>69.216705</td>\n",
       "      <td>79.531669</td>\n",
       "      <td>70.265830</td>\n",
       "      <td>76.753433</td>\n",
       "      <td>67.533268</td>\n",
       "      <td>83.520990</td>\n",
       "      <td>65.965787</td>\n",
       "      <td>72.694449</td>\n",
       "      <td>74.851661</td>\n",
       "      <td>...</td>\n",
       "      <td>121.803290</td>\n",
       "      <td>132.171372</td>\n",
       "      <td>74.110290</td>\n",
       "      <td>75.558825</td>\n",
       "      <td>78.026563</td>\n",
       "      <td>139.256283</td>\n",
       "      <td>140.241324</td>\n",
       "      <td>160.064574</td>\n",
       "      <td>77.533807</td>\n",
       "      <td>70.336446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>84.923013</td>\n",
       "      <td>66.641258</td>\n",
       "      <td>85.776118</td>\n",
       "      <td>77.325712</td>\n",
       "      <td>84.580529</td>\n",
       "      <td>70.316736</td>\n",
       "      <td>95.181907</td>\n",
       "      <td>87.418769</td>\n",
       "      <td>76.202058</td>\n",
       "      <td>96.031915</td>\n",
       "      <td>...</td>\n",
       "      <td>129.920518</td>\n",
       "      <td>133.791729</td>\n",
       "      <td>79.738818</td>\n",
       "      <td>78.028144</td>\n",
       "      <td>66.076587</td>\n",
       "      <td>114.766354</td>\n",
       "      <td>139.852257</td>\n",
       "      <td>134.533681</td>\n",
       "      <td>98.645685</td>\n",
       "      <td>82.120613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>81.722431</td>\n",
       "      <td>75.521197</td>\n",
       "      <td>90.576436</td>\n",
       "      <td>77.149673</td>\n",
       "      <td>82.326852</td>\n",
       "      <td>73.203146</td>\n",
       "      <td>103.996087</td>\n",
       "      <td>72.364971</td>\n",
       "      <td>79.477517</td>\n",
       "      <td>85.122152</td>\n",
       "      <td>...</td>\n",
       "      <td>138.939020</td>\n",
       "      <td>145.293704</td>\n",
       "      <td>73.046655</td>\n",
       "      <td>80.161137</td>\n",
       "      <td>78.151603</td>\n",
       "      <td>145.536892</td>\n",
       "      <td>158.697031</td>\n",
       "      <td>212.763437</td>\n",
       "      <td>86.089627</td>\n",
       "      <td>86.408025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          001001     001002      001003      001004      001005      001006  \\\n",
       "0     112.979632  94.558028  109.012972  105.292300  104.623389   98.806747   \n",
       "1     116.773533  97.222771  112.847540  109.052867  106.300672  101.935345   \n",
       "2     119.546584  98.990375  116.139787  113.003751  108.182353  105.169956   \n",
       "3     121.463774  99.790112  119.029576  117.346926  110.447253  108.663489   \n",
       "4     115.965042  99.871445  121.933568  122.414836  113.389459  112.696303   \n",
       "...          ...        ...         ...         ...         ...         ...   \n",
       "8754   85.517552  66.028307   70.094838   65.534252   73.493956   66.055605   \n",
       "8755   82.107537  66.425545   73.556263   67.037746   74.498008   65.691271   \n",
       "8756   81.051560  69.216705   79.531669   70.265830   76.753433   67.533268   \n",
       "8757   84.923013  66.641258   85.776118   77.325712   84.580529   70.316736   \n",
       "8758   81.722431  75.521197   90.576436   77.149673   82.326852   73.203146   \n",
       "\n",
       "          001007      001008      001009      001010  ...      001027  \\\n",
       "0     102.341168   99.229637  100.945096   96.546109  ...   76.657814   \n",
       "1     106.501526  101.818824  104.554178   98.539958  ...   93.285501   \n",
       "2     110.835437  105.005373  108.455424  101.542161  ...  101.235739   \n",
       "3     115.472482  109.059993  112.779486  105.811113  ...  102.226006   \n",
       "4     120.620953  114.270940  117.706829  111.515048  ...  104.394425   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "8754   64.616278   64.528036   70.011654   69.110241  ...  101.756341   \n",
       "8755   72.138699   64.140210   70.108171   70.659920  ...  109.730342   \n",
       "8756   83.520990   65.965787   72.694449   74.851661  ...  121.803290   \n",
       "8757   95.181907   87.418769   76.202058   96.031915  ...  129.920518   \n",
       "8758  103.996087   72.364971   79.477517   85.122152  ...  138.939020   \n",
       "\n",
       "          001028     001029     001030      001031      001032      001033  \\\n",
       "0      78.509137  93.531377  74.435169   89.525562   74.103350   88.368280   \n",
       "1      80.232474  96.495995  89.972203   93.647284   76.142415   91.739778   \n",
       "2      81.931966  98.443932  93.121400   97.075322   78.074774   95.394895   \n",
       "3      83.787046  99.414389  92.917473   99.833949   80.079464   99.463593   \n",
       "4      86.126502  99.783051  93.858217  102.235314   82.504918  104.119098   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "8754  110.175764  84.389473  76.380683   86.552459  119.705787  111.664071   \n",
       "8755  119.738275  77.498082  74.398222   80.346946  129.726792  125.189406   \n",
       "8756  132.171372  74.110290  75.558825   78.026563  139.256283  140.241324   \n",
       "8757  133.791729  79.738818  78.028144   66.076587  114.766354  139.852257   \n",
       "8758  145.293704  73.046655  80.161137   78.151603  145.536892  158.697031   \n",
       "\n",
       "          001034      001035      001036  \n",
       "0     109.708148  100.071964  118.165729  \n",
       "1     111.762013   97.526683  109.480425  \n",
       "2     114.699009  102.464654  114.007791  \n",
       "3     118.629706  105.957574  115.321597  \n",
       "4     123.537302  109.087697  114.799340  \n",
       "...          ...         ...         ...  \n",
       "8754   96.144254   76.555851   65.435776  \n",
       "8755  125.314947   75.624343   66.337765  \n",
       "8756  160.064574   77.533807   70.336446  \n",
       "8757  134.533681   98.645685   82.120613  \n",
       "8758  212.763437   86.089627   86.408025  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(G, rbfpath, 'pm25_ground.txt', dfpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
