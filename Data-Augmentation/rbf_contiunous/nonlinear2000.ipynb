{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df491ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f5d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('C:/Users/chanyoung/Desktop/RBFfitting/data/nonlinear15/non_linear15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d79ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361462</td>\n",
       "      <td>0.868128</td>\n",
       "      <td>1.119836</td>\n",
       "      <td>0.770762</td>\n",
       "      <td>1.543228</td>\n",
       "      <td>-0.484979</td>\n",
       "      <td>1.202227</td>\n",
       "      <td>1.812575</td>\n",
       "      <td>-1.328527</td>\n",
       "      <td>-0.452863</td>\n",
       "      <td>1.329738</td>\n",
       "      <td>1.401647</td>\n",
       "      <td>0.991044</td>\n",
       "      <td>-0.811218</td>\n",
       "      <td>1.467842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.575887</td>\n",
       "      <td>0.502778</td>\n",
       "      <td>1.091785</td>\n",
       "      <td>0.330353</td>\n",
       "      <td>1.899189</td>\n",
       "      <td>-0.116958</td>\n",
       "      <td>0.987593</td>\n",
       "      <td>1.239788</td>\n",
       "      <td>-1.252347</td>\n",
       "      <td>-1.719436</td>\n",
       "      <td>0.306399</td>\n",
       "      <td>0.155447</td>\n",
       "      <td>1.327731</td>\n",
       "      <td>-1.011604</td>\n",
       "      <td>0.255159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.333128</td>\n",
       "      <td>0.269990</td>\n",
       "      <td>1.162955</td>\n",
       "      <td>0.159699</td>\n",
       "      <td>1.646715</td>\n",
       "      <td>1.511054</td>\n",
       "      <td>1.234128</td>\n",
       "      <td>1.932573</td>\n",
       "      <td>-1.371554</td>\n",
       "      <td>-0.929414</td>\n",
       "      <td>0.733253</td>\n",
       "      <td>-1.374973</td>\n",
       "      <td>1.109616</td>\n",
       "      <td>-0.876114</td>\n",
       "      <td>0.516849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.546209</td>\n",
       "      <td>0.297732</td>\n",
       "      <td>1.116530</td>\n",
       "      <td>0.482296</td>\n",
       "      <td>1.882979</td>\n",
       "      <td>-0.754731</td>\n",
       "      <td>0.762650</td>\n",
       "      <td>0.752762</td>\n",
       "      <td>-1.433568</td>\n",
       "      <td>-1.254173</td>\n",
       "      <td>1.054452</td>\n",
       "      <td>0.357797</td>\n",
       "      <td>1.062463</td>\n",
       "      <td>-0.555984</td>\n",
       "      <td>0.641373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.661727</td>\n",
       "      <td>0.587748</td>\n",
       "      <td>1.013359</td>\n",
       "      <td>0.635559</td>\n",
       "      <td>1.464764</td>\n",
       "      <td>-0.546585</td>\n",
       "      <td>1.445773</td>\n",
       "      <td>1.857547</td>\n",
       "      <td>-1.568815</td>\n",
       "      <td>-0.723346</td>\n",
       "      <td>1.086332</td>\n",
       "      <td>1.656669</td>\n",
       "      <td>1.188205</td>\n",
       "      <td>-1.103284</td>\n",
       "      <td>1.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-2.679979</td>\n",
       "      <td>-1.521583</td>\n",
       "      <td>0.961226</td>\n",
       "      <td>1.383197</td>\n",
       "      <td>-1.392430</td>\n",
       "      <td>-0.701504</td>\n",
       "      <td>-1.148679</td>\n",
       "      <td>0.558870</td>\n",
       "      <td>-0.177923</td>\n",
       "      <td>0.470893</td>\n",
       "      <td>0.907261</td>\n",
       "      <td>0.241810</td>\n",
       "      <td>-0.787315</td>\n",
       "      <td>-1.079895</td>\n",
       "      <td>1.162237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.977749</td>\n",
       "      <td>-1.478321</td>\n",
       "      <td>1.206590</td>\n",
       "      <td>0.352901</td>\n",
       "      <td>-0.829146</td>\n",
       "      <td>2.016453</td>\n",
       "      <td>-0.940043</td>\n",
       "      <td>-1.517751</td>\n",
       "      <td>-0.011627</td>\n",
       "      <td>-0.044789</td>\n",
       "      <td>0.697491</td>\n",
       "      <td>1.705942</td>\n",
       "      <td>-1.024586</td>\n",
       "      <td>-0.860960</td>\n",
       "      <td>0.453126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-2.235777</td>\n",
       "      <td>-1.149230</td>\n",
       "      <td>1.102117</td>\n",
       "      <td>0.655368</td>\n",
       "      <td>-1.112439</td>\n",
       "      <td>1.523969</td>\n",
       "      <td>-0.893636</td>\n",
       "      <td>-1.060217</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.303327</td>\n",
       "      <td>1.165866</td>\n",
       "      <td>0.894137</td>\n",
       "      <td>-0.852924</td>\n",
       "      <td>-0.960449</td>\n",
       "      <td>0.966107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-2.246968</td>\n",
       "      <td>-0.699402</td>\n",
       "      <td>1.122137</td>\n",
       "      <td>0.350573</td>\n",
       "      <td>-1.382963</td>\n",
       "      <td>-0.901993</td>\n",
       "      <td>-0.138510</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>-0.155820</td>\n",
       "      <td>0.145202</td>\n",
       "      <td>0.447543</td>\n",
       "      <td>0.029613</td>\n",
       "      <td>-0.210601</td>\n",
       "      <td>-1.063089</td>\n",
       "      <td>-0.596316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-1.995263</td>\n",
       "      <td>-1.034104</td>\n",
       "      <td>1.198406</td>\n",
       "      <td>-0.165868</td>\n",
       "      <td>-1.343177</td>\n",
       "      <td>1.984393</td>\n",
       "      <td>-0.326622</td>\n",
       "      <td>0.359332</td>\n",
       "      <td>-0.523444</td>\n",
       "      <td>-1.098315</td>\n",
       "      <td>-0.004930</td>\n",
       "      <td>1.727443</td>\n",
       "      <td>-0.617220</td>\n",
       "      <td>-1.105536</td>\n",
       "      <td>-1.851388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V0        V1        V2        V3        V4        V5        V6  \\\n",
       "0   1.361462  0.868128  1.119836  0.770762  1.543228 -0.484979  1.202227   \n",
       "1   1.575887  0.502778  1.091785  0.330353  1.899189 -0.116958  0.987593   \n",
       "2   1.333128  0.269990  1.162955  0.159699  1.646715  1.511054  1.234128   \n",
       "3   1.546209  0.297732  1.116530  0.482296  1.882979 -0.754731  0.762650   \n",
       "4   1.661727  0.587748  1.013359  0.635559  1.464764 -0.546585  1.445773   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -2.679979 -1.521583  0.961226  1.383197 -1.392430 -0.701504 -1.148679   \n",
       "96 -1.977749 -1.478321  1.206590  0.352901 -0.829146  2.016453 -0.940043   \n",
       "97 -2.235777 -1.149230  1.102117  0.655368 -1.112439  1.523969 -0.893636   \n",
       "98 -2.246968 -0.699402  1.122137  0.350573 -1.382963 -0.901993 -0.138510   \n",
       "99 -1.995263 -1.034104  1.198406 -0.165868 -1.343177  1.984393 -0.326622   \n",
       "\n",
       "          V7        V8        V9       V10       V11       V12       V13  \\\n",
       "0   1.812575 -1.328527 -0.452863  1.329738  1.401647  0.991044 -0.811218   \n",
       "1   1.239788 -1.252347 -1.719436  0.306399  0.155447  1.327731 -1.011604   \n",
       "2   1.932573 -1.371554 -0.929414  0.733253 -1.374973  1.109616 -0.876114   \n",
       "3   0.752762 -1.433568 -1.254173  1.054452  0.357797  1.062463 -0.555984   \n",
       "4   1.857547 -1.568815 -0.723346  1.086332  1.656669  1.188205 -1.103284   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.558870 -0.177923  0.470893  0.907261  0.241810 -0.787315 -1.079895   \n",
       "96 -1.517751 -0.011627 -0.044789  0.697491  1.705942 -1.024586 -0.860960   \n",
       "97 -1.060217  0.021164  0.303327  1.165866  0.894137 -0.852924 -0.960449   \n",
       "98  0.038130 -0.155820  0.145202  0.447543  0.029613 -0.210601 -1.063089   \n",
       "99  0.359332 -0.523444 -1.098315 -0.004930  1.727443 -0.617220 -1.105536   \n",
       "\n",
       "         V14  \n",
       "0   1.467842  \n",
       "1   0.255159  \n",
       "2   0.516849  \n",
       "3   0.641373  \n",
       "4   1.055944  \n",
       "..       ...  \n",
       "95  1.162237  \n",
       "96  0.453126  \n",
       "97  0.966107  \n",
       "98 -0.596316  \n",
       "99 -1.851388  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c599a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     V0   V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  V12  V13  V14\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_row = pd.DataFrame(np.zeros((19, 15)), columns=data_df.columns)\n",
    "empty_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c43b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_df)):\n",
    "    if i == 0:\n",
    "        con_df = pd.concat([data_df[i:i+1], empty_row])\n",
    "    else:\n",
    "        con_df2 = pd.concat([data_df[i:i+1], empty_row])\n",
    "        con_df = pd.concat([con_df, con_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e639cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df.reset_index(drop= True,inplace = True)\n",
    "con_df.to_csv('./data/nonlinear_GAIN_2000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbb38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False  # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3954b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.5\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = con_df.values\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "# def xavier_init(size):\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc68019",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu is True:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")       # Output is multi-variate\n",
    "else:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 2. Generator\n",
    "if use_gpu is True:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")\n",
    "else:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "\n",
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = F.relu(torch.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = F.relu(torch.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = torch.sigmoid(torch.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = F.relu(torch.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = F.relu(torch.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = torch.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = torch.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "def discriminator_loss(M, New_X, H):\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd07f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████                                                                         | 44/500 [00:00<00:01, 228.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Train_loss: 0.08595\n",
      "Test_loss: 0.09306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▍                                                         | 136/500 [00:00<00:01, 208.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n",
      "Train_loss: 0.06973\n",
      "Test_loss: 0.06849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████▏                                       | 248/500 [00:01<00:01, 208.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "Train_loss: 0.08263\n",
      "Test_loss: 0.07402\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████▋                            | 321/500 [00:01<00:00, 222.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 300\n",
      "Train_loss: 0.0489\n",
      "Test_loss: 0.0643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████▎         | 439/500 [00:02<00:00, 201.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 400\n",
      "Train_loss: 0.07372\n",
      "Test_loss: 0.05876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 206.51it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer_D = torch.optim.Adam(params=theta_D)\n",
    "optimizer_G = torch.optim.Adam(params=theta_G)\n",
    "\n",
    "#%% Start Iterations\n",
    "for it in tqdm(range(500)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device=\"cuda\")\n",
    "        M_mb = torch.tensor(M_mb, device=\"cuda\")\n",
    "        H_mb = torch.tensor(H_mb, device=\"cuda\")\n",
    "        New_X_mb = torch.tensor(New_X_mb, device=\"cuda\")\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb)\n",
    "        M_mb = torch.tensor(M_mb)\n",
    "        H_mb = torch.tensor(H_mb)\n",
    "        New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    optimizer_G.zero_grad()\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66e9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentationX = Data\n",
    "augmentationM = np.zeros_like(Data)\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    augmentationM[:,i] = 1.*B\n",
    "Z_mb = sample_Z(2000, 15) \n",
    "\n",
    "# Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = augmentationM\n",
    "X_mb = augmentationX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "if use_gpu is True:\n",
    "    X_mb = torch.tensor(X_mb, device='cuda')\n",
    "    M_mb = torch.tensor(M_mb, device='cuda')\n",
    "    New_X_mb = torch.tensor(New_X_mb, device='cuda')\n",
    "else:\n",
    "    X_mb = torch.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "\n",
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fb5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df = pd.DataFrame(imputed_data.detach().numpy(), columns = con_df.columns)\n",
    "imputed_df.to_csv('./data/augmentation_nonlinear_2000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4decbf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.494903</td>\n",
       "      <td>0.744631</td>\n",
       "      <td>0.507963</td>\n",
       "      <td>0.677852</td>\n",
       "      <td>0.900503</td>\n",
       "      <td>0.410103</td>\n",
       "      <td>0.860087</td>\n",
       "      <td>0.871903</td>\n",
       "      <td>0.177729</td>\n",
       "      <td>0.518851</td>\n",
       "      <td>0.929120</td>\n",
       "      <td>0.565431</td>\n",
       "      <td>0.551208</td>\n",
       "      <td>0.351952</td>\n",
       "      <td>0.858783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.583092</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.542498</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>0.469148</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.436131</td>\n",
       "      <td>0.333661</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.551435</td>\n",
       "      <td>0.569488</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.398280</td>\n",
       "      <td>0.434470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.584911</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.540913</td>\n",
       "      <td>0.518825</td>\n",
       "      <td>0.473896</td>\n",
       "      <td>0.492872</td>\n",
       "      <td>0.435741</td>\n",
       "      <td>0.366745</td>\n",
       "      <td>0.520985</td>\n",
       "      <td>0.473305</td>\n",
       "      <td>0.551034</td>\n",
       "      <td>0.569488</td>\n",
       "      <td>0.501583</td>\n",
       "      <td>0.398280</td>\n",
       "      <td>0.483817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.598356</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.564842</td>\n",
       "      <td>0.518825</td>\n",
       "      <td>0.417770</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.467389</td>\n",
       "      <td>0.309040</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.560734</td>\n",
       "      <td>0.561949</td>\n",
       "      <td>0.492121</td>\n",
       "      <td>0.398280</td>\n",
       "      <td>0.446381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570430</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.555601</td>\n",
       "      <td>0.518825</td>\n",
       "      <td>0.469148</td>\n",
       "      <td>0.528011</td>\n",
       "      <td>0.437103</td>\n",
       "      <td>0.405531</td>\n",
       "      <td>0.517276</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.551435</td>\n",
       "      <td>0.563758</td>\n",
       "      <td>0.480119</td>\n",
       "      <td>0.356608</td>\n",
       "      <td>0.483817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.626119</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.555601</td>\n",
       "      <td>0.519747</td>\n",
       "      <td>0.427820</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.430826</td>\n",
       "      <td>0.259434</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.454175</td>\n",
       "      <td>0.551435</td>\n",
       "      <td>0.582786</td>\n",
       "      <td>0.480609</td>\n",
       "      <td>0.299080</td>\n",
       "      <td>0.483817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.576864</td>\n",
       "      <td>0.445403</td>\n",
       "      <td>0.529931</td>\n",
       "      <td>0.506971</td>\n",
       "      <td>0.469148</td>\n",
       "      <td>0.633628</td>\n",
       "      <td>0.411696</td>\n",
       "      <td>0.265557</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.551435</td>\n",
       "      <td>0.577766</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.252004</td>\n",
       "      <td>0.385198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.445399</td>\n",
       "      <td>0.391899</td>\n",
       "      <td>0.440268</td>\n",
       "      <td>0.509990</td>\n",
       "      <td>0.469148</td>\n",
       "      <td>0.494465</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.405531</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.481313</td>\n",
       "      <td>0.475979</td>\n",
       "      <td>0.659991</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.398280</td>\n",
       "      <td>0.483817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.598356</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.552377</td>\n",
       "      <td>0.518825</td>\n",
       "      <td>0.469148</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.467389</td>\n",
       "      <td>0.292202</td>\n",
       "      <td>0.504884</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.549335</td>\n",
       "      <td>0.569488</td>\n",
       "      <td>0.480565</td>\n",
       "      <td>0.304795</td>\n",
       "      <td>0.409936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.598356</td>\n",
       "      <td>0.465127</td>\n",
       "      <td>0.552633</td>\n",
       "      <td>0.518825</td>\n",
       "      <td>0.447251</td>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.436242</td>\n",
       "      <td>0.320904</td>\n",
       "      <td>0.515132</td>\n",
       "      <td>0.470468</td>\n",
       "      <td>0.551435</td>\n",
       "      <td>0.569488</td>\n",
       "      <td>0.479047</td>\n",
       "      <td>0.338920</td>\n",
       "      <td>0.438941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V0        V1        V2        V3        V4        V5        V6  \\\n",
       "0     0.494903  0.744631  0.507963  0.677852  0.900503  0.410103  0.860087   \n",
       "1     0.583092  0.485068  0.542498  0.492074  0.469148  0.515734  0.436131   \n",
       "2     0.584911  0.485068  0.540913  0.518825  0.473896  0.492872  0.435741   \n",
       "3     0.598356  0.485068  0.564842  0.518825  0.417770  0.515734  0.467389   \n",
       "4     0.570430  0.485068  0.555601  0.518825  0.469148  0.528011  0.437103   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.626119  0.485068  0.555601  0.519747  0.427820  0.515734  0.430826   \n",
       "1996  0.576864  0.445403  0.529931  0.506971  0.469148  0.633628  0.411696   \n",
       "1997  0.445399  0.391899  0.440268  0.509990  0.469148  0.494465  0.409524   \n",
       "1998  0.598356  0.485068  0.552377  0.518825  0.469148  0.515734  0.467389   \n",
       "1999  0.598356  0.465127  0.552633  0.518825  0.447251  0.515734  0.436242   \n",
       "\n",
       "            V7        V8        V9       V10       V11       V12       V13  \\\n",
       "0     0.871903  0.177729  0.518851  0.929120  0.565431  0.551208  0.351952   \n",
       "1     0.333661  0.504884  0.494785  0.551435  0.569488  0.500028  0.398280   \n",
       "2     0.366745  0.520985  0.473305  0.551034  0.569488  0.501583  0.398280   \n",
       "3     0.309040  0.504884  0.494785  0.560734  0.561949  0.492121  0.398280   \n",
       "4     0.405531  0.517276  0.494785  0.551435  0.563758  0.480119  0.356608   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.259434  0.504884  0.454175  0.551435  0.582786  0.480609  0.299080   \n",
       "1996  0.265557  0.504884  0.474915  0.551435  0.577766  0.500028  0.252004   \n",
       "1997  0.405531  0.504884  0.481313  0.475979  0.659991  0.500028  0.398280   \n",
       "1998  0.292202  0.504884  0.494785  0.549335  0.569488  0.480565  0.304795   \n",
       "1999  0.320904  0.515132  0.470468  0.551435  0.569488  0.479047  0.338920   \n",
       "\n",
       "           V14  \n",
       "0     0.858783  \n",
       "1     0.434470  \n",
       "2     0.483817  \n",
       "3     0.446381  \n",
       "4     0.483817  \n",
       "...        ...  \n",
       "1995  0.483817  \n",
       "1996  0.385198  \n",
       "1997  0.483817  \n",
       "1998  0.409936  \n",
       "1999  0.438941  \n",
       "\n",
       "[2000 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
