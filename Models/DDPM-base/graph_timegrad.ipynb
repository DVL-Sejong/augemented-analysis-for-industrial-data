{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from gluonts.core.component import validated\n",
    "\n",
    "from pts.model import weighted_average\n",
    "from pts.modules import GaussianDiffusion, DiffusionOutput, MeanScaler, NOPScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, proj_dim, max_steps=500):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            \"embedding\", self._build_embedding(dim, max_steps), persistent=False\n",
    "        )\n",
    "        self.projection1 = nn.Linear(dim * 2, proj_dim)\n",
    "        self.projection2 = nn.Linear(proj_dim, proj_dim)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        x = self.embedding[diffusion_step]\n",
    "        x = self.projection1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = F.silu(x)\n",
    "        return x\n",
    "\n",
    "    def _build_embedding(self, dim, max_steps):\n",
    "        steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n",
    "        dims = torch.arange(dim).unsqueeze(0)  # [1,dim]\n",
    "        table = steps * 10.0 ** (dims * 4.0 / dim)  # [T,dim]\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n",
    "        return table\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, residual_channels, dilation):\n",
    "        super().__init__()\n",
    "        self.dilated_conv = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            2 * residual_channels,\n",
    "            3,\n",
    "            padding=dilation,\n",
    "            dilation=dilation,\n",
    "            padding_mode=\"circular\",\n",
    "        )\n",
    "        self.diffusion_projection = nn.Linear(hidden_size, residual_channels)\n",
    "        self.conditioner_projection = nn.Conv1d(\n",
    "            1, 2 * residual_channels, 1, padding=2, padding_mode=\"circular\"\n",
    "        )\n",
    "        self.output_projection = nn.Conv1d(residual_channels, 2 * residual_channels, 1)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conditioner_projection.weight)\n",
    "        nn.init.kaiming_normal_(self.output_projection.weight)\n",
    "\n",
    "    def forward(self, x, conditioner, diffusion_step):\n",
    "        diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n",
    "        conditioner = self.conditioner_projection(conditioner)\n",
    "\n",
    "        y = x + diffusion_step\n",
    "        y = self.dilated_conv(y) + conditioner\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)\n",
    "\n",
    "        y = self.output_projection(y)\n",
    "        y = F.leaky_relu(y, 0.4)\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        return (x + residual) / math.sqrt(2.0), skip\n",
    "\n",
    "\n",
    "class CondUpsampler(nn.Module):\n",
    "    def __init__(self, cond_length, target_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cond_length, target_dim // 2)\n",
    "        self.linear2 = nn.Linear(target_dim // 2, target_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "        x = self.linear2(x)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EpsilonTheta(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dim,\n",
    "        cond_length,\n",
    "        time_emb_dim=16,\n",
    "        residual_layers=8,\n",
    "        residual_channels=8,\n",
    "        dilation_cycle_length=2,\n",
    "        residual_hidden=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Conv1d(\n",
    "            1, residual_channels, 1, padding=2, padding_mode=\"circular\"\n",
    "        )\n",
    "        self.diffusion_embedding = DiffusionEmbedding(\n",
    "            time_emb_dim, proj_dim=residual_hidden\n",
    "        )\n",
    "        self.cond_upsampler = CondUpsampler(\n",
    "            target_dim=target_dim, cond_length=cond_length\n",
    "        )\n",
    "        self.residual_layers = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    residual_channels=residual_channels,\n",
    "                    dilation=2 ** (i % dilation_cycle_length),\n",
    "                    hidden_size=residual_hidden,\n",
    "                )\n",
    "                for i in range(residual_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.skip_projection = nn.Conv1d(residual_channels, residual_channels, 3)\n",
    "        self.output_projection = nn.Conv1d(residual_channels, 1, 3)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.input_projection.weight)\n",
    "        nn.init.kaiming_normal_(self.skip_projection.weight)\n",
    "        nn.init.zeros_(self.output_projection.weight)\n",
    "\n",
    "    def forward(self, inputs, time, cond):\n",
    "        x = self.input_projection(inputs)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "\n",
    "        diffusion_step = self.diffusion_embedding(time)\n",
    "        cond_up = self.cond_upsampler(cond)\n",
    "        skip = []\n",
    "        for layer in self.residual_layers:\n",
    "            x, skip_connection = layer(x, cond_up, diffusion_step)\n",
    "            skip.append(skip_connection)\n",
    "\n",
    "        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n",
    "        x = self.skip_projection(x)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "        x = self.output_projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGradTrainingNetwork(nn.Module):\n",
    "    @validated()\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_layers: int,\n",
    "        num_cells: int,\n",
    "        cell_type: str,\n",
    "        history_length: int,\n",
    "        context_length: int,\n",
    "        prediction_length: int,\n",
    "        dropout_rate: float,\n",
    "        lags_seq: List[int],\n",
    "        target_dim: int,\n",
    "        conditioning_length: int,\n",
    "        diff_steps: int,\n",
    "        loss_type: str,\n",
    "        beta_end: float,\n",
    "        beta_schedule: str,\n",
    "        residual_layers: int,\n",
    "        residual_channels: int,\n",
    "        dilation_cycle_length: int,\n",
    "        cardinality: List[int] = [1],\n",
    "        embedding_dimension: int = 1,\n",
    "        scaling: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "    \n",
    "        super().__init__(**kwargs)\n",
    "        self.target_dim = target_dim\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.history_length = history_length\n",
    "        self.scaling = scaling\n",
    "\n",
    "        assert len(set(lags_seq)) == len(lags_seq), \"no duplicated lags allowed!\"\n",
    "        lags_seq.sort()\n",
    "        self.lags_seq = lags_seq\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        rnn_cls = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=input_size,\n",
    "            hidden_size=num_cells,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.denoise_fn = EpsilonTheta(\n",
    "            target_dim=target_dim,\n",
    "            cond_length=conditioning_length,\n",
    "            residual_layers=residual_layers,\n",
    "            residual_channels=residual_channels,\n",
    "            dilation_cycle_length=dilation_cycle_length,\n",
    "        )\n",
    "\n",
    "        self.diffusion = GaussianDiffusion(\n",
    "            self.denoise_fn,\n",
    "            input_size=target_dim,\n",
    "            diff_steps=diff_steps,\n",
    "            loss_type=loss_type,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule=beta_schedule,\n",
    "        )\n",
    "\n",
    "        self.distr_output = DiffusionOutput(\n",
    "            self.diffusion, input_size=target_dim, cond_size=conditioning_length\n",
    "        )\n",
    "\n",
    "        self.proj_dist_args = self.distr_output.get_args_proj(num_cells)\n",
    "\n",
    "        self.embed_dim = 1\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=self.target_dim, embedding_dim=self.embed_dim\n",
    "        )\n",
    "\n",
    "        if self.scaling:\n",
    "            self.scaler = MeanScaler(keepdim=True)\n",
    "        else:\n",
    "            self.scaler = NOPScaler(keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_lagged_subsequences(\n",
    "        sequence: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        indices: List[int],\n",
    "        subsequences_length: int = 1,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence\n",
    "            the sequence from which lagged subsequences should be extracted.\n",
    "            Shape: (N, T, C).\n",
    "        sequence_length\n",
    "            length of sequence in the T (time) dimension (axis = 1).\n",
    "        indices\n",
    "            list of lag indices to be used.\n",
    "        subsequences_length\n",
    "            length of the subsequences to be extracted.\n",
    "        Returns\n",
    "        --------\n",
    "        lagged : Tensor\n",
    "            a tensor of shape (N, S, C, I),\n",
    "            where S = subsequences_length and I = len(indices),\n",
    "            containing lagged subsequences.\n",
    "            Specifically, lagged[i, :, j, k] = sequence[i, -indices[k]-S+j, :].\n",
    "        \"\"\"\n",
    "        # we must have: history_length + begin_index >= 0\n",
    "        # that is: history_length - lag_index - sequence_length >= 0\n",
    "        # hence the following assert\n",
    "        assert max(indices) + subsequences_length <= sequence_length, (\n",
    "            f\"lags cannot go further than history length, found lag \"\n",
    "            f\"{max(indices)} while history length is only {sequence_length}\"\n",
    "        )\n",
    "        assert all(lag_index >= 0 for lag_index in indices)\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...].unsqueeze(1))\n",
    "        return torch.cat(lagged_values, dim=1).permute(0, 2, 3, 1)\n",
    "\n",
    "    def unroll(\n",
    "        self,\n",
    "        lags: torch.Tensor,\n",
    "        scale: torch.Tensor,\n",
    "        time_feat: torch.Tensor,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        unroll_length: int,\n",
    "        begin_state: Optional[Union[List[torch.Tensor], torch.Tensor]] = None,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        Union[List[torch.Tensor], torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "\n",
    "        # (batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        lags_scaled = lags / scale.unsqueeze(-1)\n",
    "\n",
    "        # assert_shape(\n",
    "        #     lags_scaled, (-1, unroll_length, self.target_dim, len(self.lags_seq)),\n",
    "        # )\n",
    "\n",
    "        input_lags = lags_scaled.reshape(\n",
    "            (-1, unroll_length, len(self.lags_seq) * self.target_dim)\n",
    "        )\n",
    "\n",
    "        # (batch_size, target_dim, embed_dim)\n",
    "        index_embeddings = self.embed(target_dimension_indicator)\n",
    "        # assert_shape(index_embeddings, (-1, self.target_dim, self.embed_dim))\n",
    "\n",
    "        # (batch_size, seq_len, target_dim * embed_dim)\n",
    "        repeated_index_embeddings = (\n",
    "            index_embeddings.unsqueeze(1)\n",
    "            .expand(-1, unroll_length, -1, -1)\n",
    "            .reshape((-1, unroll_length, self.target_dim * self.embed_dim))\n",
    "        )\n",
    "\n",
    "        # (batch_size, sub_seq_len, input_dim)\n",
    "        inputs = torch.cat((input_lags, repeated_index_embeddings, time_feat), dim=-1)\n",
    "\n",
    "        # unroll encoder\n",
    "        outputs, state = self.rnn(inputs, begin_state)\n",
    "\n",
    "        # assert_shape(outputs, (-1, unroll_length, self.num_cells))\n",
    "        # for s in state:\n",
    "        #     assert_shape(s, (-1, self.num_cells))\n",
    "\n",
    "        # assert_shape(\n",
    "        #     lags_scaled, (-1, unroll_length, self.target_dim, len(self.lags_seq)),\n",
    "        # )\n",
    "\n",
    "        return outputs, state, lags_scaled, inputs\n",
    "\n",
    "    def unroll_encoder(\n",
    "        self,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: Optional[torch.Tensor],\n",
    "        future_target_cdf: Optional[torch.Tensor],\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        Union[List[torch.Tensor], torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Unrolls the RNN encoder over past and, if present, future data.\n",
    "        Returns outputs and state of the encoder, plus the scale of\n",
    "        past_target_cdf and a vector of static features that was constructed\n",
    "        and fed as input to the encoder. All tensor arguments should have NTC\n",
    "        layout.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        past_time_feat\n",
    "            Past time features (batch_size, history_length, num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "        future_target_cdf\n",
    "            Future marginal CDF transformed target values (batch_size,\n",
    "            prediction_length, target_dim)\n",
    "        target_dimension_indicator\n",
    "            Dimensionality of the time series (batch_size, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs\n",
    "            RNN outputs (batch_size, seq_len, num_cells)\n",
    "        states\n",
    "            RNN states. Nested list with (batch_size, num_cells) tensors with\n",
    "        dimensions target_dim x num_layers x (batch_size, num_cells)\n",
    "        scale\n",
    "            Mean scales for the time series (batch_size, 1, target_dim)\n",
    "        lags_scaled\n",
    "            Scaled lags(batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        inputs\n",
    "            inputs to the RNN\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        if future_time_feat is None or future_target_cdf is None:\n",
    "            time_feat = past_time_feat[:, -self.context_length :, ...]\n",
    "            sequence = past_target_cdf\n",
    "            sequence_length = self.history_length\n",
    "            subsequences_length = self.context_length\n",
    "        else:\n",
    "            time_feat = torch.cat(\n",
    "                (past_time_feat[:, -self.context_length :, ...], future_time_feat),\n",
    "                dim=1,\n",
    "            )\n",
    "            sequence = torch.cat((past_target_cdf, future_target_cdf), dim=1)\n",
    "            sequence_length = self.history_length + self.prediction_length\n",
    "            subsequences_length = self.context_length + self.prediction_length\n",
    "\n",
    "        # (batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        lags = self.get_lagged_subsequences(\n",
    "            sequence=sequence,\n",
    "            sequence_length=sequence_length,\n",
    "            indices=self.lags_seq,\n",
    "            subsequences_length=subsequences_length,\n",
    "        )\n",
    "\n",
    "        # scale is computed on the context length last units of the past target\n",
    "        # scale shape is (batch_size, 1, target_dim)\n",
    "        _, scale = self.scaler(\n",
    "            past_target_cdf[:, -self.context_length :, ...],\n",
    "            past_observed_values[:, -self.context_length :, ...],\n",
    "        )\n",
    "\n",
    "        outputs, states, lags_scaled, inputs = self.unroll(\n",
    "            lags=lags,\n",
    "            scale=scale,\n",
    "            time_feat=time_feat,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "            unroll_length=subsequences_length,\n",
    "            begin_state=None,\n",
    "        )\n",
    "\n",
    "        return outputs, states, scale, lags_scaled, inputs\n",
    "\n",
    "    def distr_args(self, rnn_outputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns the distribution of DeepVAR with respect to the RNN outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_outputs\n",
    "            Outputs of the unrolled RNN (batch_size, seq_len, num_cells)\n",
    "        scale\n",
    "            Mean scale for each time series (batch_size, 1, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distr\n",
    "            Distribution instance\n",
    "        distr_args\n",
    "            Distribution arguments\n",
    "        \"\"\"\n",
    "        (distr_args,) = self.proj_dist_args(rnn_outputs)\n",
    "\n",
    "        # # compute likelihood of target given the predicted parameters\n",
    "        # distr = self.distr_output.distribution(distr_args, scale=scale)\n",
    "\n",
    "        # return distr, distr_args\n",
    "        return distr_args\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: torch.Tensor,\n",
    "        future_target_cdf: torch.Tensor,\n",
    "        future_observed_values: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the loss for training DeepVAR, all inputs tensors representing\n",
    "        time series have NTC layout.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        past_time_feat\n",
    "            Dynamic features of past time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "        future_target_cdf\n",
    "            Future marginal CDF transformed target values (batch_size,\n",
    "            prediction_length, target_dim)\n",
    "        future_observed_values\n",
    "            Indicator whether or not the future values were observed\n",
    "            (batch_size, prediction_length, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distr\n",
    "            Loss with shape (batch_size, 1)\n",
    "        likelihoods\n",
    "            Likelihoods for each time step\n",
    "            (batch_size, context + prediction_length, 1)\n",
    "        distr_args\n",
    "            Distribution arguments (context + prediction_length,\n",
    "            number_of_arguments)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = self.context_length + self.prediction_length\n",
    "\n",
    "        # unroll the decoder in \"training mode\", i.e. by providing future data\n",
    "        # as well\n",
    "        rnn_outputs, _, scale, _, _ = self.unroll_encoder(\n",
    "            past_time_feat=past_time_feat,\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            past_observed_values=past_observed_values,\n",
    "            past_is_pad=past_is_pad,\n",
    "            future_time_feat=future_time_feat,\n",
    "            future_target_cdf=future_target_cdf,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "        )\n",
    "\n",
    "        # put together target sequence\n",
    "        # (batch_size, seq_len, target_dim)\n",
    "        target = torch.cat(\n",
    "            (past_target_cdf[:, -self.context_length :, ...], future_target_cdf),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # assert_shape(target, (-1, seq_len, self.target_dim))\n",
    "\n",
    "        distr_args = self.distr_args(rnn_outputs=rnn_outputs)\n",
    "        if self.scaling:\n",
    "            self.diffusion.scale = scale\n",
    "\n",
    "        # we sum the last axis to have the same shape for all likelihoods\n",
    "        # (batch_size, subseq_length, 1)\n",
    "\n",
    "        likelihoods = self.diffusion.log_prob(target, distr_args).unsqueeze(-1)\n",
    "\n",
    "        # assert_shape(likelihoods, (-1, seq_len, 1))\n",
    "\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # (batch_size, subseq_length, target_dim)\n",
    "        observed_values = torch.cat(\n",
    "            (\n",
    "                past_observed_values[:, -self.context_length :, ...],\n",
    "                future_observed_values,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # mask the loss at one time step if one or more observations is missing\n",
    "        # in the target dimensions (batch_size, subseq_length, 1)\n",
    "        loss_weights, _ = observed_values.min(dim=-1, keepdim=True)\n",
    "\n",
    "        # assert_shape(loss_weights, (-1, seq_len, 1))\n",
    "\n",
    "        loss = weighted_average(likelihoods, weights=loss_weights, dim=1)\n",
    "\n",
    "        # assert_shape(loss, (-1, -1, 1))\n",
    "\n",
    "        # self.distribution = distr\n",
    "\n",
    "        return (loss.mean(), likelihoods, distr_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TimeGradPredictionNetwork(TimeGradTrainingNetwork):\n",
    "    def __init__(self, num_parallel_samples: int, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "\n",
    "        # for decoding the lags are shifted by one,\n",
    "        # at the first time-step of the decoder a lag of one corresponds to\n",
    "        # the last target value\n",
    "        self.shifted_lags = [l - 1 for l in self.lags_seq]\n",
    "\n",
    "    def sampling_decoder(\n",
    "        self,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        time_feat: torch.Tensor,\n",
    "        scale: torch.Tensor,\n",
    "        begin_states: Union[List[torch.Tensor], torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes sample paths by unrolling the RNN starting with a initial\n",
    "        input and state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        time_feat\n",
    "            Dynamic features of future time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        scale\n",
    "            Mean scale for each time series (batch_size, 1, target_dim)\n",
    "        begin_states\n",
    "            List of initial states for the RNN layers (batch_size, num_cells)\n",
    "        Returns\n",
    "        --------\n",
    "        sample_paths : Tensor\n",
    "            A tensor containing sampled paths. Shape: (1, num_sample_paths,\n",
    "            prediction_length, target_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        def repeat(tensor, dim=0):\n",
    "            return tensor.repeat_interleave(repeats=self.num_parallel_samples, dim=dim)\n",
    "\n",
    "        # blows-up the dimension of each tensor to\n",
    "        # batch_size * self.num_sample_paths for increasing parallelism\n",
    "        repeated_past_target_cdf = repeat(past_target_cdf)\n",
    "        repeated_time_feat = repeat(time_feat)\n",
    "        repeated_scale = repeat(scale)\n",
    "        if self.scaling:\n",
    "            self.diffusion.scale = repeated_scale\n",
    "        repeated_target_dimension_indicator = repeat(target_dimension_indicator)\n",
    "\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            repeated_states = [repeat(s, dim=1) for s in begin_states]\n",
    "        else:\n",
    "            repeated_states = repeat(begin_states, dim=1)\n",
    "\n",
    "        future_samples = []\n",
    "\n",
    "        # for each future time-units we draw new samples for this time-unit\n",
    "        # and update the state\n",
    "        for k in range(self.prediction_length):\n",
    "            lags = self.get_lagged_subsequences(\n",
    "                sequence=repeated_past_target_cdf,\n",
    "                sequence_length=self.history_length + k,\n",
    "                indices=self.shifted_lags,\n",
    "                subsequences_length=1,\n",
    "            )\n",
    "\n",
    "            rnn_outputs, repeated_states, _, _ = self.unroll(\n",
    "                begin_state=repeated_states,\n",
    "                lags=lags,\n",
    "                scale=repeated_scale,\n",
    "                time_feat=repeated_time_feat[:, k : k + 1, ...],\n",
    "                target_dimension_indicator=repeated_target_dimension_indicator,\n",
    "                unroll_length=1,\n",
    "            )\n",
    "\n",
    "            distr_args = self.distr_args(rnn_outputs=rnn_outputs)\n",
    "\n",
    "            # (batch_size, 1, target_dim)\n",
    "            new_samples = self.diffusion.sample(cond=distr_args)\n",
    "\n",
    "            # (batch_size, seq_len, target_dim)\n",
    "            future_samples.append(new_samples)\n",
    "            repeated_past_target_cdf = torch.cat(\n",
    "                (repeated_past_target_cdf, new_samples), dim=1\n",
    "            )\n",
    "\n",
    "        # (batch_size * num_samples, prediction_length, target_dim)\n",
    "        samples = torch.cat(future_samples, dim=1)\n",
    "\n",
    "        # (batch_size, num_samples, prediction_length, target_dim)\n",
    "        return samples.reshape(\n",
    "            (\n",
    "                -1,\n",
    "                self.num_parallel_samples,\n",
    "                self.prediction_length,\n",
    "                self.target_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts samples given the trained DeepVAR model.\n",
    "        All tensors should have NTC layout.\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        past_time_feat\n",
    "            Dynamic features of past time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_paths : Tensor\n",
    "            A tensor containing sampled paths (1, num_sample_paths,\n",
    "            prediction_length, target_dim).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # mark padded data as unobserved\n",
    "        # (batch_size, target_dim, seq_len)\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # unroll the decoder in \"prediction mode\", i.e. with past data only\n",
    "        _, begin_states, scale, _, _ = self.unroll_encoder(\n",
    "            past_time_feat=past_time_feat,\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            past_observed_values=past_observed_values,\n",
    "            past_is_pad=past_is_pad,\n",
    "            future_time_feat=None,\n",
    "            future_target_cdf=None,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "        )\n",
    "\n",
    "        return self.sampling_decoder(\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "            time_feat=future_time_feat,\n",
    "            scale=scale,\n",
    "            begin_states=begin_states,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolingts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
