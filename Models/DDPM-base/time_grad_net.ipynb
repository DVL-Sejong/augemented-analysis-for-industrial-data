{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import loss\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGradTrainingNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_size: int,\n",
    "        num_layers: int,\n",
    "        num_cells: int,\n",
    "        cell_type: str,\n",
    "        history_length: int,\n",
    "        context_length: int,\n",
    "        prediction_length: int,\n",
    "        dropout_rate: float,\n",
    "        lags_seq: List[int],\n",
    "        target_dim: int,\n",
    "        conditioning_length: int,\n",
    "        diff_steps: int,\n",
    "        loss_type: str,\n",
    "        beta_end: float,\n",
    "        beta_schedule: str,\n",
    "        residual_layers: int,\n",
    "        residual_channels: int,\n",
    "        dilation_cycle_length: int,\n",
    "        cardinality: List[int] = [1],\n",
    "        embedding_dimension: int = 1,\n",
    "        scaling: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGradTrainingNetwork(nn.Module):\n",
    "    @validated()\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_layers: int,\n",
    "        num_cells: int,\n",
    "        cell_type: str,\n",
    "        history_length: int,\n",
    "        context_length: int,\n",
    "        prediction_length: int,\n",
    "        dropout_rate: float,\n",
    "        lags_seq: List[int],\n",
    "        target_dim: int,\n",
    "        conditioning_length: int,\n",
    "        diff_steps: int,\n",
    "        loss_type: str,\n",
    "        beta_end: float,\n",
    "        beta_schedule: str,\n",
    "        residual_layers: int,\n",
    "        residual_channels: int,\n",
    "        dilation_cycle_length: int,\n",
    "        cardinality: List[int] = [1],\n",
    "        embedding_dimension: int = 1,\n",
    "        scaling: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.target_dim = target_dim\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.history_length = history_length\n",
    "        self.scaling = scaling\n",
    "\n",
    "        assert len(set(lags_seq)) == len(lags_seq), \"no duplicated lags allowed!\"\n",
    "        lags_seq.sort()\n",
    "        self.lags_seq = lags_seq\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        rnn_cls = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=input_size,\n",
    "            hidden_size=num_cells,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.denoise_fn = EpsilonTheta(\n",
    "            target_dim=target_dim,\n",
    "            cond_length=conditioning_length,\n",
    "            residual_layers=residual_layers,\n",
    "            residual_channels=residual_channels,\n",
    "            dilation_cycle_length=dilation_cycle_length,\n",
    "        )\n",
    "\n",
    "        self.diffusion = GaussianDiffusion(\n",
    "            self.denoise_fn,\n",
    "            input_size=target_dim,\n",
    "            diff_steps=diff_steps,\n",
    "            loss_type=loss_type,\n",
    "            beta_end=beta_end,\n",
    "            beta_schedule=beta_schedule,\n",
    "        )\n",
    "\n",
    "        self.distr_output = DiffusionOutput(\n",
    "            self.diffusion, input_size=target_dim, cond_size=conditioning_length\n",
    "        )\n",
    "\n",
    "        self.proj_dist_args = self.distr_output.get_args_proj(num_cells)\n",
    "\n",
    "        self.embed_dim = 1\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=self.target_dim, embedding_dim=self.embed_dim\n",
    "        )\n",
    "\n",
    "        if self.scaling:\n",
    "            self.scaler = MeanScaler(keepdim=True)\n",
    "        else:\n",
    "            self.scaler = NOPScaler(keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_lagged_subsequences(\n",
    "        sequence: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        indices: List[int],\n",
    "        subsequences_length: int = 1,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence\n",
    "            the sequence from which lagged subsequences should be extracted.\n",
    "            Shape: (N, T, C).\n",
    "        sequence_length\n",
    "            length of sequence in the T (time) dimension (axis = 1).\n",
    "        indices\n",
    "            list of lag indices to be used.\n",
    "        subsequences_length\n",
    "            length of the subsequences to be extracted.\n",
    "        Returns\n",
    "        --------\n",
    "        lagged : Tensor\n",
    "            a tensor of shape (N, S, C, I),\n",
    "            where S = subsequences_length and I = len(indices),\n",
    "            containing lagged subsequences.\n",
    "            Specifically, lagged[i, :, j, k] = sequence[i, -indices[k]-S+j, :].\n",
    "        \"\"\"\n",
    "        # we must have: history_length + begin_index >= 0\n",
    "        # that is: history_length - lag_index - sequence_length >= 0\n",
    "        # hence the following assert\n",
    "        assert max(indices) + subsequences_length <= sequence_length, (\n",
    "            f\"lags cannot go further than history length, found lag \"\n",
    "            f\"{max(indices)} while history length is only {sequence_length}\"\n",
    "        )\n",
    "        assert all(lag_index >= 0 for lag_index in indices)\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...].unsqueeze(1))\n",
    "        return torch.cat(lagged_values, dim=1).permute(0, 2, 3, 1)\n",
    "\n",
    "    def unroll(\n",
    "        self,\n",
    "        lags: torch.Tensor,\n",
    "        scale: torch.Tensor,\n",
    "        time_feat: torch.Tensor,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        unroll_length: int,\n",
    "        begin_state: Optional[Union[List[torch.Tensor], torch.Tensor]] = None,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        Union[List[torch.Tensor], torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "\n",
    "        # (batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        lags_scaled = lags / scale.unsqueeze(-1)\n",
    "\n",
    "        # assert_shape(\n",
    "        #     lags_scaled, (-1, unroll_length, self.target_dim, len(self.lags_seq)),\n",
    "        # )\n",
    "\n",
    "        input_lags = lags_scaled.reshape(\n",
    "            (-1, unroll_length, len(self.lags_seq) * self.target_dim)\n",
    "        )\n",
    "\n",
    "        # (batch_size, target_dim, embed_dim)\n",
    "        index_embeddings = self.embed(target_dimension_indicator)\n",
    "        # assert_shape(index_embeddings, (-1, self.target_dim, self.embed_dim))\n",
    "\n",
    "        # (batch_size, seq_len, target_dim * embed_dim)\n",
    "        repeated_index_embeddings = (\n",
    "            index_embeddings.unsqueeze(1)\n",
    "            .expand(-1, unroll_length, -1, -1)\n",
    "            .reshape((-1, unroll_length, self.target_dim * self.embed_dim))\n",
    "        )\n",
    "\n",
    "        # (batch_size, sub_seq_len, input_dim)\n",
    "        inputs = torch.cat((input_lags, repeated_index_embeddings, time_feat), dim=-1)\n",
    "\n",
    "        # unroll encoder\n",
    "        outputs, state = self.rnn(inputs, begin_state)\n",
    "\n",
    "        # assert_shape(outputs, (-1, unroll_length, self.num_cells))\n",
    "        # for s in state:\n",
    "        #     assert_shape(s, (-1, self.num_cells))\n",
    "\n",
    "        # assert_shape(\n",
    "        #     lags_scaled, (-1, unroll_length, self.target_dim, len(self.lags_seq)),\n",
    "        # )\n",
    "\n",
    "        return outputs, state, lags_scaled, inputs\n",
    "\n",
    "    def unroll_encoder(\n",
    "        self,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: Optional[torch.Tensor],\n",
    "        future_target_cdf: Optional[torch.Tensor],\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        Union[List[torch.Tensor], torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Unrolls the RNN encoder over past and, if present, future data.\n",
    "        Returns outputs and state of the encoder, plus the scale of\n",
    "        past_target_cdf and a vector of static features that was constructed\n",
    "        and fed as input to the encoder. All tensor arguments should have NTC\n",
    "        layout.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        past_time_feat\n",
    "            Past time features (batch_size, history_length, num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "        future_target_cdf\n",
    "            Future marginal CDF transformed target values (batch_size,\n",
    "            prediction_length, target_dim)\n",
    "        target_dimension_indicator\n",
    "            Dimensionality of the time series (batch_size, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs\n",
    "            RNN outputs (batch_size, seq_len, num_cells)\n",
    "        states\n",
    "            RNN states. Nested list with (batch_size, num_cells) tensors with\n",
    "        dimensions target_dim x num_layers x (batch_size, num_cells)\n",
    "        scale\n",
    "            Mean scales for the time series (batch_size, 1, target_dim)\n",
    "        lags_scaled\n",
    "            Scaled lags(batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        inputs\n",
    "            inputs to the RNN\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        if future_time_feat is None or future_target_cdf is None:\n",
    "            time_feat = past_time_feat[:, -self.context_length :, ...]\n",
    "            sequence = past_target_cdf\n",
    "            sequence_length = self.history_length\n",
    "            subsequences_length = self.context_length\n",
    "        else:\n",
    "            time_feat = torch.cat(\n",
    "                (past_time_feat[:, -self.context_length :, ...], future_time_feat),\n",
    "                dim=1,\n",
    "            )\n",
    "            sequence = torch.cat((past_target_cdf, future_target_cdf), dim=1)\n",
    "            sequence_length = self.history_length + self.prediction_length\n",
    "            subsequences_length = self.context_length + self.prediction_length\n",
    "\n",
    "        # (batch_size, sub_seq_len, target_dim, num_lags)\n",
    "        lags = self.get_lagged_subsequences(\n",
    "            sequence=sequence,\n",
    "            sequence_length=sequence_length,\n",
    "            indices=self.lags_seq,\n",
    "            subsequences_length=subsequences_length,\n",
    "        )\n",
    "\n",
    "        # scale is computed on the context length last units of the past target\n",
    "        # scale shape is (batch_size, 1, target_dim)\n",
    "        _, scale = self.scaler(\n",
    "            past_target_cdf[:, -self.context_length :, ...],\n",
    "            past_observed_values[:, -self.context_length :, ...],\n",
    "        )\n",
    "\n",
    "        outputs, states, lags_scaled, inputs = self.unroll(\n",
    "            lags=lags,\n",
    "            scale=scale,\n",
    "            time_feat=time_feat,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "            unroll_length=subsequences_length,\n",
    "            begin_state=None,\n",
    "        )\n",
    "\n",
    "        return outputs, states, scale, lags_scaled, inputs\n",
    "\n",
    "    def distr_args(self, rnn_outputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns the distribution of DeepVAR with respect to the RNN outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rnn_outputs\n",
    "            Outputs of the unrolled RNN (batch_size, seq_len, num_cells)\n",
    "        scale\n",
    "            Mean scale for each time series (batch_size, 1, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distr\n",
    "            Distribution instance\n",
    "        distr_args\n",
    "            Distribution arguments\n",
    "        \"\"\"\n",
    "        (distr_args,) = self.proj_dist_args(rnn_outputs)\n",
    "\n",
    "        # # compute likelihood of target given the predicted parameters\n",
    "        # distr = self.distr_output.distribution(distr_args, scale=scale)\n",
    "\n",
    "        # return distr, distr_args\n",
    "        return distr_args\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: torch.Tensor,\n",
    "        future_target_cdf: torch.Tensor,\n",
    "        future_observed_values: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the loss for training DeepVAR, all inputs tensors representing\n",
    "        time series have NTC layout.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        past_time_feat\n",
    "            Dynamic features of past time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "        future_target_cdf\n",
    "            Future marginal CDF transformed target values (batch_size,\n",
    "            prediction_length, target_dim)\n",
    "        future_observed_values\n",
    "            Indicator whether or not the future values were observed\n",
    "            (batch_size, prediction_length, target_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distr\n",
    "            Loss with shape (batch_size, 1)\n",
    "        likelihoods\n",
    "            Likelihoods for each time step\n",
    "            (batch_size, context + prediction_length, 1)\n",
    "        distr_args\n",
    "            Distribution arguments (context + prediction_length,\n",
    "            number_of_arguments)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = self.context_length + self.prediction_length\n",
    "\n",
    "        # unroll the decoder in \"training mode\", i.e. by providing future data\n",
    "        # as well\n",
    "        rnn_outputs, _, scale, _, _ = self.unroll_encoder(\n",
    "            past_time_feat=past_time_feat,\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            past_observed_values=past_observed_values,\n",
    "            past_is_pad=past_is_pad,\n",
    "            future_time_feat=future_time_feat,\n",
    "            future_target_cdf=future_target_cdf,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "        )\n",
    "\n",
    "        # put together target sequence\n",
    "        # (batch_size, seq_len, target_dim)\n",
    "        target = torch.cat(\n",
    "            (past_target_cdf[:, -self.context_length :, ...], future_target_cdf),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # assert_shape(target, (-1, seq_len, self.target_dim))\n",
    "\n",
    "        distr_args = self.distr_args(rnn_outputs=rnn_outputs)\n",
    "        if self.scaling:\n",
    "            self.diffusion.scale = scale\n",
    "\n",
    "        # we sum the last axis to have the same shape for all likelihoods\n",
    "        # (batch_size, subseq_length, 1)\n",
    "\n",
    "        likelihoods = self.diffusion.log_prob(target, distr_args).unsqueeze(-1)\n",
    "\n",
    "        # assert_shape(likelihoods, (-1, seq_len, 1))\n",
    "\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # (batch_size, subseq_length, target_dim)\n",
    "        observed_values = torch.cat(\n",
    "            (\n",
    "                past_observed_values[:, -self.context_length :, ...],\n",
    "                future_observed_values,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # mask the loss at one time step if one or more observations is missing\n",
    "        # in the target dimensions (batch_size, subseq_length, 1)\n",
    "        loss_weights, _ = observed_values.min(dim=-1, keepdim=True)\n",
    "\n",
    "        # assert_shape(loss_weights, (-1, seq_len, 1))\n",
    "\n",
    "        loss = weighted_average(likelihoods, weights=loss_weights, dim=1)\n",
    "\n",
    "        # assert_shape(loss, (-1, -1, 1))\n",
    "\n",
    "        # self.distribution = distr\n",
    "\n",
    "        return (loss.mean(), likelihoods, distr_args)\n",
    "\n",
    "\n",
    "class TimeGradPredictionNetwork(TimeGradTrainingNetwork):\n",
    "    def __init__(self, num_parallel_samples: int, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "\n",
    "        # for decoding the lags are shifted by one,\n",
    "        # at the first time-step of the decoder a lag of one corresponds to\n",
    "        # the last target value\n",
    "        self.shifted_lags = [l - 1 for l in self.lags_seq]\n",
    "\n",
    "    def sampling_decoder(\n",
    "        self,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        time_feat: torch.Tensor,\n",
    "        scale: torch.Tensor,\n",
    "        begin_states: Union[List[torch.Tensor], torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes sample paths by unrolling the RNN starting with a initial\n",
    "        input and state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        time_feat\n",
    "            Dynamic features of future time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        scale\n",
    "            Mean scale for each time series (batch_size, 1, target_dim)\n",
    "        begin_states\n",
    "            List of initial states for the RNN layers (batch_size, num_cells)\n",
    "        Returns\n",
    "        --------\n",
    "        sample_paths : Tensor\n",
    "            A tensor containing sampled paths. Shape: (1, num_sample_paths,\n",
    "            prediction_length, target_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        def repeat(tensor, dim=0):\n",
    "            return tensor.repeat_interleave(repeats=self.num_parallel_samples, dim=dim)\n",
    "\n",
    "        # blows-up the dimension of each tensor to\n",
    "        # batch_size * self.num_sample_paths for increasing parallelism\n",
    "        repeated_past_target_cdf = repeat(past_target_cdf)\n",
    "        repeated_time_feat = repeat(time_feat)\n",
    "        repeated_scale = repeat(scale)\n",
    "        if self.scaling:\n",
    "            self.diffusion.scale = repeated_scale\n",
    "        repeated_target_dimension_indicator = repeat(target_dimension_indicator)\n",
    "\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            repeated_states = [repeat(s, dim=1) for s in begin_states]\n",
    "        else:\n",
    "            repeated_states = repeat(begin_states, dim=1)\n",
    "\n",
    "        future_samples = []\n",
    "\n",
    "        # for each future time-units we draw new samples for this time-unit\n",
    "        # and update the state\n",
    "        for k in range(self.prediction_length):\n",
    "            lags = self.get_lagged_subsequences(\n",
    "                sequence=repeated_past_target_cdf,\n",
    "                sequence_length=self.history_length + k,\n",
    "                indices=self.shifted_lags,\n",
    "                subsequences_length=1,\n",
    "            )\n",
    "\n",
    "            rnn_outputs, repeated_states, _, _ = self.unroll(\n",
    "                begin_state=repeated_states,\n",
    "                lags=lags,\n",
    "                scale=repeated_scale,\n",
    "                time_feat=repeated_time_feat[:, k : k + 1, ...],\n",
    "                target_dimension_indicator=repeated_target_dimension_indicator,\n",
    "                unroll_length=1,\n",
    "            )\n",
    "\n",
    "            distr_args = self.distr_args(rnn_outputs=rnn_outputs)\n",
    "\n",
    "            # (batch_size, 1, target_dim)\n",
    "            new_samples = self.diffusion.sample(cond=distr_args)\n",
    "\n",
    "            # (batch_size, seq_len, target_dim)\n",
    "            future_samples.append(new_samples)\n",
    "            repeated_past_target_cdf = torch.cat(\n",
    "                (repeated_past_target_cdf, new_samples), dim=1\n",
    "            )\n",
    "\n",
    "        # (batch_size * num_samples, prediction_length, target_dim)\n",
    "        samples = torch.cat(future_samples, dim=1)\n",
    "\n",
    "        # (batch_size, num_samples, prediction_length, target_dim)\n",
    "        return samples.reshape(\n",
    "            (\n",
    "                -1,\n",
    "                self.num_parallel_samples,\n",
    "                self.prediction_length,\n",
    "                self.target_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target_dimension_indicator: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target_cdf: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        past_is_pad: torch.Tensor,\n",
    "        future_time_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts samples given the trained DeepVAR model.\n",
    "        All tensors should have NTC layout.\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_dimension_indicator\n",
    "            Indices of the target dimension (batch_size, target_dim)\n",
    "        past_time_feat\n",
    "            Dynamic features of past time series (batch_size, history_length,\n",
    "            num_features)\n",
    "        past_target_cdf\n",
    "            Past marginal CDF transformed target values (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_observed_values\n",
    "            Indicator whether or not the values were observed (batch_size,\n",
    "            history_length, target_dim)\n",
    "        past_is_pad\n",
    "            Indicator whether the past target values have been padded\n",
    "            (batch_size, history_length)\n",
    "        future_time_feat\n",
    "            Future time features (batch_size, prediction_length, num_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_paths : Tensor\n",
    "            A tensor containing sampled paths (1, num_sample_paths,\n",
    "            prediction_length, target_dim).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # mark padded data as unobserved\n",
    "        # (batch_size, target_dim, seq_len)\n",
    "        past_observed_values = torch.min(\n",
    "            past_observed_values, 1 - past_is_pad.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # unroll the decoder in \"prediction mode\", i.e. with past data only\n",
    "        _, begin_states, scale, _, _ = self.unroll_encoder(\n",
    "            past_time_feat=past_time_feat,\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            past_observed_values=past_observed_values,\n",
    "            past_is_pad=past_is_pad,\n",
    "            future_time_feat=None,\n",
    "            future_target_cdf=None,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "        )\n",
    "\n",
    "        return self.sampling_decoder(\n",
    "            past_target_cdf=past_target_cdf,\n",
    "            target_dimension_indicator=target_dimension_indicator,\n",
    "            time_feat=future_time_feat,\n",
    "            scale=scale,\n",
    "            begin_states=begin_states,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
