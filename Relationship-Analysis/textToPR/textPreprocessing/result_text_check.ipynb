{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82101512",
   "metadata": {},
   "source": [
    "### 맨 밑으로 가시면 한번에 전처리 처리를 할 수 있도록 해놨습니다.\n",
    "\n",
    "\n",
    "(처음 실행시에는, 모든 cell을 실행시켜야 합니다)\n",
    "\n",
    "[자동화 처리](#자동화-처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827b14a",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca35d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "import contractions\n",
    "\n",
    "from typing import List, Optional, Union, Callable\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8504af8",
   "metadata": {},
   "source": [
    "### DataFrame Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3c356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ed7fc",
   "metadata": {},
   "source": [
    "## text cleaning function\n",
    "\n",
    "Remove punctuations\n",
    "\n",
    "Convert text to tokens\n",
    "\n",
    "Remove tokens of length less than or equal to 3\n",
    "\n",
    "Remove stopwords using NLTK corpus stopwords list to match\n",
    "\n",
    "Apply stemming\n",
    "\n",
    "Apply lemmatization\n",
    "\n",
    "Convert words to feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf41bcf",
   "metadata": {},
   "source": [
    "### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_convertion(input_text: str) -> str:\n",
    "    \"\"\" Convert input text to lower case \"\"\"\n",
    "    return input_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a264a79",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3ecf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "def remove_urls(input_text):\n",
    "    for i in range(len(input_text)):\n",
    "        input_text[i] = re.sub(pattern=url_pattern, repl=' ', string=input_text[i])\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9eff0",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1173bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(input_text: str) -> str:\n",
    "    for i in range(len(input_text)):\n",
    "        input_text[i] = re.sub(r'\\d+', '', str(input_text[i]))\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0cc8bc",
   "metadata": {},
   "source": [
    "### Remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ef9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespaces(input_text: str) -> str:\n",
    "    for i in range(len(input_text)):\n",
    "        input_text[i] = input_text[i].strip()\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd7157",
   "metadata": {},
   "source": [
    "### 악센트 텍스트 ASCII 변환기로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94dd2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of accented text to ASCII converter in python\n",
    "\n",
    "import unidecode\n",
    "\n",
    "def accented_to_ascii(input_text):\n",
    "    for i in range(len(input_text)):\n",
    "        # apply unidecode function on text to convert\n",
    "        # accented characters to ASCII values\n",
    "        input_text[i] = unidecode.unidecode(input_text[i])\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f5da2",
   "metadata": {},
   "source": [
    "### Converting chat conversion words to normal words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a240942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open short_form file and then read sentences from text file using read())\n",
    "short_form_list = open('slang.txt', 'r')\n",
    "chat_words_str = short_form_list.read()\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[0]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c16460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex) omg => oh my god\n",
    "\n",
    "def short_to_original(input_text):\n",
    "    for i in range(len(input_text)):\n",
    "        new_text = []\n",
    "        for w in input_text[i].split():\n",
    "            if w.upper() in chat_words_list:\n",
    "                new_text.append(chat_words_map_dict[w.upper()])\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        input_text[i] = \" \".join(new_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da289e5",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b3c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import contractions_dict\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                    flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "            if contraction_mapping.get(match) \\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    try:\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    except:\n",
    "        return text\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace57afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_function(input_text):\n",
    "    for i in range(len(input_text)):\n",
    "        input_text[i] = expand_contractions(input_text[i])\n",
    "        #expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f6043",
   "metadata": {},
   "source": [
    "### NER(named entity recognition), 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc4c71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\word\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# New York -> New-York\n",
    "\n",
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')  #loaded large model\n",
    "\n",
    "def convert_NER(input_text):\n",
    "    for i in range(len(input_text)):\n",
    "        entitylist = list() #Empty list\n",
    "        for w in nlp(input_text[i]).ents: # nlp(s).ents gives us name of entities in s\n",
    "            if len(str(w.text).split()) > 1: # if number of words in s > 1\n",
    "                entitylist.append((w.text, str(w.text).replace(' ','-')))\n",
    "        #replacing space with - to join words\n",
    "        entitylist # list of entities which should be single token\n",
    "        # Output: [('New York', 'New-York')]\n",
    "        for item in entitylist:\n",
    "            input_text[i] = input_text[i].replace(item[0],\\\n",
    "                                                  item[1])\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6a589",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85764fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopword(input_text, stop_words):\n",
    "    stop_words = set(stop_words)\n",
    "    filtered_sentence = []\n",
    "    for i in range(0, len(input_text)):\n",
    "        word_tokens = word_tokenize(input_text[i])\n",
    "        filt_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence.append(filt_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e1f61",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming using PorterStemming from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525e0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Stemming using PorterStemming from nltk library\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(input_text, stemmer):\n",
    "    for i in range(len(input_text)):\n",
    "        # word tokenization\n",
    "        try:\n",
    "            tokens = word_tokenize(input_text[i])\n",
    "        except:\n",
    "            tokens = input_text[i]\n",
    "        for index in range(len(tokens)):\n",
    "            # stem word to each word\n",
    "            stem_word = stemmer.stem(tokens[index])\n",
    "            # update tokens list with stem word\n",
    "            tokens[index] = stem_word\n",
    "        input_text[i] = ' '.join(tokens)\n",
    "        # join list with space separator as string\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d9835",
   "metadata": {},
   "source": [
    "### lemmatization\n",
    "\n",
    "lemmatization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb311927",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of lemmatization using nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization(input_text, lemma):\n",
    "    for i in range(len(input_text)):\n",
    "        # word tokenization\n",
    "        try:\n",
    "            tokens = word_tokenize(input_text[i])\n",
    "        except:\n",
    "            tokens = input_text[i]\n",
    "        for index in range(len(tokens)):\n",
    "            # lemma word\n",
    "            lemma_word = lemma.lemmatize(tokens[index])\n",
    "            tokens[index] = lemma_word\n",
    "        input_text[i] = ' '.join(tokens)\n",
    "    return input_text\n",
    "\n",
    "## initialize lemmatizer object\n",
    "# lemma = WordNetLemmatizer()\n",
    "# lemma_result = lemmatization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9a7d",
   "metadata": {},
   "source": [
    "## 엑셀 파일로 저장\n",
    "\n",
    "첫번째 자리 - \"개체명 인식\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "두번째 자리 - \"줄임 기호 늘리기\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "세번째 자리 - \"줄임말을 늘렸는지\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "-\n",
    "\n",
    "네번째 자리 - \"stopword 제거\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "-\n",
    "\n",
    "다섯번째 자리 - \"Stemming\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "여섯번째 자리 - \"lemmatization\" 처리를 했는지(0: 안했음, 1: 했음)\n",
    "\n",
    "\n",
    "ex) \n",
    "\n",
    "1. 000-0-00\n",
    "\n",
    "-> 아무런 전처리 과정이 일어나지 않았다는 것\n",
    "\n",
    "2. 111-1-00\n",
    "\n",
    "-> 개체명 인식 + 줄임 기호 늘이기 + 줄임말 늘리기 O, stopword 처리 O, stemming + lemmatization 처리 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12863023",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# data_preprocessing 이라는 함수를 선언.\n",
    "# data_preprocessing의 parameter는 총 7가지.\n",
    "# input text : DataFrame\n",
    "# 나머지 : boolean - 0(False), 1(True)\n",
    "\n",
    "def data_preprocessing(input_text, is_NER, is_expand, is_sortToOriginal, m_stopword, is_stem, is_lem):\n",
    "    \n",
    "    # ----------------------------기본----------------------------\n",
    "    input_text = lower_case_convertion(input_text.str)\n",
    "    input_text = remove_urls(input_text)\n",
    "    input_text = remove_whitespaces(input_text)\n",
    "    # 텍스트에서의 강조 악센트 지우기\n",
    "    input_text = accented_to_ascii(input_text)\n",
    "    \n",
    "    # ----------------------------선택----------------------------    \n",
    "    # 개체명 인식\n",
    "    if(is_NER) : input_text = convert_NER(input_text)\n",
    "        \n",
    "    # 줄임 기호 늘리기(Expand contractions)\n",
    "    # ex) don’t -> do not\n",
    "    if(is_expand) : input_text = expand_contractions_function(input_text)\n",
    "        \n",
    "    # 줄임말(Expand Acronym)\n",
    "    # Ex) ASAP -> As Soon As Possible\n",
    "    if(is_sortToOriginal) : input_text = short_to_original(input_text)\n",
    "    \n",
    "    # stopword remove\n",
    "    input_text = remove_stopword(input_text, m_stopword)\n",
    "    \n",
    "    # Stemming\n",
    "    # Stem을 추출하는 작업. 형태학적 분석을 단순화 해주는 기법\n",
    "    # Ex) am -> am , the going -> the go, having -> hav\n",
    "    if(is_stem) : input_text = porter_stemmer(input_text, stemmer)\n",
    "        \n",
    "    # lemmatization\n",
    "    # Lemma의 기본 단어 (기본 사전 단어)로 변화 하는 기법\n",
    "    # Ex) am -> be , the going -> the going, having -> have\n",
    "    if(is_lem) : input_text = lemmatization(input_text, lemma)\n",
    "    \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7183e1",
   "metadata": {},
   "source": [
    "### 전처리 처리한 dataframe을 csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f0f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 아래 코드(for문이 3개 겹쳐있는 코드)에서 자동화를 시켜주기 위해서 parameter를 설정했습니다.\n",
    "\n",
    "## 함수 parameter 설정\n",
    "# 0(처리가 안되어있는 경우 - False) \n",
    "# 1(처리가 되어있는 경우 - True)\n",
    "is_NER = [0, 1]\n",
    "is_expand = [0, 1]\n",
    "is_sortToOriginal = [0, 1]\n",
    "\n",
    "# stopword\n",
    "# 0(stopword 처리가 없는 경우 - False)\n",
    "# 1(stopword 처리가 있는 경우 - True)\n",
    "no_stopword = []\n",
    "stopword = pd.read_csv(\"./stopword.txt\", encoding='cp949')\n",
    "stopword = stopword['word']\n",
    "stop_words = [no_stopword, stopword]\n",
    "\n",
    "## 함수 parameter 설정\n",
    "# 0(처리가 안되어있는 경우 - False) \n",
    "# 1(처리가 되어있는 경우 - True)\n",
    "is_stem = [0, 1] # Stemming\n",
    "is_lem = [0, 1] # lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987472e6",
   "metadata": {},
   "source": [
    "### 자동화 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656060a",
   "metadata": {},
   "source": [
    "초기에 전처리의 경우의 수는 2x2x2x2x2x2x2 = 128 개 였습니다.\n",
    "\n",
    "그러나 너무 많은 경우의 수로,\n",
    "\n",
    "000-0-00\n",
    "000-0-11\n",
    "000-1-00\n",
    "000-1-11\n",
    "111-0-00\n",
    "111-0-11\n",
    "111-1-00\n",
    "111-1-11\n",
    "\n",
    "총 8가지의 경우만 확인해보기로 했습니다. (X-X-X의 form이 아닌, XXX-X-XX의 form으로 남겨둔 이유는 확장성 때문에 혹시나 남겨뒀습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca025e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000-0-00\n",
      "000-0-11\n",
      "000-1-00\n",
      "000-1-11\n",
      "111-0-00\n",
      "111-0-11\n",
      "111-1-00\n",
      "111-1-11\n"
     ]
    }
   ],
   "source": [
    "for ner in is_NER:\n",
    "    for stopwordIndex in range(len(stop_words)):\n",
    "        for stem in is_stem:\n",
    "            df = pd.DataFrame({'sentence' : \n",
    "                   data_preprocessing(data['sentence'], ner, ner, ner,\n",
    "                                      stop_words[stopwordIndex], stem, stem),\n",
    "                  'trust':data.trust,\n",
    "                  'control mutuality':data['control mutuality'],\n",
    "                  'commitment':data.commitment,\n",
    "                  'satisfaction':data.satisfaction})\n",
    "            \n",
    "            label = str(ner) + str(ner) + str(ner) + '-' + str(stopwordIndex) + '-' + str(stem) + str(stem)\n",
    "            print(label)\n",
    "            #df.to_csv('textPreprocessingDataSet/text_preprocessing('+label+').csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
